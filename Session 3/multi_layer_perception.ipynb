{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"multi_layer_perception.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP4TmxTH3IcIJeEhfAtQWLE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5wMKSYN4oouw","executionInfo":{"status":"ok","timestamp":1621082811168,"user_tz":-420,"elapsed":27020,"user":{"displayName":"Hoang Phi Nguyen","photoUrl":"","userId":"08938458352174786823"}},"outputId":"b0d77752-54c7-4b03-b0f8-ad5ec32983f1"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0KdJQ2flEMeR"},"source":["Load dataset\n"]},{"cell_type":"code","metadata":{"id":"RuAOz1gxEJC0","executionInfo":{"status":"ok","timestamp":1621088128679,"user_tz":-420,"elapsed":1084,"user":{"displayName":"Hoang Phi Nguyen","photoUrl":"","userId":"08938458352174786823"}}},"source":["import urllib.request\n","import os\n","import zipfile\n","\n","url = \"https://drive.google.com/file/d/1Ok23WNlNjIpy0PdoD2JiM_jqE6gmGrVx/view?usp=sharing\"\n","target = \"/content/drive/MyDrive/DSLab/Session 3/data/data.zip\"\n","\n","if not os.path.exists(target):\n","\n","    urllib.request.urlretrieve(url, target)\n","\n"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OCFOLe10JkYZ","executionInfo":{"status":"ok","timestamp":1621088739193,"user_tz":-420,"elapsed":1046,"user":{"displayName":"Hoang Phi Nguyen","photoUrl":"","userId":"08938458352174786823"}},"outputId":"a8dd8cc7-9f65-4529-840c-6d566c8632a0"},"source":["!unzip  \"/content/drive/MyDrive/DSLab/Session 3/data/data.zip\" -d \"/content/drive/MyDrive/DSLab/Session 3/data/\""],"execution_count":48,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/MyDrive/DSLab/Session 3/data/data.zip\n","  End-of-central-directory signature not found.  Either this file is not\n","  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n","  latter case the central directory and zipfile comment will be found on\n","  the last disk(s) of this archive.\n","unzip:  cannot find zipfile directory in one of /content/drive/MyDrive/DSLab/Session 3/data/data.zip or\n","        /content/drive/MyDrive/DSLab/Session 3/data/data.zip.zip, and cannot find /content/drive/MyDrive/DSLab/Session 3/data/data.zip.ZIP, period.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NSC3W7z_nvAT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621047679357,"user_tz":-420,"elapsed":752,"user":{"displayName":"Hoang Phi Nguyen","photoUrl":"","userId":"08938458352174786823"}},"outputId":"9e1609f6-8ba6-4a80-b787-167f02ae62ba"},"source":["import numpy as np\n","\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","# import tensorflow as tf\n","import random\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7_R8F4KQ8YXa"},"source":["1. Tạo class MLP để xây dựng kiến trúc multi-layer perception gồm 1 layer input, 1 hidden layer, 1 layer output và định nghĩa 1 số hàm: build_graph, trainer"]},{"cell_type":"code","metadata":{"id":"862FicFMoBuN"},"source":["class MLP:\n","  def __init__(self, vocab_size, hidden_size_1):\n","    self._vocab_size = vocab_size\n","    self._hidden_size_1 = hidden_size_1\n","\n","  def build_graph(self):\n","    self._X = tf.placeholder(tf.float32, shape= [None, self._vocab_size])\n","    self._real_Y = tf.placeholder(tf.int32, shape=[None, ])\n","    NUM_CLASSES = 20\n","\n","    weights_1 = tf.get_variable(\n","        name= 'weights_input_hidden_1',\n","        shape= (self._vocab_size, self._hidden_size_1),\n","        initializer= tf.random_normal_initializer(seed=2021),\n","    )\n","    biases_1 = tf.get_variable(\n","        name= 'biases_input_hidden_1',\n","        shape= (self._hidden_size_1),\n","        initializer= tf.random_normal_initializer(seed= 2021)\n","    )\n","\n","    weights_2 = tf.get_variable(\n","        name= 'weights_hidden_1_2',\n","        shape= (self._hidden_size_1, NUM_CLASSES),\n","        initializer= tf.random_normal_initializer(seed=2021)\n","    ) \n","    biases_2 = tf.get_variable(\n","        name= 'biases_hidden_1_2',\n","        shape= (NUM_CLASSES),\n","        initializer= tf.random_normal_initializer(seed= 2021)\n","    )\n","\n","    # weights_2 = tf.get_variable(\n","    #     name= 'weights_hidden_1_2',\n","    #     shape= (self._hidden_size_1, self._hidden_size_2),\n","    #     initializer= tf.random_normal_initializer(seed=2021)\n","    # ) \n","    # biases_2 = tf.get_variable(\n","    #     name= 'biases_hidden_1_2',\n","    #     shape= (self._hidden_size_2),\n","    #     initializer= tf.random_normal_initializer(seed= 2021)\n","    # )\n","    # weights_3 = tf.get_variable(\n","    #     name= 'weights_hidden_2_3',\n","    #     shape= (self._hidden_size_2, self._hidden_size_3),\n","    #     initializer= tf.random_normal_initializer(seed=2021)\n","    # ) \n","    # biases_3 = tf.get_variable(\n","    #     name= 'biases_hidden_2_3',\n","    #     shape= (self._hidden_size_3),\n","    #     initializer= tf.random_normal_initializer(seed= 2021)\n","    # )\n","\n","    # weights_4 = tf.get_variable(\n","    #     name= 'weights_hidden_3_output',\n","    #     shape= (self._hidden_size_3, NUM_CLASSES),\n","    #     initializer= tf.random_normal_initializer(seed=2021)\n","    # )\n","    # biases_4 = tf.get_variable(\n","    #     name= 'biases_hidden_3_output',\n","    #     shape= (NUM_CLASSES),\n","    #     initializer= tf.random_normal_initializer(seed= 2021)\n","    # )\n","    hidden_1 = tf.matmul(self._X, weights_1) + biases_1\n","    hidden_1 = tf.nn.relu(hidden_1)\n","\n","    # hidden_2 = tf.matmul(hidden_1, weights_2) + biases_2\n","    # hidden_2 = tf.nn.relu(hidden_2)\n","\n","    # hidden_3 = tf.matmul(hidden_2, weights_3) + biases_3\n","    # hidden_3 = tf.nn.relu(hidden_3)\n","\n","\n","    logits = tf.matmul(hidden_1, weights_2) + biases_2\n","\n","    labels_one_hot = tf.one_hot(\n","        indices= self._real_Y,\n","        depth= NUM_CLASSES,\n","        dtype= tf.float32\n","    )\n","    loss = tf.nn.softmax_cross_entropy_with_logits(labels= labels_one_hot,\n","                                                   logits= logits)\n","    loss = tf.reduce_mean(loss)\n","\n","    probs = tf.nn.softmax(logits)\n","    predicted_labels = tf.argmax(probs, axis= 1)\n","    predicted_labels = tf.squeeze(predicted_labels)\n","\n","    return predicted_labels, loss\n","  def trainer(self, loss, learning_rate):\n","    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n","    return train_opt\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WD4_yde1_ucc"},"source":["2. Tạo lớp DataReader để đọc dữ liệu"]},{"cell_type":"code","metadata":{"id":"cKvTFcYEyOo-"},"source":["class DataReader:\n","  def __init__(self, data_path, batch_size, vocab_size):\n","    self._batch_size = batch_size\n","    \n","    with open(data_path) as f:\n","      d_lines = f.read().splitlines()\n","    self._data = []\n","    self._labels = []\n","    for data_id, line in enumerate(d_lines):\n","        vector = [0.0 for _ in range(vocab_size)]\n","        features = line.split('<fff>')\n","        label, doc_id = int(features[0]), int(features[1])\n","        \n","        tokens = features[2].split()\n","        for token in tokens:\n","          index, value = int(token.split(':')[0]),\\\n","                             float(token.split(':')[1])\n","          vector[index] = value\n","\n","        self._data.append(vector)\n","        self._labels.append(label)\n","    self._data = np.array(self._data)\n","    self._labels = np.array(self._labels)\n","\n","    self._num_epoch = 0\n","    self._batch_id = 0\n","\n","  def next_batch(self):\n","    start = self._batch_id * self._batch_size\n","    end = start + self._batch_size\n","    self._batch_id += 1\n","\n","    if end + self._batch_size > len(self._data):\n","      end = len(self._data)\n","      self._num_epoch += 1\n","      self._batch_id = 0\n","      indices = list(range(len(self._data)))\n","      random.seed(2018)\n","      random.shuffle(indices)\n","      self._data, self._labels = self._data[indices], self._labels[indices]\n","    return self._data[start:end], self._labels[start:end]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WVXMw6EjACke"},"source":["3. Định nghĩa 1 số hàm: \n","  - load_dataset(): đọc dữ liệu đưa và quá trình train và test\n","  - save_parameter(): lưu trọng số sau quá trình train\n","  - restore_parameter(): nạp trọng số trong quá trình test"]},{"cell_type":"code","metadata":{"id":"yeUF9aP7wc9A"},"source":["def load_dataset():\n","  train_data_reader = DataReader(\n","      data_path= '/content/drive/MyDrive/DSLab/Session 3/data/train_tf_idf.txt',\n","      batch_size= 50,\n","      vocab_size= vocab_size\n","  )\n","  test_data_reader = DataReader(\n","      data_path= '/content/drive/MyDrive/DSLab/Session 3/data/test_tf_idf.txt',\n","      batch_size= 50,\n","      vocab_size= vocab_size\n","  )\n","  return train_data_reader, test_data_reader\n","def save_parameter(name, value, epoch):\n","  filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n","  if len(value.shape) == 1:\n","    string_form = ','.join([str(number) for number in value])\n","  else:\n","    string_form = '\\n'.join([','.join([str(number) for number in value[row]])\n","                      for row in range(value.shape[0])])\n","  with open('/content/drive/MyDrive/DSLab/Session 3/save_parameters/' + filename, 'w') as f:\n","    f.write(string_form)\n","  \n","def restore_parameter(name, epoch):\n","  filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n","  with open('/content/drive/MyDrive/DSLab/Session 3/save_parameters/' + filename) as f:\n","    lines = f.read().splitlines()\n","  \n","  if len(lines) == 1:\n","    value = [float(number) for number in lines[0].split(',')]\n","  else:\n","    value = [[float(number) for number in lines[row].split(',')]\n","             for row in range(len(lines))]\n","  return value"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ibT9nEDrAp4m"},"source":["4. Mở 1 phiên chạy thực hiện training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJsbTSnFvomi","executionInfo":{"status":"ok","timestamp":1621050038734,"user_tz":-420,"elapsed":848419,"user":{"displayName":"Hoang Phi Nguyen","photoUrl":"","userId":"08938458352174786823"}},"outputId":"a019f48c-3d39-4298-faef-9d4f2b172049"},"source":["with open('/content/drive/MyDrive/DSLab/Session 3/data/words_idfs.txt') as f:\n","  vocab_size = len(f.read().splitlines())\n","\n","mlp = MLP(\n","    vocab_size= vocab_size,\n","    hidden_size_1= 50\n",")\n","\n","tf.reset_default_graph()\n","predicted_labels, loss = mlp.build_graph()\n","train_opt = mlp.trainer(loss= loss, learning_rate= 0.1)\n","train_data_reader, test_data_reader = load_dataset()\n","\n","\n","with tf.Session() as sess:\n","  \n","  step, MAX_STEP = 0, 100000\n","  print(len(train_data_reader._data))\n","\n","  sess.run(tf.global_variables_initializer())\n","  while step < MAX_STEP:\n","    train_data, train_labels = train_data_reader.next_batch()\n","    plabels_eval, loss_eval, _ = sess.run(\n","        [predicted_labels, loss, train_opt],\n","        feed_dict= {\n","            mlp._X: train_data,\n","            mlp._real_Y: train_labels\n","        }\n","    )\n","    step += 1\n","    print('Step: {}, loss: {}'.format(step, loss_eval))\n","  \n","  trainable_varibles = tf.trainable_variables()\n","  for variable in trainable_varibles:\n","    save_parameter(\n","        name= variable.name,\n","        value= variable.eval(),\n","        epoch= train_data_reader._num_epoch\n","    )\n","  \n","\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mKết quả truyền trực tuyến bị cắt bớt đến 5000 dòng cuối.\u001b[0m\n","Step: 95001, loss: 0.12950149178504944\n","Step: 95002, loss: 0.15600933134555817\n","Step: 95003, loss: 0.18207824230194092\n","Step: 95004, loss: 0.1292862743139267\n","Step: 95005, loss: 0.09533189982175827\n","Step: 95006, loss: 0.09228413552045822\n","Step: 95007, loss: 0.11983690410852432\n","Step: 95008, loss: 0.1723715215921402\n","Step: 95009, loss: 0.2551070749759674\n","Step: 95010, loss: 0.2497219443321228\n","Step: 95011, loss: 0.04062339663505554\n","Step: 95012, loss: 0.0\n","Step: 95013, loss: 0.08981812745332718\n","Step: 95014, loss: 0.043386753648519516\n","Step: 95015, loss: 0.0\n","Step: 95016, loss: 0.0\n","Step: 95017, loss: 0.0\n","Step: 95018, loss: 0.0\n","Step: 95019, loss: 0.19336219131946564\n","Step: 95020, loss: 0.09686721861362457\n","Step: 95021, loss: 0.17219330370426178\n","Step: 95022, loss: 0.038173526525497437\n","Step: 95023, loss: 0.031384486705064774\n","Step: 95024, loss: 0.09637635946273804\n","Step: 95025, loss: 0.0\n","Step: 95026, loss: 0.2938445210456848\n","Step: 95027, loss: 0.212005153298378\n","Step: 95028, loss: 0.046280279755592346\n","Step: 95029, loss: 0.035316720604896545\n","Step: 95030, loss: 0.14957018196582794\n","Step: 95031, loss: 0.10773138701915741\n","Step: 95032, loss: 0.13606864213943481\n","Step: 95033, loss: 0.3238619863986969\n","Step: 95034, loss: 0.23889614641666412\n","Step: 95035, loss: 0.07882457971572876\n","Step: 95036, loss: 0.2963232398033142\n","Step: 95037, loss: 0.14462122321128845\n","Step: 95038, loss: 0.14687579870224\n","Step: 95039, loss: 0.028819696977734566\n","Step: 95040, loss: 0.04391242936253548\n","Step: 95041, loss: 0.09547396004199982\n","Step: 95042, loss: 0.2670038342475891\n","Step: 95043, loss: 0.1973726451396942\n","Step: 95044, loss: 0.04507504403591156\n","Step: 95045, loss: 0.09389948099851608\n","Step: 95046, loss: 0.1152782142162323\n","Step: 95047, loss: 0.02730591967701912\n","Step: 95048, loss: 0.22307121753692627\n","Step: 95049, loss: 0.12163309007883072\n","Step: 95050, loss: 0.11768607050180435\n","Step: 95051, loss: 0.18747538328170776\n","Step: 95052, loss: 0.15660138428211212\n","Step: 95053, loss: 0.13471686840057373\n","Step: 95054, loss: 0.05281819775700569\n","Step: 95055, loss: 0.21723085641860962\n","Step: 95056, loss: 0.20648933947086334\n","Step: 95057, loss: 0.14614903926849365\n","Step: 95058, loss: 0.1456039845943451\n","Step: 95059, loss: 0.1342284083366394\n","Step: 95060, loss: 0.026151157915592194\n","Step: 95061, loss: 0.3309793472290039\n","Step: 95062, loss: 0.19947189092636108\n","Step: 95063, loss: 0.02670595794916153\n","Step: 95064, loss: 0.04160510003566742\n","Step: 95065, loss: 0.193024680018425\n","Step: 95066, loss: 0.18709112703800201\n","Step: 95067, loss: 0.09032268822193146\n","Step: 95068, loss: 0.11347559094429016\n","Step: 95069, loss: 0.05037200078368187\n","Step: 95070, loss: 0.02727460116147995\n","Step: 95071, loss: 0.05099963769316673\n","Step: 95072, loss: 0.16480767726898193\n","Step: 95073, loss: 0.1206449419260025\n","Step: 95074, loss: 0.16915486752986908\n","Step: 95075, loss: 0.17248746752738953\n","Step: 95076, loss: 0.08482162654399872\n","Step: 95077, loss: 0.27989473938941956\n","Step: 95078, loss: 0.3009113073348999\n","Step: 95079, loss: 0.09957090020179749\n","Step: 95080, loss: 0.09566961228847504\n","Step: 95081, loss: 0.07008195668458939\n","Step: 95082, loss: 0.3178456425666809\n","Step: 95083, loss: 0.19493766129016876\n","Step: 95084, loss: 0.1597169041633606\n","Step: 95085, loss: 0.3023433983325958\n","Step: 95086, loss: 0.11152604222297668\n","Step: 95087, loss: 0.12226182967424393\n","Step: 95088, loss: 0.1404809057712555\n","Step: 95089, loss: 0.18802009522914886\n","Step: 95090, loss: 0.07509712129831314\n","Step: 95091, loss: 0.25467759370803833\n","Step: 95092, loss: 0.0\n","Step: 95093, loss: 0.40205374360084534\n","Step: 95094, loss: 0.2641426920890808\n","Step: 95095, loss: 0.05272141844034195\n","Step: 95096, loss: 0.2919904589653015\n","Step: 95097, loss: 0.38400208950042725\n","Step: 95098, loss: 0.13350975513458252\n","Step: 95099, loss: 0.09396930038928986\n","Step: 95100, loss: 0.2066270411014557\n","Step: 95101, loss: 0.1523129791021347\n","Step: 95102, loss: 0.13125023245811462\n","Step: 95103, loss: 0.18788404762744904\n","Step: 95104, loss: 0.1804715394973755\n","Step: 95105, loss: 0.07996036857366562\n","Step: 95106, loss: 0.06577328592538834\n","Step: 95107, loss: 0.16238605976104736\n","Step: 95108, loss: 0.04866277799010277\n","Step: 95109, loss: 0.14483346045017242\n","Step: 95110, loss: 0.25950807332992554\n","Step: 95111, loss: 0.1214926689863205\n","Step: 95112, loss: 0.1821855902671814\n","Step: 95113, loss: 0.30406370759010315\n","Step: 95114, loss: 0.12774860858917236\n","Step: 95115, loss: 0.0\n","Step: 95116, loss: 0.1362566202878952\n","Step: 95117, loss: 0.076009601354599\n","Step: 95118, loss: 0.11751265823841095\n","Step: 95119, loss: 0.13372178375720978\n","Step: 95120, loss: 0.0\n","Step: 95121, loss: 0.07780525833368301\n","Step: 95122, loss: 0.17491279542446136\n","Step: 95123, loss: 0.11754216253757477\n","Step: 95124, loss: 0.036345094442367554\n","Step: 95125, loss: 0.16235190629959106\n","Step: 95126, loss: 0.0\n","Step: 95127, loss: 0.21475987136363983\n","Step: 95128, loss: 0.1120191365480423\n","Step: 95129, loss: 0.25021666288375854\n","Step: 95130, loss: 0.11365818232297897\n","Step: 95131, loss: 0.11128810793161392\n","Step: 95132, loss: 0.05408705770969391\n","Step: 95133, loss: 0.0\n","Step: 95134, loss: 0.03456266224384308\n","Step: 95135, loss: 0.07163754850625992\n","Step: 95136, loss: 0.03464119881391525\n","Step: 95137, loss: 0.11652405560016632\n","Step: 95138, loss: 0.1952849179506302\n","Step: 95139, loss: 0.11091429740190506\n","Step: 95140, loss: 0.265074759721756\n","Step: 95141, loss: 0.17252391576766968\n","Step: 95142, loss: 0.28807151317596436\n","Step: 95143, loss: 0.0\n","Step: 95144, loss: 0.05435783416032791\n","Step: 95145, loss: 0.2547479271888733\n","Step: 95146, loss: 0.06532122194766998\n","Step: 95147, loss: 0.05422535911202431\n","Step: 95148, loss: 0.0\n","Step: 95149, loss: 0.04467590153217316\n","Step: 95150, loss: 0.0676959902048111\n","Step: 95151, loss: 0.14720696210861206\n","Step: 95152, loss: 0.18849366903305054\n","Step: 95153, loss: 0.1057298481464386\n","Step: 95154, loss: 0.07166679203510284\n","Step: 95155, loss: 0.09231694042682648\n","Step: 95156, loss: 0.26906001567840576\n","Step: 95157, loss: 0.1474597007036209\n","Step: 95158, loss: 0.0726386234164238\n","Step: 95159, loss: 0.2332601398229599\n","Step: 95160, loss: 0.09402509778738022\n","Step: 95161, loss: 0.05262124165892601\n","Step: 95162, loss: 0.12557445466518402\n","Step: 95163, loss: 0.086640864610672\n","Step: 95164, loss: 0.0\n","Step: 95165, loss: 0.08028619736433029\n","Step: 95166, loss: 0.13125550746917725\n","Step: 95167, loss: 0.1612054854631424\n","Step: 95168, loss: 0.0\n","Step: 95169, loss: 0.21097905933856964\n","Step: 95170, loss: 0.046146295964717865\n","Step: 95171, loss: 0.13810774683952332\n","Step: 95172, loss: 0.18655185401439667\n","Step: 95173, loss: 0.08407814800739288\n","Step: 95174, loss: 0.19611544907093048\n","Step: 95175, loss: 0.223993718624115\n","Step: 95176, loss: 0.15977124869823456\n","Step: 95177, loss: 0.15663301944732666\n","Step: 95178, loss: 0.21339155733585358\n","Step: 95179, loss: 0.12253804504871368\n","Step: 95180, loss: 0.0\n","Step: 95181, loss: 0.058380309492349625\n","Step: 95182, loss: 0.19059506058692932\n","Step: 95183, loss: 0.15081609785556793\n","Step: 95184, loss: 0.18964749574661255\n","Step: 95185, loss: 0.06592061370611191\n","Step: 95186, loss: 0.0\n","Step: 95187, loss: 0.285521000623703\n","Step: 95188, loss: 0.30224570631980896\n","Step: 95189, loss: 0.09790806472301483\n","Step: 95190, loss: 0.0\n","Step: 95191, loss: 0.2226019650697708\n","Step: 95192, loss: 0.0610114187002182\n","Step: 95193, loss: 0.14156170189380646\n","Step: 95194, loss: 0.1468137949705124\n","Step: 95195, loss: 0.13533876836299896\n","Step: 95196, loss: 0.09061568975448608\n","Step: 95197, loss: 0.06474619358778\n","Step: 95198, loss: 0.269347608089447\n","Step: 95199, loss: 0.18619507551193237\n","Step: 95200, loss: 0.2949830889701843\n","Step: 95201, loss: 0.1300240159034729\n","Step: 95202, loss: 0.0\n","Step: 95203, loss: 0.1342211812734604\n","Step: 95204, loss: 0.08960533887147903\n","Step: 95205, loss: 0.10088516026735306\n","Step: 95206, loss: 0.14895832538604736\n","Step: 95207, loss: 0.05736619606614113\n","Step: 95208, loss: 0.2581455111503601\n","Step: 95209, loss: 0.24292966723442078\n","Step: 95210, loss: 0.05250343307852745\n","Step: 95211, loss: 0.06599967181682587\n","Step: 95212, loss: 0.04712814465165138\n","Step: 95213, loss: 0.032289404422044754\n","Step: 95214, loss: 0.0\n","Step: 95215, loss: 0.24874572455883026\n","Step: 95216, loss: 0.16849547624588013\n","Step: 95217, loss: 0.32411476969718933\n","Step: 95218, loss: 0.21914170682430267\n","Step: 95219, loss: 0.04776262119412422\n","Step: 95220, loss: 0.0\n","Step: 95221, loss: 0.09638026356697083\n","Step: 95222, loss: 0.13085247576236725\n","Step: 95223, loss: 0.10087724775075912\n","Step: 95224, loss: 0.1613548994064331\n","Step: 95225, loss: 0.0\n","Step: 95226, loss: 0.19632357358932495\n","Step: 95227, loss: 0.23675896227359772\n","Step: 95228, loss: 0.09241887927055359\n","Step: 95229, loss: 0.0736783966422081\n","Step: 95230, loss: 0.36134621500968933\n","Step: 95231, loss: 0.37229546904563904\n","Step: 95232, loss: 0.02479214407503605\n","Step: 95233, loss: 0.1956920176744461\n","Step: 95234, loss: 0.3061530292034149\n","Step: 95235, loss: 0.04505880922079086\n","Step: 95236, loss: 0.1680244505405426\n","Step: 95237, loss: 0.12310320138931274\n","Step: 95238, loss: 0.1626787632703781\n","Step: 95239, loss: 0.1983267217874527\n","Step: 95240, loss: 0.07110191136598587\n","Step: 95241, loss: 0.0\n","Step: 95242, loss: 0.09366587549448013\n","Step: 95243, loss: 0.13272596895694733\n","Step: 95244, loss: 0.2371882200241089\n","Step: 95245, loss: 0.17985393106937408\n","Step: 95246, loss: 0.09743437170982361\n","Step: 95247, loss: 0.1327614188194275\n","Step: 95248, loss: 0.28075459599494934\n","Step: 95249, loss: 0.25067609548568726\n","Step: 95250, loss: 0.373687207698822\n","Step: 95251, loss: 0.17564602196216583\n","Step: 95252, loss: 0.21623243391513824\n","Step: 95253, loss: 0.1435771882534027\n","Step: 95254, loss: 0.1914471834897995\n","Step: 95255, loss: 0.13202647864818573\n","Step: 95256, loss: 0.04525378346443176\n","Step: 95257, loss: 0.30465948581695557\n","Step: 95258, loss: 0.044765494763851166\n","Step: 95259, loss: 0.1272161304950714\n","Step: 95260, loss: 0.2872883677482605\n","Step: 95261, loss: 0.2613380551338196\n","Step: 95262, loss: 0.21169692277908325\n","Step: 95263, loss: 0.0\n","Step: 95264, loss: 0.04071063920855522\n","Step: 95265, loss: 0.0\n","Step: 95266, loss: 0.14295010268688202\n","Step: 95267, loss: 0.22663356363773346\n","Step: 95268, loss: 0.10019271075725555\n","Step: 95269, loss: 0.0476515106856823\n","Step: 95270, loss: 0.0\n","Step: 95271, loss: 0.04421122372150421\n","Step: 95272, loss: 0.20050914585590363\n","Step: 95273, loss: 0.14236700534820557\n","Step: 95274, loss: 0.19672857224941254\n","Step: 95275, loss: 0.13161200284957886\n","Step: 95276, loss: 0.19486863911151886\n","Step: 95277, loss: 0.17929695546627045\n","Step: 95278, loss: 0.09639696031808853\n","Step: 95279, loss: 0.04692757502198219\n","Step: 95280, loss: 0.20521530508995056\n","Step: 95281, loss: 0.06615633517503738\n","Step: 95282, loss: 0.08911135047674179\n","Step: 95283, loss: 0.09190664440393448\n","Step: 95284, loss: 0.16607670485973358\n","Step: 95285, loss: 0.25895750522613525\n","Step: 95286, loss: 0.16358546912670135\n","Step: 95287, loss: 0.270344078540802\n","Step: 95288, loss: 0.04176463186740875\n","Step: 95289, loss: 0.0\n","Step: 95290, loss: 0.14846129715442657\n","Step: 95291, loss: 0.07978573441505432\n","Step: 95292, loss: 0.06294184923171997\n","Step: 95293, loss: 0.07974757254123688\n","Step: 95294, loss: 0.13668183982372284\n","Step: 95295, loss: 0.1170082837343216\n","Step: 95296, loss: 0.0\n","Step: 95297, loss: 0.16306255757808685\n","Step: 95298, loss: 0.07841827720403671\n","Step: 95299, loss: 0.04365536570549011\n","Step: 95300, loss: 0.0821354016661644\n","Step: 95301, loss: 0.13118451833724976\n","Step: 95302, loss: 0.06703272461891174\n","Step: 95303, loss: 0.16346031427383423\n","Step: 95304, loss: 0.04867962747812271\n","Step: 95305, loss: 0.04842264950275421\n","Step: 95306, loss: 0.1594756543636322\n","Step: 95307, loss: 0.04694630950689316\n","Step: 95308, loss: 0.08824396133422852\n","Step: 95309, loss: 0.16415482759475708\n","Step: 95310, loss: 0.07654575258493423\n","Step: 95311, loss: 0.12782016396522522\n","Step: 95312, loss: 0.0868043527007103\n","Step: 95313, loss: 0.17867499589920044\n","Step: 95314, loss: 0.23344457149505615\n","Step: 95315, loss: 0.10533512383699417\n","Step: 95316, loss: 0.1363299936056137\n","Step: 95317, loss: 0.17464092373847961\n","Step: 95318, loss: 0.1656048595905304\n","Step: 95319, loss: 0.022796478122472763\n","Step: 95320, loss: 0.21006254851818085\n","Step: 95321, loss: 0.0\n","Step: 95322, loss: 0.09745959937572479\n","Step: 95323, loss: 0.05499789118766785\n","Step: 95324, loss: 0.19156280159950256\n","Step: 95325, loss: 0.21816329658031464\n","Step: 95326, loss: 0.0\n","Step: 95327, loss: 0.17351429164409637\n","Step: 95328, loss: 0.08085606247186661\n","Step: 95329, loss: 0.1743811070919037\n","Step: 95330, loss: 0.16475555300712585\n","Step: 95331, loss: 0.228911891579628\n","Step: 95332, loss: 0.1356571465730667\n","Step: 95333, loss: 0.04400373622775078\n","Step: 95334, loss: 0.09625902026891708\n","Step: 95335, loss: 0.14782054722309113\n","Step: 95336, loss: 0.1385369896888733\n","Step: 95337, loss: 0.12888532876968384\n","Step: 95338, loss: 0.0482209213078022\n","Step: 95339, loss: 0.14029841125011444\n","Step: 95340, loss: 0.10658872872591019\n","Step: 95341, loss: 0.06142266094684601\n","Step: 95342, loss: 0.0\n","Step: 95343, loss: 0.34709465503692627\n","Step: 95344, loss: 0.07488147914409637\n","Step: 95345, loss: 0.10575828701257706\n","Step: 95346, loss: 0.17633147537708282\n","Step: 95347, loss: 0.1258523017168045\n","Step: 95348, loss: 0.2899583578109741\n","Step: 95349, loss: 0.07756471633911133\n","Step: 95350, loss: 0.058128632605075836\n","Step: 95351, loss: 0.04905889555811882\n","Step: 95352, loss: 0.13660836219787598\n","Step: 95353, loss: 0.05456481873989105\n","Step: 95354, loss: 0.0\n","Step: 95355, loss: 0.1606294959783554\n","Step: 95356, loss: 0.13586676120758057\n","Step: 95357, loss: 0.13021573424339294\n","Step: 95358, loss: 0.19804784655570984\n","Step: 95359, loss: 0.08484919369220734\n","Step: 95360, loss: 0.0\n","Step: 95361, loss: 0.07426853477954865\n","Step: 95362, loss: 0.0\n","Step: 95363, loss: 0.10440091788768768\n","Step: 95364, loss: 0.12171302735805511\n","Step: 95365, loss: 0.12728862464427948\n","Step: 95366, loss: 0.0935206189751625\n","Step: 95367, loss: 0.30091923475265503\n","Step: 95368, loss: 0.20953865349292755\n","Step: 95369, loss: 0.0\n","Step: 95370, loss: 0.09989836812019348\n","Step: 95371, loss: 0.1676846295595169\n","Step: 95372, loss: 0.20710361003875732\n","Step: 95373, loss: 0.12154112011194229\n","Step: 95374, loss: 0.18940958380699158\n","Step: 95375, loss: 0.0\n","Step: 95376, loss: 0.14271777868270874\n","Step: 95377, loss: 0.18718664348125458\n","Step: 95378, loss: 0.0\n","Step: 95379, loss: 0.03574885055422783\n","Step: 95380, loss: 0.06936021149158478\n","Step: 95381, loss: 0.038888119161129\n","Step: 95382, loss: 0.08335163444280624\n","Step: 95383, loss: 0.23937618732452393\n","Step: 95384, loss: 0.06080513074994087\n","Step: 95385, loss: 0.12001178413629532\n","Step: 95386, loss: 0.075226791203022\n","Step: 95387, loss: 0.049674853682518005\n","Step: 95388, loss: 0.22999264299869537\n","Step: 95389, loss: 0.1418241560459137\n","Step: 95390, loss: 0.13486884534358978\n","Step: 95391, loss: 0.1310577243566513\n","Step: 95392, loss: 0.05862739682197571\n","Step: 95393, loss: 0.1011851504445076\n","Step: 95394, loss: 0.1847018599510193\n","Step: 95395, loss: 0.0\n","Step: 95396, loss: 0.18785175681114197\n","Step: 95397, loss: 0.0\n","Step: 95398, loss: 0.20306804776191711\n","Step: 95399, loss: 0.30740851163864136\n","Step: 95400, loss: 0.24310985207557678\n","Step: 95401, loss: 0.11781889200210571\n","Step: 95402, loss: 0.12854762375354767\n","Step: 95403, loss: 0.260761559009552\n","Step: 95404, loss: 0.23639440536499023\n","Step: 95405, loss: 0.10883811861276627\n","Step: 95406, loss: 0.22250251471996307\n","Step: 95407, loss: 0.08586867153644562\n","Step: 95408, loss: 0.1329035758972168\n","Step: 95409, loss: 0.09070088714361191\n","Step: 95410, loss: 0.07875313609838486\n","Step: 95411, loss: 0.19858421385288239\n","Step: 95412, loss: 0.2812916040420532\n","Step: 95413, loss: 0.2015000581741333\n","Step: 95414, loss: 0.2369309812784195\n","Step: 95415, loss: 0.061038170009851456\n","Step: 95416, loss: 0.18975333869457245\n","Step: 95417, loss: 0.21678633987903595\n","Step: 95418, loss: 0.040639542043209076\n","Step: 95419, loss: 0.12291817367076874\n","Step: 95420, loss: 0.162913978099823\n","Step: 95421, loss: 0.0\n","Step: 95422, loss: 0.16430671513080597\n","Step: 95423, loss: 0.11714664101600647\n","Step: 95424, loss: 0.1584465056657791\n","Step: 95425, loss: 0.0542943961918354\n","Step: 95426, loss: 0.2815452516078949\n","Step: 95427, loss: 0.05583822354674339\n","Step: 95428, loss: 0.3277130424976349\n","Step: 95429, loss: 0.2596193552017212\n","Step: 95430, loss: 0.055365972220897675\n","Step: 95431, loss: 0.3017288148403168\n","Step: 95432, loss: 0.0\n","Step: 95433, loss: 0.0\n","Step: 95434, loss: 0.16043928265571594\n","Step: 95435, loss: 0.14922599494457245\n","Step: 95436, loss: 0.135764017701149\n","Step: 95437, loss: 0.1811443567276001\n","Step: 95438, loss: 0.2054166942834854\n","Step: 95439, loss: 0.19677600264549255\n","Step: 95440, loss: 0.14769956469535828\n","Step: 95441, loss: 0.28389209508895874\n","Step: 95442, loss: 0.23611974716186523\n","Step: 95443, loss: 0.03455556556582451\n","Step: 95444, loss: 0.13142865896224976\n","Step: 95445, loss: 0.06621665507555008\n","Step: 95446, loss: 0.040908508002758026\n","Step: 95447, loss: 0.04686536639928818\n","Step: 95448, loss: 0.18303000926971436\n","Step: 95449, loss: 0.07539485394954681\n","Step: 95450, loss: 0.11919660866260529\n","Step: 95451, loss: 0.13792455196380615\n","Step: 95452, loss: 0.045454688370227814\n","Step: 95453, loss: 0.3717586100101471\n","Step: 95454, loss: 0.051394060254096985\n","Step: 95455, loss: 0.1010902002453804\n","Step: 95456, loss: 0.05526807904243469\n","Step: 95457, loss: 0.12339881807565689\n","Step: 95458, loss: 0.03653838112950325\n","Step: 95459, loss: 0.07175350189208984\n","Step: 95460, loss: 0.0\n","Step: 95461, loss: 0.1469767987728119\n","Step: 95462, loss: 0.10955321043729782\n","Step: 95463, loss: 0.08392513543367386\n","Step: 95464, loss: 0.0\n","Step: 95465, loss: 0.20685595273971558\n","Step: 95466, loss: 0.08452458679676056\n","Step: 95467, loss: 0.17636899650096893\n","Step: 95468, loss: 0.2763462960720062\n","Step: 95469, loss: 0.1243637278676033\n","Step: 95470, loss: 0.18200825154781342\n","Step: 95471, loss: 0.08990571647882462\n","Step: 95472, loss: 0.10515579581260681\n","Step: 95473, loss: 0.31207165122032166\n","Step: 95474, loss: 0.2867361009120941\n","Step: 95475, loss: 0.17445775866508484\n","Step: 95476, loss: 0.26068058609962463\n","Step: 95477, loss: 0.245167076587677\n","Step: 95478, loss: 0.09672021865844727\n","Step: 95479, loss: 0.20477554202079773\n","Step: 95480, loss: 0.03144504129886627\n","Step: 95481, loss: 0.09311455488204956\n","Step: 95482, loss: 0.04763512685894966\n","Step: 95483, loss: 0.11419736593961716\n","Step: 95484, loss: 0.13946762681007385\n","Step: 95485, loss: 0.1940147578716278\n","Step: 95486, loss: 0.21896249055862427\n","Step: 95487, loss: 0.04555879160761833\n","Step: 95488, loss: 0.16551902890205383\n","Step: 95489, loss: 0.08462808281183243\n","Step: 95490, loss: 0.034697357565164566\n","Step: 95491, loss: 0.3979901075363159\n","Step: 95492, loss: 0.0\n","Step: 95493, loss: 0.22452764213085175\n","Step: 95494, loss: 0.0917668342590332\n","Step: 95495, loss: 0.05007721111178398\n","Step: 95496, loss: 0.11799356341362\n","Step: 95497, loss: 0.06626687198877335\n","Step: 95498, loss: 0.1126491129398346\n","Step: 95499, loss: 0.039568062871694565\n","Step: 95500, loss: 0.34494510293006897\n","Step: 95501, loss: 0.051544439047575\n","Step: 95502, loss: 0.0\n","Step: 95503, loss: 0.151264950633049\n","Step: 95504, loss: 0.09153226763010025\n","Step: 95505, loss: 0.2477317452430725\n","Step: 95506, loss: 0.20957279205322266\n","Step: 95507, loss: 0.1879948377609253\n","Step: 95508, loss: 0.0\n","Step: 95509, loss: 0.1413055956363678\n","Step: 95510, loss: 0.0\n","Step: 95511, loss: 0.0\n","Step: 95512, loss: 0.0\n","Step: 95513, loss: 0.08946840465068817\n","Step: 95514, loss: 0.11773678660392761\n","Step: 95515, loss: 0.052287664264440536\n","Step: 95516, loss: 0.25510090589523315\n","Step: 95517, loss: 0.036173224449157715\n","Step: 95518, loss: 0.30082032084465027\n","Step: 95519, loss: 0.13648147881031036\n","Step: 95520, loss: 0.07671039551496506\n","Step: 95521, loss: 0.0\n","Step: 95522, loss: 0.27437642216682434\n","Step: 95523, loss: 0.1502797156572342\n","Step: 95524, loss: 0.17862626910209656\n","Step: 95525, loss: 0.03214450180530548\n","Step: 95526, loss: 0.04493958503007889\n","Step: 95527, loss: 0.17108333110809326\n","Step: 95528, loss: 0.0932488888502121\n","Step: 95529, loss: 0.3504200875759125\n","Step: 95530, loss: 0.09455029666423798\n","Step: 95531, loss: 0.1142059713602066\n","Step: 95532, loss: 0.09754535555839539\n","Step: 95533, loss: 0.0\n","Step: 95534, loss: 0.09640581160783768\n","Step: 95535, loss: 0.2764052152633667\n","Step: 95536, loss: 0.2344408482313156\n","Step: 95537, loss: 0.05409569665789604\n","Step: 95538, loss: 0.029612071812152863\n","Step: 95539, loss: 0.17993706464767456\n","Step: 95540, loss: 0.1113680750131607\n","Step: 95541, loss: 0.0835692286491394\n","Step: 95542, loss: 0.1511269211769104\n","Step: 95543, loss: 0.20036432147026062\n","Step: 95544, loss: 0.0964977815747261\n","Step: 95545, loss: 0.08102526515722275\n","Step: 95546, loss: 0.0\n","Step: 95547, loss: 0.18362712860107422\n","Step: 95548, loss: 0.05191568285226822\n","Step: 95549, loss: 0.05223449692130089\n","Step: 95550, loss: 0.037600964307785034\n","Step: 95551, loss: 0.08989811688661575\n","Step: 95552, loss: 0.06728304177522659\n","Step: 95553, loss: 0.06620895117521286\n","Step: 95554, loss: 0.10706379264593124\n","Step: 95555, loss: 0.07320509850978851\n","Step: 95556, loss: 0.14990760385990143\n","Step: 95557, loss: 0.25029894709587097\n","Step: 95558, loss: 0.10147387534379959\n","Step: 95559, loss: 0.05375291779637337\n","Step: 95560, loss: 0.09205947071313858\n","Step: 95561, loss: 0.12867452204227448\n","Step: 95562, loss: 0.031457412987947464\n","Step: 95563, loss: 0.0\n","Step: 95564, loss: 0.20269913971424103\n","Step: 95565, loss: 0.08074934780597687\n","Step: 95566, loss: 0.17153622210025787\n","Step: 95567, loss: 0.06405780464410782\n","Step: 95568, loss: 0.12268982827663422\n","Step: 95569, loss: 0.07800517976284027\n","Step: 95570, loss: 0.10358233749866486\n","Step: 95571, loss: 0.31365543603897095\n","Step: 95572, loss: 0.39340007305145264\n","Step: 95573, loss: 0.04750481992959976\n","Step: 95574, loss: 0.0\n","Step: 95575, loss: 0.27160006761550903\n","Step: 95576, loss: 0.159682959318161\n","Step: 95577, loss: 0.11145393550395966\n","Step: 95578, loss: 0.14376072585582733\n","Step: 95579, loss: 0.04655654728412628\n","Step: 95580, loss: 0.08064518123865128\n","Step: 95581, loss: 0.0756787583231926\n","Step: 95582, loss: 0.19363407790660858\n","Step: 95583, loss: 0.12592348456382751\n","Step: 95584, loss: 0.176007479429245\n","Step: 95585, loss: 0.07807821035385132\n","Step: 95586, loss: 0.04494008421897888\n","Step: 95587, loss: 0.028600439429283142\n","Step: 95588, loss: 0.11263907700777054\n","Step: 95589, loss: 0.17811687290668488\n","Step: 95590, loss: 0.11201195418834686\n","Step: 95591, loss: 0.197529137134552\n","Step: 95592, loss: 0.1739974170923233\n","Step: 95593, loss: 0.16776058077812195\n","Step: 95594, loss: 0.08844532072544098\n","Step: 95595, loss: 0.0\n","Step: 95596, loss: 0.04954035207629204\n","Step: 95597, loss: 0.11460278183221817\n","Step: 95598, loss: 0.08611355721950531\n","Step: 95599, loss: 0.04389261081814766\n","Step: 95600, loss: 0.04495130479335785\n","Step: 95601, loss: 0.19390200078487396\n","Step: 95602, loss: 0.11844515800476074\n","Step: 95603, loss: 0.08928894996643066\n","Step: 95604, loss: 0.10810258984565735\n","Step: 95605, loss: 0.1263347864151001\n","Step: 95606, loss: 0.04168829321861267\n","Step: 95607, loss: 0.22233280539512634\n","Step: 95608, loss: 0.10765015333890915\n","Step: 95609, loss: 0.17178915441036224\n","Step: 95610, loss: 0.20749904215335846\n","Step: 95611, loss: 0.3418172001838684\n","Step: 95612, loss: 0.05573571100831032\n","Step: 95613, loss: 0.0927986204624176\n","Step: 95614, loss: 0.12355146557092667\n","Step: 95615, loss: 0.1635386347770691\n","Step: 95616, loss: 0.08545688539743423\n","Step: 95617, loss: 0.054545510560274124\n","Step: 95618, loss: 0.28667184710502625\n","Step: 95619, loss: 0.0897594466805458\n","Step: 95620, loss: 0.10534702241420746\n","Step: 95621, loss: 0.2104177474975586\n","Step: 95622, loss: 0.040153149515390396\n","Step: 95623, loss: 0.043548207730054855\n","Step: 95624, loss: 0.04478465020656586\n","Step: 95625, loss: 0.09663277864456177\n","Step: 95626, loss: 0.21248137950897217\n","Step: 95627, loss: 0.13640868663787842\n","Step: 95628, loss: 0.1298362761735916\n","Step: 95629, loss: 0.12552715837955475\n","Step: 95630, loss: 0.27100422978401184\n","Step: 95631, loss: 0.27151384949684143\n","Step: 95632, loss: 0.08957690000534058\n","Step: 95633, loss: 0.07994293421506882\n","Step: 95634, loss: 0.11382956802845001\n","Step: 95635, loss: 0.09930647164583206\n","Step: 95636, loss: 0.05276627838611603\n","Step: 95637, loss: 0.10050259530544281\n","Step: 95638, loss: 0.05390467494726181\n","Step: 95639, loss: 0.04293013736605644\n","Step: 95640, loss: 0.07431063801050186\n","Step: 95641, loss: 0.12629494071006775\n","Step: 95642, loss: 0.030623285099864006\n","Step: 95643, loss: 0.12308140844106674\n","Step: 95644, loss: 0.19766563177108765\n","Step: 95645, loss: 0.1345038264989853\n","Step: 95646, loss: 0.08200839906930923\n","Step: 95647, loss: 0.08829868584871292\n","Step: 95648, loss: 0.10468216240406036\n","Step: 95649, loss: 0.21219013631343842\n","Step: 95650, loss: 0.19139181077480316\n","Step: 95651, loss: 0.168568953871727\n","Step: 95652, loss: 0.1324356347322464\n","Step: 95653, loss: 0.03062010370194912\n","Step: 95654, loss: 0.3006334602832794\n","Step: 95655, loss: 0.11772867292165756\n","Step: 95656, loss: 0.13861043751239777\n","Step: 95657, loss: 0.1193540021777153\n","Step: 95658, loss: 0.20648294687271118\n","Step: 95659, loss: 0.20296159386634827\n","Step: 95660, loss: 0.2770809829235077\n","Step: 95661, loss: 0.04837696626782417\n","Step: 95662, loss: 0.0991479903459549\n","Step: 95663, loss: 0.04523361101746559\n","Step: 95664, loss: 0.0\n","Step: 95665, loss: 0.08799770474433899\n","Step: 95666, loss: 0.22089919447898865\n","Step: 95667, loss: 0.03099561296403408\n","Step: 95668, loss: 0.04386197403073311\n","Step: 95669, loss: 0.030865250155329704\n","Step: 95670, loss: 0.27230891585350037\n","Step: 95671, loss: 0.10148201882839203\n","Step: 95672, loss: 0.21926380693912506\n","Step: 95673, loss: 0.0\n","Step: 95674, loss: 0.21101075410842896\n","Step: 95675, loss: 0.09929768741130829\n","Step: 95676, loss: 0.2409639209508896\n","Step: 95677, loss: 0.029339129105210304\n","Step: 95678, loss: 0.35266441106796265\n","Step: 95679, loss: 0.04655754193663597\n","Step: 95680, loss: 0.0507308766245842\n","Step: 95681, loss: 0.22461943328380585\n","Step: 95682, loss: 0.20483940839767456\n","Step: 95683, loss: 0.029676353558897972\n","Step: 95684, loss: 0.07559802383184433\n","Step: 95685, loss: 0.04815467819571495\n","Step: 95686, loss: 0.073249451816082\n","Step: 95687, loss: 0.11382748931646347\n","Step: 95688, loss: 0.33282357454299927\n","Step: 95689, loss: 0.11262736469507217\n","Step: 95690, loss: 0.3191867470741272\n","Step: 95691, loss: 0.12030685693025589\n","Step: 95692, loss: 0.20900237560272217\n","Step: 95693, loss: 0.22236615419387817\n","Step: 95694, loss: 0.1590607911348343\n","Step: 95695, loss: 0.20508937537670135\n","Step: 95696, loss: 0.19891533255577087\n","Step: 95697, loss: 0.043491180986166\n","Step: 95698, loss: 0.04825952649116516\n","Step: 95699, loss: 0.08533120900392532\n","Step: 95700, loss: 0.08804645389318466\n","Step: 95701, loss: 0.0\n","Step: 95702, loss: 0.1239212155342102\n","Step: 95703, loss: 0.127772256731987\n","Step: 95704, loss: 0.24739624559879303\n","Step: 95705, loss: 0.04732918739318848\n","Step: 95706, loss: 0.1317213773727417\n","Step: 95707, loss: 0.16269615292549133\n","Step: 95708, loss: 0.04213709011673927\n","Step: 95709, loss: 0.23281852900981903\n","Step: 95710, loss: 0.0\n","Step: 95711, loss: 0.23705407977104187\n","Step: 95712, loss: 0.26714539527893066\n","Step: 95713, loss: 0.31998389959335327\n","Step: 95714, loss: 0.0\n","Step: 95715, loss: 0.322665810585022\n","Step: 95716, loss: 0.045079320669174194\n","Step: 95717, loss: 0.07360117137432098\n","Step: 95718, loss: 0.14518950879573822\n","Step: 95719, loss: 0.1986793577671051\n","Step: 95720, loss: 0.07414615899324417\n","Step: 95721, loss: 0.05244424194097519\n","Step: 95722, loss: 0.08758959919214249\n","Step: 95723, loss: 0.0\n","Step: 95724, loss: 0.2711608409881592\n","Step: 95725, loss: 0.16144542396068573\n","Step: 95726, loss: 0.0\n","Step: 95727, loss: 0.04585461691021919\n","Step: 95728, loss: 0.31184902787208557\n","Step: 95729, loss: 0.14376921951770782\n","Step: 95730, loss: 0.13768181204795837\n","Step: 95731, loss: 0.09748640656471252\n","Step: 95732, loss: 0.09405184537172318\n","Step: 95733, loss: 0.14594954252243042\n","Step: 95734, loss: 0.19761821627616882\n","Step: 95735, loss: 0.13062125444412231\n","Step: 95736, loss: 0.04002329334616661\n","Step: 95737, loss: 0.03987054526805878\n","Step: 95738, loss: 0.1291218400001526\n","Step: 95739, loss: 0.0613301657140255\n","Step: 95740, loss: 0.1512487381696701\n","Step: 95741, loss: 0.0\n","Step: 95742, loss: 0.05267343670129776\n","Step: 95743, loss: 0.17223209142684937\n","Step: 95744, loss: 0.0\n","Step: 95745, loss: 0.1765875369310379\n","Step: 95746, loss: 0.091251440346241\n","Step: 95747, loss: 0.25602930784225464\n","Step: 95748, loss: 0.13576357066631317\n","Step: 95749, loss: 0.0\n","Step: 95750, loss: 0.1489923894405365\n","Step: 95751, loss: 0.09619799256324768\n","Step: 95752, loss: 0.12962615489959717\n","Step: 95753, loss: 0.2080015242099762\n","Step: 95754, loss: 0.19150705635547638\n","Step: 95755, loss: 0.11518480628728867\n","Step: 95756, loss: 0.0\n","Step: 95757, loss: 0.1536278873682022\n","Step: 95758, loss: 0.118703693151474\n","Step: 95759, loss: 0.21720169484615326\n","Step: 95760, loss: 0.07735685259103775\n","Step: 95761, loss: 0.11875306814908981\n","Step: 95762, loss: 0.04892517253756523\n","Step: 95763, loss: 0.07853145152330399\n","Step: 95764, loss: 0.27559611201286316\n","Step: 95765, loss: 0.14891193807125092\n","Step: 95766, loss: 0.0\n","Step: 95767, loss: 0.12279213964939117\n","Step: 95768, loss: 0.08018855005502701\n","Step: 95769, loss: 0.08303284645080566\n","Step: 95770, loss: 0.039719365537166595\n","Step: 95771, loss: 0.10969801992177963\n","Step: 95772, loss: 0.2458326369524002\n","Step: 95773, loss: 0.2084549367427826\n","Step: 95774, loss: 0.02829265594482422\n","Step: 95775, loss: 0.0\n","Step: 95776, loss: 0.0472240224480629\n","Step: 95777, loss: 0.1017146185040474\n","Step: 95778, loss: 0.21101699769496918\n","Step: 95779, loss: 0.0\n","Step: 95780, loss: 0.16516807675361633\n","Step: 95781, loss: 0.045563068240880966\n","Step: 95782, loss: 0.04829072952270508\n","Step: 95783, loss: 0.0\n","Step: 95784, loss: 0.3416256010532379\n","Step: 95785, loss: 0.08154283463954926\n","Step: 95786, loss: 0.07420507073402405\n","Step: 95787, loss: 0.2113482654094696\n","Step: 95788, loss: 0.22630131244659424\n","Step: 95789, loss: 0.06457051634788513\n","Step: 95790, loss: 0.07449033111333847\n","Step: 95791, loss: 0.14473100006580353\n","Step: 95792, loss: 0.0738183856010437\n","Step: 95793, loss: 0.02984258159995079\n","Step: 95794, loss: 0.36255159974098206\n","Step: 95795, loss: 0.10940705239772797\n","Step: 95796, loss: 0.041017185896635056\n","Step: 95797, loss: 0.03573049604892731\n","Step: 95798, loss: 0.029536383226513863\n","Step: 95799, loss: 0.26949524879455566\n","Step: 95800, loss: 0.11969266831874847\n","Step: 95801, loss: 0.3636423945426941\n","Step: 95802, loss: 0.0\n","Step: 95803, loss: 0.09201119095087051\n","Step: 95804, loss: 0.16084685921669006\n","Step: 95805, loss: 0.11883221566677094\n","Step: 95806, loss: 0.22559309005737305\n","Step: 95807, loss: 0.19901397824287415\n","Step: 95808, loss: 0.0\n","Step: 95809, loss: 0.12451539933681488\n","Step: 95810, loss: 0.1503726989030838\n","Step: 95811, loss: 0.0\n","Step: 95812, loss: 0.1860324889421463\n","Step: 95813, loss: 0.12765373289585114\n","Step: 95814, loss: 0.08485034108161926\n","Step: 95815, loss: 0.16817504167556763\n","Step: 95816, loss: 0.0880606546998024\n","Step: 95817, loss: 0.11785782128572464\n","Step: 95818, loss: 0.2401803731918335\n","Step: 95819, loss: 0.2164059430360794\n","Step: 95820, loss: 0.06561146676540375\n","Step: 95821, loss: 0.15407323837280273\n","Step: 95822, loss: 0.19048279523849487\n","Step: 95823, loss: 0.2949454188346863\n","Step: 95824, loss: 0.06639572978019714\n","Step: 95825, loss: 0.15402233600616455\n","Step: 95826, loss: 0.04922894015908241\n","Step: 95827, loss: 0.10526739060878754\n","Step: 95828, loss: 0.25507453083992004\n","Step: 95829, loss: 0.2835044860839844\n","Step: 95830, loss: 0.12272367626428604\n","Step: 95831, loss: 0.12914280593395233\n","Step: 95832, loss: 0.1444854587316513\n","Step: 95833, loss: 0.08586505800485611\n","Step: 95834, loss: 0.1760719269514084\n","Step: 95835, loss: 0.1852305382490158\n","Step: 95836, loss: 0.027703385800123215\n","Step: 95837, loss: 0.0\n","Step: 95838, loss: 0.06680318713188171\n","Step: 95839, loss: 0.1871722936630249\n","Step: 95840, loss: 0.0836620032787323\n","Step: 95841, loss: 0.05833995342254639\n","Step: 95842, loss: 0.18169327080249786\n","Step: 95843, loss: 0.22276580333709717\n","Step: 95844, loss: 0.3537537455558777\n","Step: 95845, loss: 0.21981880068778992\n","Step: 95846, loss: 0.06104005500674248\n","Step: 95847, loss: 0.1316184550523758\n","Step: 95848, loss: 0.08798166364431381\n","Step: 95849, loss: 0.06504543125629425\n","Step: 95850, loss: 0.327627569437027\n","Step: 95851, loss: 0.03321341425180435\n","Step: 95852, loss: 0.0\n","Step: 95853, loss: 0.18532009422779083\n","Step: 95854, loss: 0.10605841875076294\n","Step: 95855, loss: 0.04842115566134453\n","Step: 95856, loss: 0.18324771523475647\n","Step: 95857, loss: 0.12067321687936783\n","Step: 95858, loss: 0.1433926522731781\n","Step: 95859, loss: 0.10006861388683319\n","Step: 95860, loss: 0.05361723527312279\n","Step: 95861, loss: 0.03555408865213394\n","Step: 95862, loss: 0.12547113001346588\n","Step: 95863, loss: 0.3487186133861542\n","Step: 95864, loss: 0.22687162458896637\n","Step: 95865, loss: 0.10909084975719452\n","Step: 95866, loss: 0.0\n","Step: 95867, loss: 0.09285633265972137\n","Step: 95868, loss: 0.15849830210208893\n","Step: 95869, loss: 0.0645177811384201\n","Step: 95870, loss: 0.15067483484745026\n","Step: 95871, loss: 0.10611739009618759\n","Step: 95872, loss: 0.30434808135032654\n","Step: 95873, loss: 0.03441726788878441\n","Step: 95874, loss: 0.11966272443532944\n","Step: 95875, loss: 0.03342706710100174\n","Step: 95876, loss: 0.22689181566238403\n","Step: 95877, loss: 0.07312483340501785\n","Step: 95878, loss: 0.044755373150110245\n","Step: 95879, loss: 0.06924398243427277\n","Step: 95880, loss: 0.04385805502533913\n","Step: 95881, loss: 0.28922969102859497\n","Step: 95882, loss: 0.11739140748977661\n","Step: 95883, loss: 0.1097903922200203\n","Step: 95884, loss: 0.21900048851966858\n","Step: 95885, loss: 0.09934687614440918\n","Step: 95886, loss: 0.15462321043014526\n","Step: 95887, loss: 0.0\n","Step: 95888, loss: 0.11020660400390625\n","Step: 95889, loss: 0.0\n","Step: 95890, loss: 0.19875392317771912\n","Step: 95891, loss: 0.030705662444233894\n","Step: 95892, loss: 0.1751261204481125\n","Step: 95893, loss: 0.14717495441436768\n","Step: 95894, loss: 0.030248232185840607\n","Step: 95895, loss: 0.0\n","Step: 95896, loss: 0.11946049332618713\n","Step: 95897, loss: 0.10720556974411011\n","Step: 95898, loss: 0.08004222065210342\n","Step: 95899, loss: 0.1386750191450119\n","Step: 95900, loss: 0.3000625967979431\n","Step: 95901, loss: 0.0\n","Step: 95902, loss: 0.3390374481678009\n","Step: 95903, loss: 0.0\n","Step: 95904, loss: 0.14097535610198975\n","Step: 95905, loss: 0.18268154561519623\n","Step: 95906, loss: 0.06091989949345589\n","Step: 95907, loss: 0.027557116001844406\n","Step: 95908, loss: 0.2014050930738449\n","Step: 95909, loss: 0.19541168212890625\n","Step: 95910, loss: 0.09548450261354446\n","Step: 95911, loss: 0.07810036838054657\n","Step: 95912, loss: 0.18187835812568665\n","Step: 95913, loss: 0.30136099457740784\n","Step: 95914, loss: 0.16141277551651\n","Step: 95915, loss: 0.08310410380363464\n","Step: 95916, loss: 0.15075235068798065\n","Step: 95917, loss: 0.05558624118566513\n","Step: 95918, loss: 0.08387055993080139\n","Step: 95919, loss: 0.1346152424812317\n","Step: 95920, loss: 0.0\n","Step: 95921, loss: 0.1999872624874115\n","Step: 95922, loss: 0.20109374821186066\n","Step: 95923, loss: 0.0653783529996872\n","Step: 95924, loss: 0.12790034711360931\n","Step: 95925, loss: 0.14054521918296814\n","Step: 95926, loss: 0.2044050544500351\n","Step: 95927, loss: 0.153302863240242\n","Step: 95928, loss: 0.029127540066838264\n","Step: 95929, loss: 0.180748850107193\n","Step: 95930, loss: 0.0\n","Step: 95931, loss: 0.09048490226268768\n","Step: 95932, loss: 0.1874416321516037\n","Step: 95933, loss: 0.11309383064508438\n","Step: 95934, loss: 0.10851576924324036\n","Step: 95935, loss: 0.07631532847881317\n","Step: 95936, loss: 0.07667414844036102\n","Step: 95937, loss: 0.17741809785366058\n","Step: 95938, loss: 0.3307970464229584\n","Step: 95939, loss: 0.05840574577450752\n","Step: 95940, loss: 0.15714429318904877\n","Step: 95941, loss: 0.0\n","Step: 95942, loss: 0.134134903550148\n","Step: 95943, loss: 0.18808533251285553\n","Step: 95944, loss: 0.12704429030418396\n","Step: 95945, loss: 0.057923536747694016\n","Step: 95946, loss: 0.19976554811000824\n","Step: 95947, loss: 0.1885872632265091\n","Step: 95948, loss: 0.14508956670761108\n","Step: 95949, loss: 0.07714442908763885\n","Step: 95950, loss: 0.10320141911506653\n","Step: 95951, loss: 0.11530573666095734\n","Step: 95952, loss: 0.08388909697532654\n","Step: 95953, loss: 0.03663504496216774\n","Step: 95954, loss: 0.08472684025764465\n","Step: 95955, loss: 0.03720034658908844\n","Step: 95956, loss: 0.16720731556415558\n","Step: 95957, loss: 0.08356969803571701\n","Step: 95958, loss: 0.2611072063446045\n","Step: 95959, loss: 0.09485948830842972\n","Step: 95960, loss: 0.1857660710811615\n","Step: 95961, loss: 0.09801643341779709\n","Step: 95962, loss: 0.04646596312522888\n","Step: 95963, loss: 0.18236389756202698\n","Step: 95964, loss: 0.04187119007110596\n","Step: 95965, loss: 0.10316357016563416\n","Step: 95966, loss: 0.32752156257629395\n","Step: 95967, loss: 0.0\n","Step: 95968, loss: 0.043825797736644745\n","Step: 95969, loss: 0.0328836590051651\n","Step: 95970, loss: 0.10339576005935669\n","Step: 95971, loss: 0.1658327430486679\n","Step: 95972, loss: 0.09842793643474579\n","Step: 95973, loss: 0.19037647545337677\n","Step: 95974, loss: 0.047648657113313675\n","Step: 95975, loss: 0.18721485137939453\n","Step: 95976, loss: 0.1530689299106598\n","Step: 95977, loss: 0.08964380621910095\n","Step: 95978, loss: 0.043449047952890396\n","Step: 95979, loss: 0.19429263472557068\n","Step: 95980, loss: 0.03174005448818207\n","Step: 95981, loss: 0.11298727244138718\n","Step: 95982, loss: 0.031501077115535736\n","Step: 95983, loss: 0.15294873714447021\n","Step: 95984, loss: 0.14609262347221375\n","Step: 95985, loss: 0.10960918664932251\n","Step: 95986, loss: 0.0\n","Step: 95987, loss: 0.15731972455978394\n","Step: 95988, loss: 0.4223219156265259\n","Step: 95989, loss: 0.0708862841129303\n","Step: 95990, loss: 0.20158952474594116\n","Step: 95991, loss: 0.0\n","Step: 95992, loss: 0.18780474364757538\n","Step: 95993, loss: 0.2982112765312195\n","Step: 95994, loss: 0.0\n","Step: 95995, loss: 0.14798064529895782\n","Step: 95996, loss: 0.18005920946598053\n","Step: 95997, loss: 0.10983003675937653\n","Step: 95998, loss: 0.048066433519124985\n","Step: 95999, loss: 0.1144617572426796\n","Step: 96000, loss: 0.03943362087011337\n","Step: 96001, loss: 0.28658363223075867\n","Step: 96002, loss: 0.039610546082258224\n","Step: 96003, loss: 0.07481878995895386\n","Step: 96004, loss: 0.1009170264005661\n","Step: 96005, loss: 0.1919410675764084\n","Step: 96006, loss: 0.20030909776687622\n","Step: 96007, loss: 0.22213001549243927\n","Step: 96008, loss: 0.03562067449092865\n","Step: 96009, loss: 0.17601272463798523\n","Step: 96010, loss: 0.16811300814151764\n","Step: 96011, loss: 0.1174192875623703\n","Step: 96012, loss: 0.0\n","Step: 96013, loss: 0.374726265668869\n","Step: 96014, loss: 0.03773088380694389\n","Step: 96015, loss: 0.12154746800661087\n","Step: 96016, loss: 0.15942157804965973\n","Step: 96017, loss: 0.041461411863565445\n","Step: 96018, loss: 0.2074393332004547\n","Step: 96019, loss: 0.11771368235349655\n","Step: 96020, loss: 0.037691641598939896\n","Step: 96021, loss: 0.0\n","Step: 96022, loss: 0.20238208770751953\n","Step: 96023, loss: 0.24038779735565186\n","Step: 96024, loss: 0.24595576524734497\n","Step: 96025, loss: 0.3707946836948395\n","Step: 96026, loss: 0.03594856336712837\n","Step: 96027, loss: 0.1935134083032608\n","Step: 96028, loss: 0.1065717414021492\n","Step: 96029, loss: 0.07469729334115982\n","Step: 96030, loss: 0.06946050375699997\n","Step: 96031, loss: 0.35471081733703613\n","Step: 96032, loss: 0.13878624141216278\n","Step: 96033, loss: 0.03711953014135361\n","Step: 96034, loss: 0.0\n","Step: 96035, loss: 0.037307966500520706\n","Step: 96036, loss: 0.12265868484973907\n","Step: 96037, loss: 0.28369805216789246\n","Step: 96038, loss: 0.046094320714473724\n","Step: 96039, loss: 0.1382010132074356\n","Step: 96040, loss: 0.0328882560133934\n","Step: 96041, loss: 0.17180901765823364\n","Step: 96042, loss: 0.0\n","Step: 96043, loss: 0.09511545300483704\n","Step: 96044, loss: 0.16623260080814362\n","Step: 96045, loss: 0.28238144516944885\n","Step: 96046, loss: 0.0904172733426094\n","Step: 96047, loss: 0.41276076436042786\n","Step: 96048, loss: 0.18244396150112152\n","Step: 96049, loss: 0.03803788498044014\n","Step: 96050, loss: 0.10783541947603226\n","Step: 96051, loss: 0.03481408953666687\n","Step: 96052, loss: 0.06997889280319214\n","Step: 96053, loss: 0.08502764999866486\n","Step: 96054, loss: 0.20164218544960022\n","Step: 96055, loss: 0.0414574034512043\n","Step: 96056, loss: 0.3102577328681946\n","Step: 96057, loss: 0.040669139474630356\n","Step: 96058, loss: 0.1532287448644638\n","Step: 96059, loss: 0.08511518687009811\n","Step: 96060, loss: 0.17290829122066498\n","Step: 96061, loss: 0.05773263797163963\n","Step: 96062, loss: 0.11813382804393768\n","Step: 96063, loss: 0.21034926176071167\n","Step: 96064, loss: 0.16340522468090057\n","Step: 96065, loss: 0.19306644797325134\n","Step: 96066, loss: 0.07093210518360138\n","Step: 96067, loss: 0.21977069973945618\n","Step: 96068, loss: 0.07075393944978714\n","Step: 96069, loss: 0.2397288829088211\n","Step: 96070, loss: 0.20952093601226807\n","Step: 96071, loss: 0.034476734697818756\n","Step: 96072, loss: 0.21844364702701569\n","Step: 96073, loss: 0.11282693594694138\n","Step: 96074, loss: 0.08309610188007355\n","Step: 96075, loss: 0.11088251322507858\n","Step: 96076, loss: 0.07408807426691055\n","Step: 96077, loss: 0.04936504364013672\n","Step: 96078, loss: 0.1914140284061432\n","Step: 96079, loss: 0.0\n","Step: 96080, loss: 0.1295960545539856\n","Step: 96081, loss: 0.0\n","Step: 96082, loss: 0.21106189489364624\n","Step: 96083, loss: 0.04022252559661865\n","Step: 96084, loss: 0.08751396834850311\n","Step: 96085, loss: 0.12410957366228104\n","Step: 96086, loss: 0.12245358526706696\n","Step: 96087, loss: 0.04708453640341759\n","Step: 96088, loss: 0.2915710508823395\n","Step: 96089, loss: 0.08775267004966736\n","Step: 96090, loss: 0.0\n","Step: 96091, loss: 0.07304500788450241\n","Step: 96092, loss: 0.12229996919631958\n","Step: 96093, loss: 0.09996240586042404\n","Step: 96094, loss: 0.11094113439321518\n","Step: 96095, loss: 0.13186751306056976\n","Step: 96096, loss: 0.0419435054063797\n","Step: 96097, loss: 0.21773697435855865\n","Step: 96098, loss: 0.0\n","Step: 96099, loss: 0.042316824197769165\n","Step: 96100, loss: 0.0\n","Step: 96101, loss: 0.03735574334859848\n","Step: 96102, loss: 0.03748198598623276\n","Step: 96103, loss: 0.08476799726486206\n","Step: 96104, loss: 0.1609552949666977\n","Step: 96105, loss: 0.0\n","Step: 96106, loss: 0.07698202133178711\n","Step: 96107, loss: 0.07378514111042023\n","Step: 96108, loss: 0.3135146200656891\n","Step: 96109, loss: 0.03563953936100006\n","Step: 96110, loss: 0.24618259072303772\n","Step: 96111, loss: 0.0985671728849411\n","Step: 96112, loss: 0.04134422913193703\n","Step: 96113, loss: 0.15607944130897522\n","Step: 96114, loss: 0.0\n","Step: 96115, loss: 0.1646956205368042\n","Step: 96116, loss: 0.0756467878818512\n","Step: 96117, loss: 0.12616246938705444\n","Step: 96118, loss: 0.19959430396556854\n","Step: 96119, loss: 0.32438936829566956\n","Step: 96120, loss: 0.23512473702430725\n","Step: 96121, loss: 0.0\n","Step: 96122, loss: 0.05414840579032898\n","Step: 96123, loss: 0.057342737913131714\n","Step: 96124, loss: 0.09652438759803772\n","Step: 96125, loss: 0.05506356060504913\n","Step: 96126, loss: 0.1929427683353424\n","Step: 96127, loss: 0.13574190437793732\n","Step: 96128, loss: 0.12843364477157593\n","Step: 96129, loss: 0.09614118933677673\n","Step: 96130, loss: 0.0\n","Step: 96131, loss: 0.17270980775356293\n","Step: 96132, loss: 0.17410828173160553\n","Step: 96133, loss: 0.02549760602414608\n","Step: 96134, loss: 0.04379485175013542\n","Step: 96135, loss: 0.0\n","Step: 96136, loss: 0.12720827758312225\n","Step: 96137, loss: 0.1285470873117447\n","Step: 96138, loss: 0.19516421854496002\n","Step: 96139, loss: 0.14504696428775787\n","Step: 96140, loss: 0.16239452362060547\n","Step: 96141, loss: 0.0\n","Step: 96142, loss: 0.25091904401779175\n","Step: 96143, loss: 0.07521312683820724\n","Step: 96144, loss: 0.12241723388433456\n","Step: 96145, loss: 0.08583526313304901\n","Step: 96146, loss: 0.13233987987041473\n","Step: 96147, loss: 0.2480102926492691\n","Step: 96148, loss: 0.0946742594242096\n","Step: 96149, loss: 0.16507716476917267\n","Step: 96150, loss: 0.2330484241247177\n","Step: 96151, loss: 0.09791459888219833\n","Step: 96152, loss: 0.23344460129737854\n","Step: 96153, loss: 0.0\n","Step: 96154, loss: 0.10797632485628128\n","Step: 96155, loss: 0.2695046365261078\n","Step: 96156, loss: 0.22068767249584198\n","Step: 96157, loss: 0.2267926186323166\n","Step: 96158, loss: 0.2235228717327118\n","Step: 96159, loss: 0.08484792709350586\n","Step: 96160, loss: 0.17300796508789062\n","Step: 96161, loss: 0.13205021619796753\n","Step: 96162, loss: 0.0\n","Step: 96163, loss: 0.13385798037052155\n","Step: 96164, loss: 0.19170492887496948\n","Step: 96165, loss: 0.04991588741540909\n","Step: 96166, loss: 0.1544152945280075\n","Step: 96167, loss: 0.13969001173973083\n","Step: 96168, loss: 0.07844451069831848\n","Step: 96169, loss: 0.08235710859298706\n","Step: 96170, loss: 0.17455902695655823\n","Step: 96171, loss: 0.19507110118865967\n","Step: 96172, loss: 0.27171996235847473\n","Step: 96173, loss: 0.07957547903060913\n","Step: 96174, loss: 0.19678638875484467\n","Step: 96175, loss: 0.14317551255226135\n","Step: 96176, loss: 0.15983827412128448\n","Step: 96177, loss: 0.17881596088409424\n","Step: 96178, loss: 0.07629337161779404\n","Step: 96179, loss: 0.17585590481758118\n","Step: 96180, loss: 0.09994108974933624\n","Step: 96181, loss: 0.09815805405378342\n","Step: 96182, loss: 0.035085372626781464\n","Step: 96183, loss: 0.21460658311843872\n","Step: 96184, loss: 0.15261369943618774\n","Step: 96185, loss: 0.054402608424425125\n","Step: 96186, loss: 0.08375772833824158\n","Step: 96187, loss: 0.13070236146450043\n","Step: 96188, loss: 0.19605889916419983\n","Step: 96189, loss: 0.0\n","Step: 96190, loss: 0.29486289620399475\n","Step: 96191, loss: 0.14553314447402954\n","Step: 96192, loss: 0.0758439153432846\n","Step: 96193, loss: 0.11225409805774689\n","Step: 96194, loss: 0.12723542749881744\n","Step: 96195, loss: 0.1368364840745926\n","Step: 96196, loss: 0.16079181432724\n","Step: 96197, loss: 0.11470487713813782\n","Step: 96198, loss: 0.0\n","Step: 96199, loss: 0.1307717263698578\n","Step: 96200, loss: 0.18454048037528992\n","Step: 96201, loss: 0.10463793575763702\n","Step: 96202, loss: 0.16949346661567688\n","Step: 96203, loss: 0.07846084237098694\n","Step: 96204, loss: 0.08603890240192413\n","Step: 96205, loss: 0.1315111666917801\n","Step: 96206, loss: 0.036367811262607574\n","Step: 96207, loss: 0.3279953896999359\n","Step: 96208, loss: 0.0\n","Step: 96209, loss: 0.0931890457868576\n","Step: 96210, loss: 0.0\n","Step: 96211, loss: 0.10524888336658478\n","Step: 96212, loss: 0.21656079590320587\n","Step: 96213, loss: 0.11600179970264435\n","Step: 96214, loss: 0.11505335569381714\n","Step: 96215, loss: 0.13766787946224213\n","Step: 96216, loss: 0.03397131338715553\n","Step: 96217, loss: 0.2305884212255478\n","Step: 96218, loss: 0.1435742825269699\n","Step: 96219, loss: 0.2535874545574188\n","Step: 96220, loss: 0.13561801612377167\n","Step: 96221, loss: 0.12405376136302948\n","Step: 96222, loss: 0.2580195963382721\n","Step: 96223, loss: 0.17266429960727692\n","Step: 96224, loss: 0.12758246064186096\n","Step: 96225, loss: 0.2208261936903\n","Step: 96226, loss: 0.10087762027978897\n","Step: 96227, loss: 0.0\n","Step: 96228, loss: 0.3068660497665405\n","Step: 96229, loss: 0.2565438449382782\n","Step: 96230, loss: 0.09212794899940491\n","Step: 96231, loss: 0.23783378303050995\n","Step: 96232, loss: 0.07479698956012726\n","Step: 96233, loss: 0.10170622169971466\n","Step: 96234, loss: 0.12554813921451569\n","Step: 96235, loss: 0.0654895156621933\n","Step: 96236, loss: 0.18838809430599213\n","Step: 96237, loss: 0.13003745675086975\n","Step: 96238, loss: 0.13128036260604858\n","Step: 96239, loss: 0.048068296164274216\n","Step: 96240, loss: 0.15525098145008087\n","Step: 96241, loss: 0.11769571155309677\n","Step: 96242, loss: 0.14062750339508057\n","Step: 96243, loss: 0.1315871775150299\n","Step: 96244, loss: 0.02769532985985279\n","Step: 96245, loss: 0.33013594150543213\n","Step: 96246, loss: 0.09507438540458679\n","Step: 96247, loss: 0.1666988581418991\n","Step: 96248, loss: 0.054724425077438354\n","Step: 96249, loss: 0.07833284139633179\n","Step: 96250, loss: 0.10103987902402878\n","Step: 96251, loss: 0.2725384831428528\n","Step: 96252, loss: 0.1528380960226059\n","Step: 96253, loss: 0.16699939966201782\n","Step: 96254, loss: 0.08335372805595398\n","Step: 96255, loss: 0.31653130054473877\n","Step: 96256, loss: 0.09909254312515259\n","Step: 96257, loss: 0.04843117669224739\n","Step: 96258, loss: 0.24375998973846436\n","Step: 96259, loss: 0.1202838346362114\n","Step: 96260, loss: 0.14126481115818024\n","Step: 96261, loss: 0.10644613951444626\n","Step: 96262, loss: 0.2931511700153351\n","Step: 96263, loss: 0.10688525438308716\n","Step: 96264, loss: 0.0\n","Step: 96265, loss: 0.09110529720783234\n","Step: 96266, loss: 0.1269896924495697\n","Step: 96267, loss: 0.0\n","Step: 96268, loss: 0.11966221034526825\n","Step: 96269, loss: 0.0\n","Step: 96270, loss: 0.18677031993865967\n","Step: 96271, loss: 0.22203129529953003\n","Step: 96272, loss: 0.0\n","Step: 96273, loss: 0.12249550223350525\n","Step: 96274, loss: 0.23418869078159332\n","Step: 96275, loss: 0.39809107780456543\n","Step: 96276, loss: 0.09022802114486694\n","Step: 96277, loss: 0.16204658150672913\n","Step: 96278, loss: 0.048030126839876175\n","Step: 96279, loss: 0.21063478291034698\n","Step: 96280, loss: 0.11295435577630997\n","Step: 96281, loss: 0.033607929944992065\n","Step: 96282, loss: 0.05410614609718323\n","Step: 96283, loss: 0.06584781408309937\n","Step: 96284, loss: 0.12286213040351868\n","Step: 96285, loss: 0.0333341509103775\n","Step: 96286, loss: 0.0\n","Step: 96287, loss: 0.19270889461040497\n","Step: 96288, loss: 0.1461150199174881\n","Step: 96289, loss: 0.3209210932254791\n","Step: 96290, loss: 0.09475283324718475\n","Step: 96291, loss: 0.11685129255056381\n","Step: 96292, loss: 0.14659395813941956\n","Step: 96293, loss: 0.1282091587781906\n","Step: 96294, loss: 0.0\n","Step: 96295, loss: 0.14294928312301636\n","Step: 96296, loss: 0.16377557814121246\n","Step: 96297, loss: 0.27943333983421326\n","Step: 96298, loss: 0.21472346782684326\n","Step: 96299, loss: 0.09224000573158264\n","Step: 96300, loss: 0.2870224714279175\n","Step: 96301, loss: 0.08303119987249374\n","Step: 96302, loss: 0.10007593035697937\n","Step: 96303, loss: 0.15608227252960205\n","Step: 96304, loss: 0.0\n","Step: 96305, loss: 0.0\n","Step: 96306, loss: 0.06044311821460724\n","Step: 96307, loss: 0.21980342268943787\n","Step: 96308, loss: 0.12293531745672226\n","Step: 96309, loss: 0.02886081486940384\n","Step: 96310, loss: 0.05210644379258156\n","Step: 96311, loss: 0.04951022192835808\n","Step: 96312, loss: 0.14282220602035522\n","Step: 96313, loss: 0.027202587574720383\n","Step: 96314, loss: 0.09693017601966858\n","Step: 96315, loss: 0.1319933980703354\n","Step: 96316, loss: 0.3938615322113037\n","Step: 96317, loss: 0.2115788608789444\n","Step: 96318, loss: 0.14991901814937592\n","Step: 96319, loss: 0.1663511097431183\n","Step: 96320, loss: 0.19897548854351044\n","Step: 96321, loss: 0.07929740846157074\n","Step: 96322, loss: 0.3877212405204773\n","Step: 96323, loss: 0.350403368473053\n","Step: 96324, loss: 0.33985671401023865\n","Step: 96325, loss: 0.17056691646575928\n","Step: 96326, loss: 0.12438735365867615\n","Step: 96327, loss: 0.07175523042678833\n","Step: 96328, loss: 0.0\n","Step: 96329, loss: 0.052555300295352936\n","Step: 96330, loss: 0.35539883375167847\n","Step: 96331, loss: 0.03225108981132507\n","Step: 96332, loss: 0.33045825362205505\n","Step: 96333, loss: 0.04902242496609688\n","Step: 96334, loss: 0.11056193709373474\n","Step: 96335, loss: 0.06988293677568436\n","Step: 96336, loss: 0.1110878437757492\n","Step: 96337, loss: 0.04526593163609505\n","Step: 96338, loss: 0.04396846890449524\n","Step: 96339, loss: 0.09058195352554321\n","Step: 96340, loss: 0.21408626437187195\n","Step: 96341, loss: 0.20080077648162842\n","Step: 96342, loss: 0.08042053878307343\n","Step: 96343, loss: 0.16087184846401215\n","Step: 96344, loss: 0.07201226055622101\n","Step: 96345, loss: 0.08043667674064636\n","Step: 96346, loss: 0.044664546847343445\n","Step: 96347, loss: 0.07872767746448517\n","Step: 96348, loss: 0.10423034429550171\n","Step: 96349, loss: 0.21283413469791412\n","Step: 96350, loss: 0.20497699081897736\n","Step: 96351, loss: 0.083221435546875\n","Step: 96352, loss: 0.09867434203624725\n","Step: 96353, loss: 0.0\n","Step: 96354, loss: 0.1145782545208931\n","Step: 96355, loss: 0.0\n","Step: 96356, loss: 0.028854088857769966\n","Step: 96357, loss: 0.25519615411758423\n","Step: 96358, loss: 0.13859982788562775\n","Step: 96359, loss: 0.09291089326143265\n","Step: 96360, loss: 0.0496908575296402\n","Step: 96361, loss: 0.12713059782981873\n","Step: 96362, loss: 0.046351108700037\n","Step: 96363, loss: 0.0\n","Step: 96364, loss: 0.0870649591088295\n","Step: 96365, loss: 0.22567492723464966\n","Step: 96366, loss: 0.0\n","Step: 96367, loss: 0.1919332891702652\n","Step: 96368, loss: 0.067286416888237\n","Step: 96369, loss: 0.23654764890670776\n","Step: 96370, loss: 0.12657834589481354\n","Step: 96371, loss: 0.1893410086631775\n","Step: 96372, loss: 0.13752050697803497\n","Step: 96373, loss: 0.09710098057985306\n","Step: 96374, loss: 0.031978439539670944\n","Step: 96375, loss: 0.24165380001068115\n","Step: 96376, loss: 0.12523941695690155\n","Step: 96377, loss: 0.1266830563545227\n","Step: 96378, loss: 0.0\n","Step: 96379, loss: 0.09098171442747116\n","Step: 96380, loss: 0.15861055254936218\n","Step: 96381, loss: 0.09232598543167114\n","Step: 96382, loss: 0.0\n","Step: 96383, loss: 0.0\n","Step: 96384, loss: 0.14651905000209808\n","Step: 96385, loss: 0.17449074983596802\n","Step: 96386, loss: 0.1681516468524933\n","Step: 96387, loss: 0.11475147306919098\n","Step: 96388, loss: 0.18231771886348724\n","Step: 96389, loss: 0.08492426574230194\n","Step: 96390, loss: 0.17907385528087616\n","Step: 96391, loss: 0.1927223652601242\n","Step: 96392, loss: 0.0829070582985878\n","Step: 96393, loss: 0.08601147681474686\n","Step: 96394, loss: 0.25146445631980896\n","Step: 96395, loss: 0.09573778510093689\n","Step: 96396, loss: 0.17897777259349823\n","Step: 96397, loss: 0.2745856046676636\n","Step: 96398, loss: 0.13144274055957794\n","Step: 96399, loss: 0.12760144472122192\n","Step: 96400, loss: 0.1949380487203598\n","Step: 96401, loss: 0.15413713455200195\n","Step: 96402, loss: 0.0\n","Step: 96403, loss: 0.12851005792617798\n","Step: 96404, loss: 0.29201918840408325\n","Step: 96405, loss: 0.16727977991104126\n","Step: 96406, loss: 0.1610361635684967\n","Step: 96407, loss: 0.0681227594614029\n","Step: 96408, loss: 0.08694785833358765\n","Step: 96409, loss: 0.24950391054153442\n","Step: 96410, loss: 0.0\n","Step: 96411, loss: 0.1859569400548935\n","Step: 96412, loss: 0.08173742890357971\n","Step: 96413, loss: 0.12158247083425522\n","Step: 96414, loss: 0.13214614987373352\n","Step: 96415, loss: 0.2635657787322998\n","Step: 96416, loss: 0.14967544376850128\n","Step: 96417, loss: 0.0\n","Step: 96418, loss: 0.2182920277118683\n","Step: 96419, loss: 0.12447939068078995\n","Step: 96420, loss: 0.06909700483083725\n","Step: 96421, loss: 0.22002384066581726\n","Step: 96422, loss: 0.135844424366951\n","Step: 96423, loss: 0.0733848437666893\n","Step: 96424, loss: 0.13278257846832275\n","Step: 96425, loss: 0.37946373224258423\n","Step: 96426, loss: 0.2088232934474945\n","Step: 96427, loss: 0.24888122081756592\n","Step: 96428, loss: 0.07554268091917038\n","Step: 96429, loss: 0.12835556268692017\n","Step: 96430, loss: 0.16234150528907776\n","Step: 96431, loss: 0.04400962218642235\n","Step: 96432, loss: 0.19842121005058289\n","Step: 96433, loss: 0.11132857948541641\n","Step: 96434, loss: 0.19698093831539154\n","Step: 96435, loss: 0.07621553540229797\n","Step: 96436, loss: 0.22451047599315643\n","Step: 96437, loss: 0.09282311797142029\n","Step: 96438, loss: 0.20120403170585632\n","Step: 96439, loss: 0.1637955904006958\n","Step: 96440, loss: 0.13069742918014526\n","Step: 96441, loss: 0.2573010325431824\n","Step: 96442, loss: 0.07800433784723282\n","Step: 96443, loss: 0.0\n","Step: 96444, loss: 0.032939113676548004\n","Step: 96445, loss: 0.16038784384727478\n","Step: 96446, loss: 0.13902482390403748\n","Step: 96447, loss: 0.0839477926492691\n","Step: 96448, loss: 0.23151573538780212\n","Step: 96449, loss: 0.15113312005996704\n","Step: 96450, loss: 0.030337361618876457\n","Step: 96451, loss: 0.04501735046505928\n","Step: 96452, loss: 0.21388782560825348\n","Step: 96453, loss: 0.051421795040369034\n","Step: 96454, loss: 0.04206451028585434\n","Step: 96455, loss: 0.0808388814330101\n","Step: 96456, loss: 0.13777104020118713\n","Step: 96457, loss: 0.18836216628551483\n","Step: 96458, loss: 0.0\n","Step: 96459, loss: 0.09779585152864456\n","Step: 96460, loss: 0.0\n","Step: 96461, loss: 0.15984110534191132\n","Step: 96462, loss: 0.0\n","Step: 96463, loss: 0.30007463693618774\n","Step: 96464, loss: 0.1030060201883316\n","Step: 96465, loss: 0.03366028517484665\n","Step: 96466, loss: 0.11263374239206314\n","Step: 96467, loss: 0.11970390379428864\n","Step: 96468, loss: 0.203083798289299\n","Step: 96469, loss: 0.13229982554912567\n","Step: 96470, loss: 0.2136930525302887\n","Step: 96471, loss: 0.0\n","Step: 96472, loss: 0.0900205448269844\n","Step: 96473, loss: 0.07031827419996262\n","Step: 96474, loss: 0.1634911596775055\n","Step: 96475, loss: 0.15839368104934692\n","Step: 96476, loss: 0.07917234301567078\n","Step: 96477, loss: 0.09677369147539139\n","Step: 96478, loss: 0.16973777115345\n","Step: 96479, loss: 0.1737479269504547\n","Step: 96480, loss: 0.15135756134986877\n","Step: 96481, loss: 0.18795615434646606\n","Step: 96482, loss: 0.11658071726560593\n","Step: 96483, loss: 0.043095313012599945\n","Step: 96484, loss: 0.32867953181266785\n","Step: 96485, loss: 0.23482777178287506\n","Step: 96486, loss: 0.0\n","Step: 96487, loss: 0.048945728689432144\n","Step: 96488, loss: 0.16291896998882294\n","Step: 96489, loss: 0.0\n","Step: 96490, loss: 0.09484606981277466\n","Step: 96491, loss: 0.048270177096128464\n","Step: 96492, loss: 0.11665437370538712\n","Step: 96493, loss: 0.1697831004858017\n","Step: 96494, loss: 0.18623356521129608\n","Step: 96495, loss: 0.12816639244556427\n","Step: 96496, loss: 0.07133443653583527\n","Step: 96497, loss: 0.08171769976615906\n","Step: 96498, loss: 0.25766900181770325\n","Step: 96499, loss: 0.10728327929973602\n","Step: 96500, loss: 0.17071343958377838\n","Step: 96501, loss: 0.1504284292459488\n","Step: 96502, loss: 0.14770379662513733\n","Step: 96503, loss: 0.11263708770275116\n","Step: 96504, loss: 0.0\n","Step: 96505, loss: 0.2519732117652893\n","Step: 96506, loss: 0.10149682313203812\n","Step: 96507, loss: 0.04936111345887184\n","Step: 96508, loss: 0.18899264931678772\n","Step: 96509, loss: 0.07764274626970291\n","Step: 96510, loss: 0.23297175765037537\n","Step: 96511, loss: 0.34275662899017334\n","Step: 96512, loss: 0.13482508063316345\n","Step: 96513, loss: 0.14305725693702698\n","Step: 96514, loss: 0.20957699418067932\n","Step: 96515, loss: 0.14372596144676208\n","Step: 96516, loss: 0.038768645375967026\n","Step: 96517, loss: 0.086652472615242\n","Step: 96518, loss: 0.036293305456638336\n","Step: 96519, loss: 0.07302563637495041\n","Step: 96520, loss: 0.1448245793581009\n","Step: 96521, loss: 0.27473559975624084\n","Step: 96522, loss: 0.19265834987163544\n","Step: 96523, loss: 0.0\n","Step: 96524, loss: 0.43581026792526245\n","Step: 96525, loss: 0.1903267502784729\n","Step: 96526, loss: 0.08283315598964691\n","Step: 96527, loss: 0.03829917311668396\n","Step: 96528, loss: 0.04676137492060661\n","Step: 96529, loss: 0.1529569774866104\n","Step: 96530, loss: 0.2310684621334076\n","Step: 96531, loss: 0.07030947506427765\n","Step: 96532, loss: 0.09882201999425888\n","Step: 96533, loss: 0.0\n","Step: 96534, loss: 0.07894404232501984\n","Step: 96535, loss: 0.13811008632183075\n","Step: 96536, loss: 0.10831291973590851\n","Step: 96537, loss: 0.16423211991786957\n","Step: 96538, loss: 0.046258870512247086\n","Step: 96539, loss: 0.17116087675094604\n","Step: 96540, loss: 0.3311848044395447\n","Step: 96541, loss: 0.0\n","Step: 96542, loss: 0.2791951894760132\n","Step: 96543, loss: 0.1929030865430832\n","Step: 96544, loss: 0.12089578807353973\n","Step: 96545, loss: 0.18776309490203857\n","Step: 96546, loss: 0.1924830824136734\n","Step: 96547, loss: 0.11578206717967987\n","Step: 96548, loss: 0.17728576064109802\n","Step: 96549, loss: 0.0362200029194355\n","Step: 96550, loss: 0.13521243631839752\n","Step: 96551, loss: 0.0\n","Step: 96552, loss: 0.13577131927013397\n","Step: 96553, loss: 0.052082281559705734\n","Step: 96554, loss: 0.0\n","Step: 96555, loss: 0.0\n","Step: 96556, loss: 0.12582913041114807\n","Step: 96557, loss: 0.04053834453225136\n","Step: 96558, loss: 0.0\n","Step: 96559, loss: 0.25876373052597046\n","Step: 96560, loss: 0.0\n","Step: 96561, loss: 0.21321040391921997\n","Step: 96562, loss: 0.11372727155685425\n","Step: 96563, loss: 0.06977654993534088\n","Step: 96564, loss: 0.12086953967809677\n","Step: 96565, loss: 0.13055305182933807\n","Step: 96566, loss: 0.15465788543224335\n","Step: 96567, loss: 0.17085646092891693\n","Step: 96568, loss: 0.07751215249300003\n","Step: 96569, loss: 0.0\n","Step: 96570, loss: 0.22613415122032166\n","Step: 96571, loss: 0.22910156846046448\n","Step: 96572, loss: 0.21645347774028778\n","Step: 96573, loss: 0.0\n","Step: 96574, loss: 0.18493223190307617\n","Step: 96575, loss: 0.11062461882829666\n","Step: 96576, loss: 0.27158427238464355\n","Step: 96577, loss: 0.33529266715049744\n","Step: 96578, loss: 0.16966457664966583\n","Step: 96579, loss: 0.14187639951705933\n","Step: 96580, loss: 0.23119904100894928\n","Step: 96581, loss: 0.10146025568246841\n","Step: 96582, loss: 0.1466130018234253\n","Step: 96583, loss: 0.029244303703308105\n","Step: 96584, loss: 0.13969004154205322\n","Step: 96585, loss: 0.028421446681022644\n","Step: 96586, loss: 0.0\n","Step: 96587, loss: 0.09662298858165741\n","Step: 96588, loss: 0.44764432311058044\n","Step: 96589, loss: 0.0948789194226265\n","Step: 96590, loss: 0.12416455894708633\n","Step: 96591, loss: 0.14831386506557465\n","Step: 96592, loss: 0.20128177106380463\n","Step: 96593, loss: 0.055735863745212555\n","Step: 96594, loss: 0.0\n","Step: 96595, loss: 0.07887285947799683\n","Step: 96596, loss: 0.2632863521575928\n","Step: 96597, loss: 0.06410004943609238\n","Step: 96598, loss: 0.12154142558574677\n","Step: 96599, loss: 0.25690409541130066\n","Step: 96600, loss: 0.16727261245250702\n","Step: 96601, loss: 0.12631064653396606\n","Step: 96602, loss: 0.15470825135707855\n","Step: 96603, loss: 0.12830527126789093\n","Step: 96604, loss: 0.1985231339931488\n","Step: 96605, loss: 0.18465758860111237\n","Step: 96606, loss: 0.0\n","Step: 96607, loss: 0.20997373759746552\n","Step: 96608, loss: 0.0\n","Step: 96609, loss: 0.026598766446113586\n","Step: 96610, loss: 0.048779621720314026\n","Step: 96611, loss: 0.22479981184005737\n","Step: 96612, loss: 0.1511489450931549\n","Step: 96613, loss: 0.20615051686763763\n","Step: 96614, loss: 0.19422298669815063\n","Step: 96615, loss: 0.0\n","Step: 96616, loss: 0.23841777443885803\n","Step: 96617, loss: 0.16825567185878754\n","Step: 96618, loss: 0.046244774013757706\n","Step: 96619, loss: 0.09217242151498795\n","Step: 96620, loss: 0.0847630724310875\n","Step: 96621, loss: 0.08088432997465134\n","Step: 96622, loss: 0.40146830677986145\n","Step: 96623, loss: 0.12827104330062866\n","Step: 96624, loss: 0.13051927089691162\n","Step: 96625, loss: 0.11779844015836716\n","Step: 96626, loss: 0.09908955544233322\n","Step: 96627, loss: 0.24049502611160278\n","Step: 96628, loss: 0.06553933024406433\n","Step: 96629, loss: 0.1575087308883667\n","Step: 96630, loss: 0.0\n","Step: 96631, loss: 0.19606199860572815\n","Step: 96632, loss: 0.1491650640964508\n","Step: 96633, loss: 0.0\n","Step: 96634, loss: 0.2897111177444458\n","Step: 96635, loss: 0.153604656457901\n","Step: 96636, loss: 0.0\n","Step: 96637, loss: 0.15806517004966736\n","Step: 96638, loss: 0.15164603292942047\n","Step: 96639, loss: 0.13582855463027954\n","Step: 96640, loss: 0.09738562256097794\n","Step: 96641, loss: 0.1060769334435463\n","Step: 96642, loss: 0.10869573801755905\n","Step: 96643, loss: 0.05132359638810158\n","Step: 96644, loss: 0.05025751516222954\n","Step: 96645, loss: 0.08317039161920547\n","Step: 96646, loss: 0.30499467253685\n","Step: 96647, loss: 0.09236888587474823\n","Step: 96648, loss: 0.0395934097468853\n","Step: 96649, loss: 0.11309326440095901\n","Step: 96650, loss: 0.17051684856414795\n","Step: 96651, loss: 0.13398022949695587\n","Step: 96652, loss: 0.186369851231575\n","Step: 96653, loss: 0.2019520252943039\n","Step: 96654, loss: 0.2522609233856201\n","Step: 96655, loss: 0.08569138497114182\n","Step: 96656, loss: 0.22862491011619568\n","Step: 96657, loss: 0.21662893891334534\n","Step: 96658, loss: 0.13987980782985687\n","Step: 96659, loss: 0.20271991193294525\n","Step: 96660, loss: 0.08584253489971161\n","Step: 96661, loss: 0.070980966091156\n","Step: 96662, loss: 0.03511516749858856\n","Step: 96663, loss: 0.08315736055374146\n","Step: 96664, loss: 0.08267971873283386\n","Step: 96665, loss: 0.2653484046459198\n","Step: 96666, loss: 0.16675783693790436\n","Step: 96667, loss: 0.23827478289604187\n","Step: 96668, loss: 0.08506117761135101\n","Step: 96669, loss: 0.10271625220775604\n","Step: 96670, loss: 0.1410713642835617\n","Step: 96671, loss: 0.17509551346302032\n","Step: 96672, loss: 0.17937465012073517\n","Step: 96673, loss: 0.0\n","Step: 96674, loss: 0.1557355672121048\n","Step: 96675, loss: 0.1623062640428543\n","Step: 96676, loss: 0.0\n","Step: 96677, loss: 0.06149750575423241\n","Step: 96678, loss: 0.11845087260007858\n","Step: 96679, loss: 0.12046074122190475\n","Step: 96680, loss: 0.05964069813489914\n","Step: 96681, loss: 0.030953826382756233\n","Step: 96682, loss: 0.09126809239387512\n","Step: 96683, loss: 0.0\n","Step: 96684, loss: 0.17583869397640228\n","Step: 96685, loss: 0.0810152217745781\n","Step: 96686, loss: 0.14871151745319366\n","Step: 96687, loss: 0.08121759444475174\n","Step: 96688, loss: 0.13806098699569702\n","Step: 96689, loss: 0.0\n","Step: 96690, loss: 0.26421183347702026\n","Step: 96691, loss: 0.20218606293201447\n","Step: 96692, loss: 0.1594562828540802\n","Step: 96693, loss: 0.19815008342266083\n","Step: 96694, loss: 0.103211410343647\n","Step: 96695, loss: 0.09421095252037048\n","Step: 96696, loss: 0.10638318955898285\n","Step: 96697, loss: 0.0990583673119545\n","Step: 96698, loss: 0.1138320341706276\n","Step: 96699, loss: 0.0786035805940628\n","Step: 96700, loss: 0.23390713334083557\n","Step: 96701, loss: 0.0\n","Step: 96702, loss: 0.028387220576405525\n","Step: 96703, loss: 0.07519190013408661\n","Step: 96704, loss: 0.14665810763835907\n","Step: 96705, loss: 0.08438056707382202\n","Step: 96706, loss: 0.07591956108808517\n","Step: 96707, loss: 0.0\n","Step: 96708, loss: 0.1987023800611496\n","Step: 96709, loss: 0.1356346309185028\n","Step: 96710, loss: 0.10612854361534119\n","Step: 96711, loss: 0.051929835230112076\n","Step: 96712, loss: 0.09822803735733032\n","Step: 96713, loss: 0.23012946546077728\n","Step: 96714, loss: 0.0\n","Step: 96715, loss: 0.0397113636136055\n","Step: 96716, loss: 0.2457384318113327\n","Step: 96717, loss: 0.23658426105976105\n","Step: 96718, loss: 0.0\n","Step: 96719, loss: 0.09842764586210251\n","Step: 96720, loss: 0.057795826345682144\n","Step: 96721, loss: 0.11495858430862427\n","Step: 96722, loss: 0.0783161148428917\n","Step: 96723, loss: 0.07643868774175644\n","Step: 96724, loss: 0.16143110394477844\n","Step: 96725, loss: 0.0\n","Step: 96726, loss: 0.18611225485801697\n","Step: 96727, loss: 0.12361982464790344\n","Step: 96728, loss: 0.11892880499362946\n","Step: 96729, loss: 0.24093827605247498\n","Step: 96730, loss: 0.06348786503076553\n","Step: 96731, loss: 0.03159645199775696\n","Step: 96732, loss: 0.29430362582206726\n","Step: 96733, loss: 0.22166915237903595\n","Step: 96734, loss: 0.14457644522190094\n","Step: 96735, loss: 0.1188589483499527\n","Step: 96736, loss: 0.1324923038482666\n","Step: 96737, loss: 0.29534199833869934\n","Step: 96738, loss: 0.04776917025446892\n","Step: 96739, loss: 0.2102336287498474\n","Step: 96740, loss: 0.08919231593608856\n","Step: 96741, loss: 0.031948961317539215\n","Step: 96742, loss: 0.0\n","Step: 96743, loss: 0.18827839195728302\n","Step: 96744, loss: 0.05206473171710968\n","Step: 96745, loss: 0.07641293853521347\n","Step: 96746, loss: 0.1536475121974945\n","Step: 96747, loss: 0.17102521657943726\n","Step: 96748, loss: 0.15299318730831146\n","Step: 96749, loss: 0.04981179162859917\n","Step: 96750, loss: 0.20625430345535278\n","Step: 96751, loss: 0.1524808257818222\n","Step: 96752, loss: 0.1234651580452919\n","Step: 96753, loss: 0.1190304085612297\n","Step: 96754, loss: 0.1199236586689949\n","Step: 96755, loss: 0.18278102576732635\n","Step: 96756, loss: 0.22838512063026428\n","Step: 96757, loss: 0.2318466603755951\n","Step: 96758, loss: 0.20664240419864655\n","Step: 96759, loss: 0.11403582245111465\n","Step: 96760, loss: 0.13384367525577545\n","Step: 96761, loss: 0.19453571736812592\n","Step: 96762, loss: 0.053672704845666885\n","Step: 96763, loss: 0.16895776987075806\n","Step: 96764, loss: 0.0823020339012146\n","Step: 96765, loss: 0.0\n","Step: 96766, loss: 0.21019384264945984\n","Step: 96767, loss: 0.04703211784362793\n","Step: 96768, loss: 0.13792067766189575\n","Step: 96769, loss: 0.0\n","Step: 96770, loss: 0.12041278928518295\n","Step: 96771, loss: 0.27937039732933044\n","Step: 96772, loss: 0.17738491296768188\n","Step: 96773, loss: 0.06081699952483177\n","Step: 96774, loss: 0.035549864172935486\n","Step: 96775, loss: 0.2670667767524719\n","Step: 96776, loss: 0.1625153124332428\n","Step: 96777, loss: 0.140353724360466\n","Step: 96778, loss: 0.12203134596347809\n","Step: 96779, loss: 0.13344354927539825\n","Step: 96780, loss: 0.0834302306175232\n","Step: 96781, loss: 0.12476399540901184\n","Step: 96782, loss: 0.2358909249305725\n","Step: 96783, loss: 0.1718188226222992\n","Step: 96784, loss: 0.045306216925382614\n","Step: 96785, loss: 0.0999898612499237\n","Step: 96786, loss: 0.3899826109409332\n","Step: 96787, loss: 0.10746420174837112\n","Step: 96788, loss: 0.11410993337631226\n","Step: 96789, loss: 0.16697853803634644\n","Step: 96790, loss: 0.14959630370140076\n","Step: 96791, loss: 0.04057618975639343\n","Step: 96792, loss: 0.12998618185520172\n","Step: 96793, loss: 0.08613967150449753\n","Step: 96794, loss: 0.14406566321849823\n","Step: 96795, loss: 0.09761849045753479\n","Step: 96796, loss: 0.0\n","Step: 96797, loss: 0.1549423635005951\n","Step: 96798, loss: 0.08502793312072754\n","Step: 96799, loss: 0.043863870203495026\n","Step: 96800, loss: 0.0440807119011879\n","Step: 96801, loss: 0.2023158222436905\n","Step: 96802, loss: 0.11274614185094833\n","Step: 96803, loss: 0.08237642049789429\n","Step: 96804, loss: 0.18240685760974884\n","Step: 96805, loss: 0.08398666232824326\n","Step: 96806, loss: 0.17925406992435455\n","Step: 96807, loss: 0.037027034908533096\n","Step: 96808, loss: 0.21576078236103058\n","Step: 96809, loss: 0.07330615073442459\n","Step: 96810, loss: 0.19864463806152344\n","Step: 96811, loss: 0.07115964591503143\n","Step: 96812, loss: 0.05261152237653732\n","Step: 96813, loss: 0.07929453998804092\n","Step: 96814, loss: 0.12161701172590256\n","Step: 96815, loss: 0.12644065916538239\n","Step: 96816, loss: 0.27849438786506653\n","Step: 96817, loss: 0.09694217890501022\n","Step: 96818, loss: 0.2569374740123749\n","Step: 96819, loss: 0.17498177289962769\n","Step: 96820, loss: 0.19642764329910278\n","Step: 96821, loss: 0.2767665386199951\n","Step: 96822, loss: 0.12792019546031952\n","Step: 96823, loss: 0.1677299439907074\n","Step: 96824, loss: 0.0795854926109314\n","Step: 96825, loss: 0.1626259982585907\n","Step: 96826, loss: 0.03288816288113594\n","Step: 96827, loss: 0.11993880569934845\n","Step: 96828, loss: 0.04410345107316971\n","Step: 96829, loss: 0.22230187058448792\n","Step: 96830, loss: 0.14283709228038788\n","Step: 96831, loss: 0.2607329487800598\n","Step: 96832, loss: 0.1083446815609932\n","Step: 96833, loss: 0.22460094094276428\n","Step: 96834, loss: 0.042739659547805786\n","Step: 96835, loss: 0.10320982336997986\n","Step: 96836, loss: 0.30016613006591797\n","Step: 96837, loss: 0.04326016455888748\n","Step: 96838, loss: 0.15140245854854584\n","Step: 96839, loss: 0.2209520787000656\n","Step: 96840, loss: 0.15580005943775177\n","Step: 96841, loss: 0.12618659436702728\n","Step: 96842, loss: 0.11825216561555862\n","Step: 96843, loss: 0.2061130553483963\n","Step: 96844, loss: 0.030368752777576447\n","Step: 96845, loss: 0.19278492033481598\n","Step: 96846, loss: 0.044152893126010895\n","Step: 96847, loss: 0.1692962795495987\n","Step: 96848, loss: 0.08366329222917557\n","Step: 96849, loss: 0.1591029316186905\n","Step: 96850, loss: 0.12689340114593506\n","Step: 96851, loss: 0.16311392188072205\n","Step: 96852, loss: 0.3211050033569336\n","Step: 96853, loss: 0.07300178706645966\n","Step: 96854, loss: 0.25478851795196533\n","Step: 96855, loss: 0.06517019867897034\n","Step: 96856, loss: 0.10459664463996887\n","Step: 96857, loss: 0.12462732195854187\n","Step: 96858, loss: 0.0\n","Step: 96859, loss: 0.04679097235202789\n","Step: 96860, loss: 0.2585617005825043\n","Step: 96861, loss: 0.1021728441119194\n","Step: 96862, loss: 0.11765068024396896\n","Step: 96863, loss: 0.13680118322372437\n","Step: 96864, loss: 0.2710646390914917\n","Step: 96865, loss: 0.12500634789466858\n","Step: 96866, loss: 0.2958996891975403\n","Step: 96867, loss: 0.10087046027183533\n","Step: 96868, loss: 0.0\n","Step: 96869, loss: 0.04932727664709091\n","Step: 96870, loss: 0.07650696486234665\n","Step: 96871, loss: 0.14671240746974945\n","Step: 96872, loss: 0.24793310463428497\n","Step: 96873, loss: 0.21993908286094666\n","Step: 96874, loss: 0.20749948918819427\n","Step: 96875, loss: 0.09391573816537857\n","Step: 96876, loss: 0.08653394877910614\n","Step: 96877, loss: 0.14596080780029297\n","Step: 96878, loss: 0.1042427122592926\n","Step: 96879, loss: 0.11177138239145279\n","Step: 96880, loss: 0.1723030060529709\n","Step: 96881, loss: 0.0259956493973732\n","Step: 96882, loss: 0.10028272867202759\n","Step: 96883, loss: 0.2174975723028183\n","Step: 96884, loss: 0.09096494317054749\n","Step: 96885, loss: 0.0490553081035614\n","Step: 96886, loss: 0.17009574174880981\n","Step: 96887, loss: 0.11705365031957626\n","Step: 96888, loss: 0.0\n","Step: 96889, loss: 0.11653822660446167\n","Step: 96890, loss: 0.16401806473731995\n","Step: 96891, loss: 0.30306476354599\n","Step: 96892, loss: 0.08305547386407852\n","Step: 96893, loss: 0.06456586718559265\n","Step: 96894, loss: 0.0\n","Step: 96895, loss: 0.058911848813295364\n","Step: 96896, loss: 0.049455877393484116\n","Step: 96897, loss: 0.05060696229338646\n","Step: 96898, loss: 0.0\n","Step: 96899, loss: 0.21646757423877716\n","Step: 96900, loss: 0.08265224099159241\n","Step: 96901, loss: 0.20027607679367065\n","Step: 96902, loss: 0.04443119838833809\n","Step: 96903, loss: 0.0\n","Step: 96904, loss: 0.0819128230214119\n","Step: 96905, loss: 0.08052675426006317\n","Step: 96906, loss: 0.044349245727062225\n","Step: 96907, loss: 0.19304049015045166\n","Step: 96908, loss: 0.17841559648513794\n","Step: 96909, loss: 0.19680042564868927\n","Step: 96910, loss: 0.17683689296245575\n","Step: 96911, loss: 0.15711553394794464\n","Step: 96912, loss: 0.1573120355606079\n","Step: 96913, loss: 0.1852557361125946\n","Step: 96914, loss: 0.11855356395244598\n","Step: 96915, loss: 0.048232149332761765\n","Step: 96916, loss: 0.11094304174184799\n","Step: 96917, loss: 0.2603367567062378\n","Step: 96918, loss: 0.14616522192955017\n","Step: 96919, loss: 0.09728650748729706\n","Step: 96920, loss: 0.04770398139953613\n","Step: 96921, loss: 0.07899346947669983\n","Step: 96922, loss: 0.03841214254498482\n","Step: 96923, loss: 0.12272750586271286\n","Step: 96924, loss: 0.07084911316633224\n","Step: 96925, loss: 0.1576242446899414\n","Step: 96926, loss: 0.0\n","Step: 96927, loss: 0.04266225919127464\n","Step: 96928, loss: 0.2354910969734192\n","Step: 96929, loss: 0.20650538802146912\n","Step: 96930, loss: 0.0\n","Step: 96931, loss: 0.29679566621780396\n","Step: 96932, loss: 0.08914269506931305\n","Step: 96933, loss: 0.04733561351895332\n","Step: 96934, loss: 0.0\n","Step: 96935, loss: 0.055793844163417816\n","Step: 96936, loss: 0.08285363018512726\n","Step: 96937, loss: 0.03718750178813934\n","Step: 96938, loss: 0.0\n","Step: 96939, loss: 0.0\n","Step: 96940, loss: 0.12585259974002838\n","Step: 96941, loss: 0.26691240072250366\n","Step: 96942, loss: 0.2693449854850769\n","Step: 96943, loss: 0.13798150420188904\n","Step: 96944, loss: 0.1257745325565338\n","Step: 96945, loss: 0.08412007987499237\n","Step: 96946, loss: 0.16842925548553467\n","Step: 96947, loss: 0.055902447551488876\n","Step: 96948, loss: 0.15784552693367004\n","Step: 96949, loss: 0.049016647040843964\n","Step: 96950, loss: 0.11735797673463821\n","Step: 96951, loss: 0.1325865387916565\n","Step: 96952, loss: 0.03525072708725929\n","Step: 96953, loss: 0.03967300429940224\n","Step: 96954, loss: 0.32873019576072693\n","Step: 96955, loss: 0.1175600215792656\n","Step: 96956, loss: 0.0753713995218277\n","Step: 96957, loss: 0.28802624344825745\n","Step: 96958, loss: 0.0911644771695137\n","Step: 96959, loss: 0.042840566486120224\n","Step: 96960, loss: 0.18536502122879028\n","Step: 96961, loss: 0.09343249350786209\n","Step: 96962, loss: 0.1427021026611328\n","Step: 96963, loss: 0.07274378091096878\n","Step: 96964, loss: 0.08114370703697205\n","Step: 96965, loss: 0.11762655526399612\n","Step: 96966, loss: 0.09489068388938904\n","Step: 96967, loss: 0.09260345250368118\n","Step: 96968, loss: 0.2645697593688965\n","Step: 96969, loss: 0.14109551906585693\n","Step: 96970, loss: 0.43667373061180115\n","Step: 96971, loss: 0.07428206503391266\n","Step: 96972, loss: 0.13868463039398193\n","Step: 96973, loss: 0.08744382113218307\n","Step: 96974, loss: 0.14767447113990784\n","Step: 96975, loss: 0.19489099085330963\n","Step: 96976, loss: 0.1278180629014969\n","Step: 96977, loss: 0.12364330887794495\n","Step: 96978, loss: 0.04857722297310829\n","Step: 96979, loss: 0.1262962967157364\n","Step: 96980, loss: 0.11640796810388565\n","Step: 96981, loss: 0.1321686953306198\n","Step: 96982, loss: 0.05618216469883919\n","Step: 96983, loss: 0.053437862545251846\n","Step: 96984, loss: 0.2531013488769531\n","Step: 96985, loss: 0.12247646600008011\n","Step: 96986, loss: 0.10121763497591019\n","Step: 96987, loss: 0.12158331274986267\n","Step: 96988, loss: 0.2722568213939667\n","Step: 96989, loss: 0.11678409576416016\n","Step: 96990, loss: 0.14176100492477417\n","Step: 96991, loss: 0.1050867810845375\n","Step: 96992, loss: 0.10519810020923615\n","Step: 96993, loss: 0.16530083119869232\n","Step: 96994, loss: 0.19502617418766022\n","Step: 96995, loss: 0.13902835547924042\n","Step: 96996, loss: 0.24818630516529083\n","Step: 96997, loss: 0.1824144423007965\n","Step: 96998, loss: 0.0\n","Step: 96999, loss: 0.025401733815670013\n","Step: 97000, loss: 0.14511282742023468\n","Step: 97001, loss: 0.02552986703813076\n","Step: 97002, loss: 0.06731938570737839\n","Step: 97003, loss: 0.30203142762184143\n","Step: 97004, loss: 0.1587374061346054\n","Step: 97005, loss: 0.0\n","Step: 97006, loss: 0.06839702278375626\n","Step: 97007, loss: 0.06789278239011765\n","Step: 97008, loss: 0.3731365501880646\n","Step: 97009, loss: 0.210451140999794\n","Step: 97010, loss: 0.0\n","Step: 97011, loss: 0.10799074918031693\n","Step: 97012, loss: 0.0\n","Step: 97013, loss: 0.13968171179294586\n","Step: 97014, loss: 0.13124677538871765\n","Step: 97015, loss: 0.09717882424592972\n","Step: 97016, loss: 0.03871893510222435\n","Step: 97017, loss: 0.09075480699539185\n","Step: 97018, loss: 0.12067005038261414\n","Step: 97019, loss: 0.14772053062915802\n","Step: 97020, loss: 0.23815755546092987\n","Step: 97021, loss: 0.11557044833898544\n","Step: 97022, loss: 0.0861276164650917\n","Step: 97023, loss: 0.15704068541526794\n","Step: 97024, loss: 0.21807484328746796\n","Step: 97025, loss: 0.11439197510480881\n","Step: 97026, loss: 0.05238833278417587\n","Step: 97027, loss: 0.14348912239074707\n","Step: 97028, loss: 0.08193421363830566\n","Step: 97029, loss: 0.17932924628257751\n","Step: 97030, loss: 0.17295822501182556\n","Step: 97031, loss: 0.029378047212958336\n","Step: 97032, loss: 0.08061356842517853\n","Step: 97033, loss: 0.0763741284608841\n","Step: 97034, loss: 0.047336455434560776\n","Step: 97035, loss: 0.13934586942195892\n","Step: 97036, loss: 0.10964158177375793\n","Step: 97037, loss: 0.045815326273441315\n","Step: 97038, loss: 0.3057307004928589\n","Step: 97039, loss: 0.10690845549106598\n","Step: 97040, loss: 0.0693352222442627\n","Step: 97041, loss: 0.04312889277935028\n","Step: 97042, loss: 0.19260965287685394\n","Step: 97043, loss: 0.08339996635913849\n","Step: 97044, loss: 0.04024292901158333\n","Step: 97045, loss: 0.14769935607910156\n","Step: 97046, loss: 0.0982339158654213\n","Step: 97047, loss: 0.16154766082763672\n","Step: 97048, loss: 0.18726177513599396\n","Step: 97049, loss: 0.072875015437603\n","Step: 97050, loss: 0.09876777976751328\n","Step: 97051, loss: 0.14582635462284088\n","Step: 97052, loss: 0.03266768902540207\n","Step: 97053, loss: 0.08007372170686722\n","Step: 97054, loss: 0.06806495785713196\n","Step: 97055, loss: 0.1866905242204666\n","Step: 97056, loss: 0.0\n","Step: 97057, loss: 0.1192336454987526\n","Step: 97058, loss: 0.04648297280073166\n","Step: 97059, loss: 0.12422019243240356\n","Step: 97060, loss: 0.0\n","Step: 97061, loss: 0.09201674163341522\n","Step: 97062, loss: 0.12638060748577118\n","Step: 97063, loss: 0.13545770943164825\n","Step: 97064, loss: 0.047418542206287384\n","Step: 97065, loss: 0.12264545261859894\n","Step: 97066, loss: 0.0\n","Step: 97067, loss: 0.15956079959869385\n","Step: 97068, loss: 0.33864811062812805\n","Step: 97069, loss: 0.03774736821651459\n","Step: 97070, loss: 0.08976422995328903\n","Step: 97071, loss: 0.18798881769180298\n","Step: 97072, loss: 0.1706094890832901\n","Step: 97073, loss: 0.15738575160503387\n","Step: 97074, loss: 0.029230201616883278\n","Step: 97075, loss: 0.13682691752910614\n","Step: 97076, loss: 0.24500051140785217\n","Step: 97077, loss: 0.10334359854459763\n","Step: 97078, loss: 0.17223431169986725\n","Step: 97079, loss: 0.24588114023208618\n","Step: 97080, loss: 0.2148035764694214\n","Step: 97081, loss: 0.04949932172894478\n","Step: 97082, loss: 0.15300527215003967\n","Step: 97083, loss: 0.2558329701423645\n","Step: 97084, loss: 0.2362576276063919\n","Step: 97085, loss: 0.0\n","Step: 97086, loss: 0.13685716688632965\n","Step: 97087, loss: 0.1682419776916504\n","Step: 97088, loss: 0.11810993403196335\n","Step: 97089, loss: 0.05180320888757706\n","Step: 97090, loss: 0.05106917396187782\n","Step: 97091, loss: 0.17554014921188354\n","Step: 97092, loss: 0.2654639184474945\n","Step: 97093, loss: 0.1506642997264862\n","Step: 97094, loss: 0.15181700885295868\n","Step: 97095, loss: 0.2040351927280426\n","Step: 97096, loss: 0.11936482787132263\n","Step: 97097, loss: 0.1818116009235382\n","Step: 97098, loss: 0.2597602903842926\n","Step: 97099, loss: 0.049372900277376175\n","Step: 97100, loss: 0.08845201134681702\n","Step: 97101, loss: 0.08155883103609085\n","Step: 97102, loss: 0.09303249418735504\n","Step: 97103, loss: 0.041570596396923065\n","Step: 97104, loss: 0.12149392068386078\n","Step: 97105, loss: 0.14003446698188782\n","Step: 97106, loss: 0.3829447031021118\n","Step: 97107, loss: 0.3526932895183563\n","Step: 97108, loss: 0.04092632606625557\n","Step: 97109, loss: 0.1352822482585907\n","Step: 97110, loss: 0.22871670126914978\n","Step: 97111, loss: 0.23499678075313568\n","Step: 97112, loss: 0.302264541387558\n","Step: 97113, loss: 0.15127207338809967\n","Step: 97114, loss: 0.09430503100156784\n","Step: 97115, loss: 0.16122853755950928\n","Step: 97116, loss: 0.14196088910102844\n","Step: 97117, loss: 0.1113051325082779\n","Step: 97118, loss: 0.12476526945829391\n","Step: 97119, loss: 0.1309509426355362\n","Step: 97120, loss: 0.0857834443449974\n","Step: 97121, loss: 0.17730700969696045\n","Step: 97122, loss: 0.2061743140220642\n","Step: 97123, loss: 0.05094681680202484\n","Step: 97124, loss: 0.12684942781925201\n","Step: 97125, loss: 0.0\n","Step: 97126, loss: 0.14363068342208862\n","Step: 97127, loss: 0.23185725510120392\n","Step: 97128, loss: 0.03821505606174469\n","Step: 97129, loss: 0.075810007750988\n","Step: 97130, loss: 0.12364868819713593\n","Step: 97131, loss: 0.0\n","Step: 97132, loss: 0.34022632241249084\n","Step: 97133, loss: 0.09664955735206604\n","Step: 97134, loss: 0.15057674050331116\n","Step: 97135, loss: 0.1487514078617096\n","Step: 97136, loss: 0.077509306371212\n","Step: 97137, loss: 0.05795377865433693\n","Step: 97138, loss: 0.045449551194906235\n","Step: 97139, loss: 0.057887669652700424\n","Step: 97140, loss: 0.23359932005405426\n","Step: 97141, loss: 0.08433040976524353\n","Step: 97142, loss: 0.08786626160144806\n","Step: 97143, loss: 0.07720673829317093\n","Step: 97144, loss: 0.0\n","Step: 97145, loss: 0.06069062277674675\n","Step: 97146, loss: 0.0372343510389328\n","Step: 97147, loss: 0.18460404872894287\n","Step: 97148, loss: 0.09116378426551819\n","Step: 97149, loss: 0.07672184705734253\n","Step: 97150, loss: 0.17554937303066254\n","Step: 97151, loss: 0.0\n","Step: 97152, loss: 0.2615359127521515\n","Step: 97153, loss: 0.2931886613368988\n","Step: 97154, loss: 0.1358167827129364\n","Step: 97155, loss: 0.08228835463523865\n","Step: 97156, loss: 0.15692616999149323\n","Step: 97157, loss: 0.08650865405797958\n","Step: 97158, loss: 0.10499875247478485\n","Step: 97159, loss: 0.055549584329128265\n","Step: 97160, loss: 0.08201224356889725\n","Step: 97161, loss: 0.12482760101556778\n","Step: 97162, loss: 0.052983641624450684\n","Step: 97163, loss: 0.055160749703645706\n","Step: 97164, loss: 0.13554567098617554\n","Step: 97165, loss: 0.20169396698474884\n","Step: 97166, loss: 0.03599927946925163\n","Step: 97167, loss: 0.0397883839905262\n","Step: 97168, loss: 0.10500411689281464\n","Step: 97169, loss: 0.11744670569896698\n","Step: 97170, loss: 0.12341254949569702\n","Step: 97171, loss: 0.11763067543506622\n","Step: 97172, loss: 0.07803941518068314\n","Step: 97173, loss: 0.3253799378871918\n","Step: 97174, loss: 0.09801620244979858\n","Step: 97175, loss: 0.12436608225107193\n","Step: 97176, loss: 0.15357595682144165\n","Step: 97177, loss: 0.07810544967651367\n","Step: 97178, loss: 0.06065439060330391\n","Step: 97179, loss: 0.1662089228630066\n","Step: 97180, loss: 0.16696974635124207\n","Step: 97181, loss: 0.17551665008068085\n","Step: 97182, loss: 0.0\n","Step: 97183, loss: 0.03792785480618477\n","Step: 97184, loss: 0.16056954860687256\n","Step: 97185, loss: 0.0\n","Step: 97186, loss: 0.16546247899532318\n","Step: 97187, loss: 0.0\n","Step: 97188, loss: 0.1944109946489334\n","Step: 97189, loss: 0.16732211410999298\n","Step: 97190, loss: 0.07034657150506973\n","Step: 97191, loss: 0.24655784666538239\n","Step: 97192, loss: 0.13205574452877045\n","Step: 97193, loss: 0.08304677903652191\n","Step: 97194, loss: 0.17428188025951385\n","Step: 97195, loss: 0.0906405821442604\n","Step: 97196, loss: 0.034202028065919876\n","Step: 97197, loss: 0.1750367134809494\n","Step: 97198, loss: 0.1471690833568573\n","Step: 97199, loss: 0.15688012540340424\n","Step: 97200, loss: 0.29817986488342285\n","Step: 97201, loss: 0.04496018588542938\n","Step: 97202, loss: 0.10587269812822342\n","Step: 97203, loss: 0.23672351241111755\n","Step: 97204, loss: 0.1580757051706314\n","Step: 97205, loss: 0.25931259989738464\n","Step: 97206, loss: 0.23820698261260986\n","Step: 97207, loss: 0.21153315901756287\n","Step: 97208, loss: 0.12133914977312088\n","Step: 97209, loss: 0.16750800609588623\n","Step: 97210, loss: 0.10366188734769821\n","Step: 97211, loss: 0.03247906267642975\n","Step: 97212, loss: 0.17154617607593536\n","Step: 97213, loss: 0.1296147108078003\n","Step: 97214, loss: 0.032728780061006546\n","Step: 97215, loss: 0.10065354406833649\n","Step: 97216, loss: 0.0757831409573555\n","Step: 97217, loss: 0.057004984468221664\n","Step: 97218, loss: 0.2099558413028717\n","Step: 97219, loss: 0.1204834058880806\n","Step: 97220, loss: 0.2089683711528778\n","Step: 97221, loss: 0.15512700378894806\n","Step: 97222, loss: 0.093767449259758\n","Step: 97223, loss: 0.15313290059566498\n","Step: 97224, loss: 0.101142518222332\n","Step: 97225, loss: 0.1169130802154541\n","Step: 97226, loss: 0.2664821445941925\n","Step: 97227, loss: 0.17976726591587067\n","Step: 97228, loss: 0.15326693654060364\n","Step: 97229, loss: 0.20662684738636017\n","Step: 97230, loss: 0.0\n","Step: 97231, loss: 0.23283754289150238\n","Step: 97232, loss: 0.1051408126950264\n","Step: 97233, loss: 0.0\n","Step: 97234, loss: 0.11962593346834183\n","Step: 97235, loss: 0.0\n","Step: 97236, loss: 0.0\n","Step: 97237, loss: 0.2846284806728363\n","Step: 97238, loss: 0.09626889228820801\n","Step: 97239, loss: 0.14191429316997528\n","Step: 97240, loss: 0.10889614373445511\n","Step: 97241, loss: 0.37978222966194153\n","Step: 97242, loss: 0.07278405129909515\n","Step: 97243, loss: 0.12163959443569183\n","Step: 97244, loss: 0.21734732389450073\n","Step: 97245, loss: 0.043366603553295135\n","Step: 97246, loss: 0.14001800119876862\n","Step: 97247, loss: 0.09412982314825058\n","Step: 97248, loss: 0.13225577771663666\n","Step: 97249, loss: 0.0927933007478714\n","Step: 97250, loss: 0.29808226227760315\n","Step: 97251, loss: 0.08421894907951355\n","Step: 97252, loss: 0.09019368141889572\n","Step: 97253, loss: 0.16963118314743042\n","Step: 97254, loss: 0.05845163017511368\n","Step: 97255, loss: 0.03353242203593254\n","Step: 97256, loss: 0.05777520313858986\n","Step: 97257, loss: 0.21959838271141052\n","Step: 97258, loss: 0.08057679235935211\n","Step: 97259, loss: 0.031465813517570496\n","Step: 97260, loss: 0.08730161935091019\n","Step: 97261, loss: 0.04524941369891167\n","Step: 97262, loss: 0.07379140704870224\n","Step: 97263, loss: 0.04276641830801964\n","Step: 97264, loss: 0.2026047557592392\n","Step: 97265, loss: 0.169984832406044\n","Step: 97266, loss: 0.07743001729249954\n","Step: 97267, loss: 0.1032143384218216\n","Step: 97268, loss: 0.22386521100997925\n","Step: 97269, loss: 0.04631609842181206\n","Step: 97270, loss: 0.20818449556827545\n","Step: 97271, loss: 0.0\n","Step: 97272, loss: 0.04711076617240906\n","Step: 97273, loss: 0.08931585401296616\n","Step: 97274, loss: 0.0\n","Step: 97275, loss: 0.05754892900586128\n","Step: 97276, loss: 0.15623541176319122\n","Step: 97277, loss: 0.07544344663619995\n","Step: 97278, loss: 0.0\n","Step: 97279, loss: 0.2212768793106079\n","Step: 97280, loss: 0.10438432544469833\n","Step: 97281, loss: 0.21182599663734436\n","Step: 97282, loss: 0.0\n","Step: 97283, loss: 0.08130986988544464\n","Step: 97284, loss: 0.27890658378601074\n","Step: 97285, loss: 0.11305911839008331\n","Step: 97286, loss: 0.14655373990535736\n","Step: 97287, loss: 0.08687550574541092\n","Step: 97288, loss: 0.13322658836841583\n","Step: 97289, loss: 0.15690694749355316\n","Step: 97290, loss: 0.09820692241191864\n","Step: 97291, loss: 0.0969393253326416\n","Step: 97292, loss: 0.04611697047948837\n","Step: 97293, loss: 0.09783687442541122\n","Step: 97294, loss: 0.0680030807852745\n","Step: 97295, loss: 0.10236258804798126\n","Step: 97296, loss: 0.0\n","Step: 97297, loss: 0.2809768319129944\n","Step: 97298, loss: 0.11558887362480164\n","Step: 97299, loss: 0.07625642418861389\n","Step: 97300, loss: 0.21966511011123657\n","Step: 97301, loss: 0.04874788224697113\n","Step: 97302, loss: 0.14101891219615936\n","Step: 97303, loss: 0.046944573521614075\n","Step: 97304, loss: 0.08828651160001755\n","Step: 97305, loss: 0.17620237171649933\n","Step: 97306, loss: 0.03015781007707119\n","Step: 97307, loss: 0.14979124069213867\n","Step: 97308, loss: 0.0\n","Step: 97309, loss: 0.05167793110013008\n","Step: 97310, loss: 0.24215251207351685\n","Step: 97311, loss: 0.06030226871371269\n","Step: 97312, loss: 0.12559156119823456\n","Step: 97313, loss: 0.15040820837020874\n","Step: 97314, loss: 0.05089395493268967\n","Step: 97315, loss: 0.029435643926262856\n","Step: 97316, loss: 0.05228860676288605\n","Step: 97317, loss: 0.11986047774553299\n","Step: 97318, loss: 0.3913501799106598\n","Step: 97319, loss: 0.17699748277664185\n","Step: 97320, loss: 0.03880305960774422\n","Step: 97321, loss: 0.11200334876775742\n","Step: 97322, loss: 0.1455467939376831\n","Step: 97323, loss: 0.28723666071891785\n","Step: 97324, loss: 0.11452209204435349\n","Step: 97325, loss: 0.09110212326049805\n","Step: 97326, loss: 0.10706344246864319\n","Step: 97327, loss: 0.16773271560668945\n","Step: 97328, loss: 0.05220314487814903\n","Step: 97329, loss: 0.1306648701429367\n","Step: 97330, loss: 0.1280815154314041\n","Step: 97331, loss: 0.31216317415237427\n","Step: 97332, loss: 0.20021553337574005\n","Step: 97333, loss: 0.2626103162765503\n","Step: 97334, loss: 0.0\n","Step: 97335, loss: 0.13285942375659943\n","Step: 97336, loss: 0.039418965578079224\n","Step: 97337, loss: 0.18868325650691986\n","Step: 97338, loss: 0.0\n","Step: 97339, loss: 0.0676574632525444\n","Step: 97340, loss: 0.11815127730369568\n","Step: 97341, loss: 0.08671281486749649\n","Step: 97342, loss: 0.14175784587860107\n","Step: 97343, loss: 0.16568656265735626\n","Step: 97344, loss: 0.24138280749320984\n","Step: 97345, loss: 0.15460091829299927\n","Step: 97346, loss: 0.08488940447568893\n","Step: 97347, loss: 0.39686137437820435\n","Step: 97348, loss: 0.21942882239818573\n","Step: 97349, loss: 0.0\n","Step: 97350, loss: 0.24399623274803162\n","Step: 97351, loss: 0.13030844926834106\n","Step: 97352, loss: 0.12558113038539886\n","Step: 97353, loss: 0.0937221422791481\n","Step: 97354, loss: 0.3281504511833191\n","Step: 97355, loss: 0.17204678058624268\n","Step: 97356, loss: 0.12737898528575897\n","Step: 97357, loss: 0.13685846328735352\n","Step: 97358, loss: 0.1220019981265068\n","Step: 97359, loss: 0.15753881633281708\n","Step: 97360, loss: 0.0759660080075264\n","Step: 97361, loss: 0.24878810346126556\n","Step: 97362, loss: 0.10709181427955627\n","Step: 97363, loss: 0.28010404109954834\n","Step: 97364, loss: 0.18351028859615326\n","Step: 97365, loss: 0.0\n","Step: 97366, loss: 0.2406219094991684\n","Step: 97367, loss: 0.23363003134727478\n","Step: 97368, loss: 0.1695639044046402\n","Step: 97369, loss: 0.18160849809646606\n","Step: 97370, loss: 0.053919468075037\n","Step: 97371, loss: 0.17978335916996002\n","Step: 97372, loss: 0.09909258037805557\n","Step: 97373, loss: 0.09903276711702347\n","Step: 97374, loss: 0.09005733579397202\n","Step: 97375, loss: 0.157600536942482\n","Step: 97376, loss: 0.0\n","Step: 97377, loss: 0.16759711503982544\n","Step: 97378, loss: 0.2503197193145752\n","Step: 97379, loss: 0.2235868126153946\n","Step: 97380, loss: 0.11508138477802277\n","Step: 97381, loss: 0.044721685349941254\n","Step: 97382, loss: 0.1014038622379303\n","Step: 97383, loss: 0.17295651137828827\n","Step: 97384, loss: 0.044080667197704315\n","Step: 97385, loss: 0.0\n","Step: 97386, loss: 0.035643674433231354\n","Step: 97387, loss: 0.24490803480148315\n","Step: 97388, loss: 0.0697651281952858\n","Step: 97389, loss: 0.0\n","Step: 97390, loss: 0.3000735938549042\n","Step: 97391, loss: 0.24028214812278748\n","Step: 97392, loss: 0.053836070001125336\n","Step: 97393, loss: 0.11937792599201202\n","Step: 97394, loss: 0.03431705757975578\n","Step: 97395, loss: 0.16091212630271912\n","Step: 97396, loss: 0.1324130743741989\n","Step: 97397, loss: 0.04767841473221779\n","Step: 97398, loss: 0.033743154257535934\n","Step: 97399, loss: 0.27178215980529785\n","Step: 97400, loss: 0.11637094616889954\n","Step: 97401, loss: 0.04427952691912651\n","Step: 97402, loss: 0.14404696226119995\n","Step: 97403, loss: 0.12171246856451035\n","Step: 97404, loss: 0.09522676467895508\n","Step: 97405, loss: 0.20635183155536652\n","Step: 97406, loss: 0.18411411345005035\n","Step: 97407, loss: 0.2403087615966797\n","Step: 97408, loss: 0.14847998321056366\n","Step: 97409, loss: 0.043041907250881195\n","Step: 97410, loss: 0.1687529981136322\n","Step: 97411, loss: 0.12661084532737732\n","Step: 97412, loss: 0.14108312129974365\n","Step: 97413, loss: 0.13292673230171204\n","Step: 97414, loss: 0.09947079420089722\n","Step: 97415, loss: 0.04534990340471268\n","Step: 97416, loss: 0.0788860023021698\n","Step: 97417, loss: 0.05112028121948242\n","Step: 97418, loss: 0.2934339642524719\n","Step: 97419, loss: 0.18713092803955078\n","Step: 97420, loss: 0.1056528389453888\n","Step: 97421, loss: 0.27323126792907715\n","Step: 97422, loss: 0.1391225904226303\n","Step: 97423, loss: 0.11420092731714249\n","Step: 97424, loss: 0.049530841410160065\n","Step: 97425, loss: 0.10419843345880508\n","Step: 97426, loss: 0.13510794937610626\n","Step: 97427, loss: 0.21400415897369385\n","Step: 97428, loss: 0.16741062700748444\n","Step: 97429, loss: 0.07515497505664825\n","Step: 97430, loss: 0.14237439632415771\n","Step: 97431, loss: 0.13551132380962372\n","Step: 97432, loss: 0.09938880801200867\n","Step: 97433, loss: 0.3671378195285797\n","Step: 97434, loss: 0.08551378548145294\n","Step: 97435, loss: 0.2026224285364151\n","Step: 97436, loss: 0.0755501389503479\n","Step: 97437, loss: 0.19996266067028046\n","Step: 97438, loss: 0.0\n","Step: 97439, loss: 0.0\n","Step: 97440, loss: 0.16422848403453827\n","Step: 97441, loss: 0.11022277921438217\n","Step: 97442, loss: 0.08105901628732681\n","Step: 97443, loss: 0.0\n","Step: 97444, loss: 0.20084671676158905\n","Step: 97445, loss: 0.2338613122701645\n","Step: 97446, loss: 0.147719606757164\n","Step: 97447, loss: 0.08299382030963898\n","Step: 97448, loss: 0.19898749887943268\n","Step: 97449, loss: 0.2879306972026825\n","Step: 97450, loss: 0.0\n","Step: 97451, loss: 0.24495619535446167\n","Step: 97452, loss: 0.16482600569725037\n","Step: 97453, loss: 0.15686306357383728\n","Step: 97454, loss: 0.16031326353549957\n","Step: 97455, loss: 0.0\n","Step: 97456, loss: 0.09676408022642136\n","Step: 97457, loss: 0.13174861669540405\n","Step: 97458, loss: 0.09439171105623245\n","Step: 97459, loss: 0.1269054114818573\n","Step: 97460, loss: 0.045929908752441406\n","Step: 97461, loss: 0.21047715842723846\n","Step: 97462, loss: 0.10909302532672882\n","Step: 97463, loss: 0.0\n","Step: 97464, loss: 0.09006855636835098\n","Step: 97465, loss: 0.07906998693943024\n","Step: 97466, loss: 0.0\n","Step: 97467, loss: 0.05292700603604317\n","Step: 97468, loss: 0.09853746742010117\n","Step: 97469, loss: 0.0960887223482132\n","Step: 97470, loss: 0.03390440344810486\n","Step: 97471, loss: 0.1602298766374588\n","Step: 97472, loss: 0.04062521830201149\n","Step: 97473, loss: 0.11921581625938416\n","Step: 97474, loss: 0.15073935687541962\n","Step: 97475, loss: 0.11248525977134705\n","Step: 97476, loss: 0.07853821665048599\n","Step: 97477, loss: 0.1187521442770958\n","Step: 97478, loss: 0.20300564169883728\n","Step: 97479, loss: 0.18282122910022736\n","Step: 97480, loss: 0.2285170704126358\n","Step: 97481, loss: 0.055320825427770615\n","Step: 97482, loss: 0.03284825384616852\n","Step: 97483, loss: 0.0\n","Step: 97484, loss: 0.12100181728601456\n","Step: 97485, loss: 0.054514799267053604\n","Step: 97486, loss: 0.0\n","Step: 97487, loss: 0.14083121716976166\n","Step: 97488, loss: 0.39664238691329956\n","Step: 97489, loss: 0.03306783363223076\n","Step: 97490, loss: 0.07521659135818481\n","Step: 97491, loss: 0.0\n","Step: 97492, loss: 0.03317943215370178\n","Step: 97493, loss: 0.17338596284389496\n","Step: 97494, loss: 0.2989599108695984\n","Step: 97495, loss: 0.2374030351638794\n","Step: 97496, loss: 0.18358512222766876\n","Step: 97497, loss: 0.10809805244207382\n","Step: 97498, loss: 0.08860171586275101\n","Step: 97499, loss: 0.04999095946550369\n","Step: 97500, loss: 0.1748044341802597\n","Step: 97501, loss: 0.050933193415403366\n","Step: 97502, loss: 0.15938839316368103\n","Step: 97503, loss: 0.09944851696491241\n","Step: 97504, loss: 0.04725265130400658\n","Step: 97505, loss: 0.22512996196746826\n","Step: 97506, loss: 0.17369690537452698\n","Step: 97507, loss: 0.09556007385253906\n","Step: 97508, loss: 0.11960018426179886\n","Step: 97509, loss: 0.04359763488173485\n","Step: 97510, loss: 0.0920775905251503\n","Step: 97511, loss: 0.1380242556333542\n","Step: 97512, loss: 0.04136137664318085\n","Step: 97513, loss: 0.16596893966197968\n","Step: 97514, loss: 0.17756450176239014\n","Step: 97515, loss: 0.03698284551501274\n","Step: 97516, loss: 0.1955471783876419\n","Step: 97517, loss: 0.03406543284654617\n","Step: 97518, loss: 0.13236884772777557\n","Step: 97519, loss: 0.21214346587657928\n","Step: 97520, loss: 0.2059052288532257\n","Step: 97521, loss: 0.04555566608905792\n","Step: 97522, loss: 0.15269912779331207\n","Step: 97523, loss: 0.05382756143808365\n","Step: 97524, loss: 0.08525950461626053\n","Step: 97525, loss: 0.15146717429161072\n","Step: 97526, loss: 0.13428905606269836\n","Step: 97527, loss: 0.1295490711927414\n","Step: 97528, loss: 0.2229788601398468\n","Step: 97529, loss: 0.10055415332317352\n","Step: 97530, loss: 0.13468195497989655\n","Step: 97531, loss: 0.12249775230884552\n","Step: 97532, loss: 0.06351017206907272\n","Step: 97533, loss: 0.08188890665769577\n","Step: 97534, loss: 0.12957942485809326\n","Step: 97535, loss: 0.17419666051864624\n","Step: 97536, loss: 0.3203031122684479\n","Step: 97537, loss: 0.1110936626791954\n","Step: 97538, loss: 0.12764030694961548\n","Step: 97539, loss: 0.2120579481124878\n","Step: 97540, loss: 0.057600803673267365\n","Step: 97541, loss: 0.2978943884372711\n","Step: 97542, loss: 0.045735083520412445\n","Step: 97543, loss: 0.029381025582551956\n","Step: 97544, loss: 0.0\n","Step: 97545, loss: 0.1550058126449585\n","Step: 97546, loss: 0.23015064001083374\n","Step: 97547, loss: 0.04142610728740692\n","Step: 97548, loss: 0.07185030728578568\n","Step: 97549, loss: 0.15909796953201294\n","Step: 97550, loss: 0.2298649400472641\n","Step: 97551, loss: 0.0865076556801796\n","Step: 97552, loss: 0.10628614574670792\n","Step: 97553, loss: 0.07788107544183731\n","Step: 97554, loss: 0.05756156146526337\n","Step: 97555, loss: 0.1483246088027954\n","Step: 97556, loss: 0.09463237971067429\n","Step: 97557, loss: 0.268594890832901\n","Step: 97558, loss: 0.2206348180770874\n","Step: 97559, loss: 0.2350367307662964\n","Step: 97560, loss: 0.0539315789937973\n","Step: 97561, loss: 0.33268409967422485\n","Step: 97562, loss: 0.18377700448036194\n","Step: 97563, loss: 0.1788678765296936\n","Step: 97564, loss: 0.06108789145946503\n","Step: 97565, loss: 0.21427510678768158\n","Step: 97566, loss: 0.10474858433008194\n","Step: 97567, loss: 0.2762451171875\n","Step: 97568, loss: 0.19169263541698456\n","Step: 97569, loss: 0.22824330627918243\n","Step: 97570, loss: 0.1174180880188942\n","Step: 97571, loss: 0.21419170498847961\n","Step: 97572, loss: 0.07639013230800629\n","Step: 97573, loss: 0.04319121316075325\n","Step: 97574, loss: 0.03270363435149193\n","Step: 97575, loss: 0.12469539791345596\n","Step: 97576, loss: 0.042317189276218414\n","Step: 97577, loss: 0.13632696866989136\n","Step: 97578, loss: 0.15147483348846436\n","Step: 97579, loss: 0.17970433831214905\n","Step: 97580, loss: 0.21790851652622223\n","Step: 97581, loss: 0.2838033437728882\n","Step: 97582, loss: 0.03875298053026199\n","Step: 97583, loss: 0.09086218476295471\n","Step: 97584, loss: 0.21296104788780212\n","Step: 97585, loss: 0.03616271913051605\n","Step: 97586, loss: 0.15831704437732697\n","Step: 97587, loss: 0.03613562509417534\n","Step: 97588, loss: 0.26743656396865845\n","Step: 97589, loss: 0.1347443014383316\n","Step: 97590, loss: 0.16252809762954712\n","Step: 97591, loss: 0.12985928356647491\n","Step: 97592, loss: 0.24672448635101318\n","Step: 97593, loss: 0.14287710189819336\n","Step: 97594, loss: 0.0\n","Step: 97595, loss: 0.1366567462682724\n","Step: 97596, loss: 0.06636238098144531\n","Step: 97597, loss: 0.12152457982301712\n","Step: 97598, loss: 0.1015789806842804\n","Step: 97599, loss: 0.140868678689003\n","Step: 97600, loss: 0.18900422751903534\n","Step: 97601, loss: 0.16457654535770416\n","Step: 97602, loss: 0.1273263692855835\n","Step: 97603, loss: 0.030284419655799866\n","Step: 97604, loss: 0.10819289833307266\n","Step: 97605, loss: 0.1130964457988739\n","Step: 97606, loss: 0.13553497195243835\n","Step: 97607, loss: 0.06776393949985504\n","Step: 97608, loss: 0.1094842255115509\n","Step: 97609, loss: 0.13972273468971252\n","Step: 97610, loss: 0.1044551432132721\n","Step: 97611, loss: 0.24630457162857056\n","Step: 97612, loss: 0.18911701440811157\n","Step: 97613, loss: 0.15948286652565002\n","Step: 97614, loss: 0.0\n","Step: 97615, loss: 0.3170137107372284\n","Step: 97616, loss: 0.046750325709581375\n","Step: 97617, loss: 0.24327616393566132\n","Step: 97618, loss: 0.0\n","Step: 97619, loss: 0.08180943131446838\n","Step: 97620, loss: 0.16897855699062347\n","Step: 97621, loss: 0.056299228221178055\n","Step: 97622, loss: 0.11326484382152557\n","Step: 97623, loss: 0.04002136364579201\n","Step: 97624, loss: 0.13226597011089325\n","Step: 97625, loss: 0.08375690132379532\n","Step: 97626, loss: 0.028508415445685387\n","Step: 97627, loss: 0.0\n","Step: 97628, loss: 0.18265001475811005\n","Step: 97629, loss: 0.15470491349697113\n","Step: 97630, loss: 0.07889116555452347\n","Step: 97631, loss: 0.127688929438591\n","Step: 97632, loss: 0.0212568249553442\n","Step: 97633, loss: 0.1429826319217682\n","Step: 97634, loss: 0.1497848927974701\n","Step: 97635, loss: 0.0\n","Step: 97636, loss: 0.05196978524327278\n","Step: 97637, loss: 0.12096208333969116\n","Step: 97638, loss: 0.18185962736606598\n","Step: 97639, loss: 0.0992172434926033\n","Step: 97640, loss: 0.0\n","Step: 97641, loss: 0.11673840880393982\n","Step: 97642, loss: 0.11743421852588654\n","Step: 97643, loss: 0.07229765504598618\n","Step: 97644, loss: 0.26267188787460327\n","Step: 97645, loss: 0.05228159576654434\n","Step: 97646, loss: 0.29051458835601807\n","Step: 97647, loss: 0.163823664188385\n","Step: 97648, loss: 0.07693664729595184\n","Step: 97649, loss: 0.06811762601137161\n","Step: 97650, loss: 0.04859989508986473\n","Step: 97651, loss: 0.2362687736749649\n","Step: 97652, loss: 0.2775593101978302\n","Step: 97653, loss: 0.0\n","Step: 97654, loss: 0.27446192502975464\n","Step: 97655, loss: 0.06643763184547424\n","Step: 97656, loss: 0.13261419534683228\n","Step: 97657, loss: 0.09042058885097504\n","Step: 97658, loss: 0.04763022065162659\n","Step: 97659, loss: 0.24259823560714722\n","Step: 97660, loss: 0.18457302451133728\n","Step: 97661, loss: 0.24743011593818665\n","Step: 97662, loss: 0.0665736049413681\n","Step: 97663, loss: 0.044880885630846024\n","Step: 97664, loss: 0.12384459376335144\n","Step: 97665, loss: 0.5268734693527222\n","Step: 97666, loss: 0.1267520934343338\n","Step: 97667, loss: 0.10891038924455643\n","Step: 97668, loss: 0.05753074958920479\n","Step: 97669, loss: 0.36778801679611206\n","Step: 97670, loss: 0.0396912582218647\n","Step: 97671, loss: 0.18388694524765015\n","Step: 97672, loss: 0.1581273078918457\n","Step: 97673, loss: 0.2924330234527588\n","Step: 97674, loss: 0.09847290813922882\n","Step: 97675, loss: 0.0862022414803505\n","Step: 97676, loss: 0.04960283264517784\n","Step: 97677, loss: 0.11621811240911484\n","Step: 97678, loss: 0.1893453150987625\n","Step: 97679, loss: 0.17266425490379333\n","Step: 97680, loss: 0.06676032394170761\n","Step: 97681, loss: 0.2477463334798813\n","Step: 97682, loss: 0.2888689339160919\n","Step: 97683, loss: 0.12910470366477966\n","Step: 97684, loss: 0.15418443083763123\n","Step: 97685, loss: 0.033860377967357635\n","Step: 97686, loss: 0.0340639092028141\n","Step: 97687, loss: 0.13014085590839386\n","Step: 97688, loss: 0.06189865991473198\n","Step: 97689, loss: 0.09258083254098892\n","Step: 97690, loss: 0.04435420036315918\n","Step: 97691, loss: 0.06521356850862503\n","Step: 97692, loss: 0.31479156017303467\n","Step: 97693, loss: 0.2562426030635834\n","Step: 97694, loss: 0.1008971780538559\n","Step: 97695, loss: 0.09001109004020691\n","Step: 97696, loss: 0.1570568084716797\n","Step: 97697, loss: 0.08733459562063217\n","Step: 97698, loss: 0.16239310801029205\n","Step: 97699, loss: 0.0372331440448761\n","Step: 97700, loss: 0.33413928747177124\n","Step: 97701, loss: 0.17989708483219147\n","Step: 97702, loss: 0.10269699990749359\n","Step: 97703, loss: 0.1842641979455948\n","Step: 97704, loss: 0.15434376895427704\n","Step: 97705, loss: 0.07865792512893677\n","Step: 97706, loss: 0.302531898021698\n","Step: 97707, loss: 0.14160501956939697\n","Step: 97708, loss: 0.0\n","Step: 97709, loss: 0.27000942826271057\n","Step: 97710, loss: 0.11349082738161087\n","Step: 97711, loss: 0.1669560670852661\n","Step: 97712, loss: 0.14158591628074646\n","Step: 97713, loss: 0.22754888236522675\n","Step: 97714, loss: 0.2565162777900696\n","Step: 97715, loss: 0.0834486111998558\n","Step: 97716, loss: 0.32186782360076904\n","Step: 97717, loss: 0.16013702750205994\n","Step: 97718, loss: 0.10632146894931793\n","Step: 97719, loss: 0.32388290762901306\n","Step: 97720, loss: 0.3027482330799103\n","Step: 97721, loss: 0.0\n","Step: 97722, loss: 0.039737749844789505\n","Step: 97723, loss: 0.16026294231414795\n","Step: 97724, loss: 0.18903855979442596\n","Step: 97725, loss: 0.08482193201780319\n","Step: 97726, loss: 0.06370121985673904\n","Step: 97727, loss: 0.04617013409733772\n","Step: 97728, loss: 0.10506606101989746\n","Step: 97729, loss: 0.1221039742231369\n","Step: 97730, loss: 0.1151401549577713\n","Step: 97731, loss: 0.037665609270334244\n","Step: 97732, loss: 0.08600408583879471\n","Step: 97733, loss: 0.14808140695095062\n","Step: 97734, loss: 0.09778788685798645\n","Step: 97735, loss: 0.0\n","Step: 97736, loss: 0.22629408538341522\n","Step: 97737, loss: 0.049254704266786575\n","Step: 97738, loss: 0.14332430064678192\n","Step: 97739, loss: 0.240386962890625\n","Step: 97740, loss: 0.12801840901374817\n","Step: 97741, loss: 0.1398904174566269\n","Step: 97742, loss: 0.14111952483654022\n","Step: 97743, loss: 0.08385810256004333\n","Step: 97744, loss: 0.0\n","Step: 97745, loss: 0.07149804383516312\n","Step: 97746, loss: 0.17095154523849487\n","Step: 97747, loss: 0.0\n","Step: 97748, loss: 0.09819561243057251\n","Step: 97749, loss: 0.050132859498262405\n","Step: 97750, loss: 0.04649782180786133\n","Step: 97751, loss: 0.07042504101991653\n","Step: 97752, loss: 0.09638095647096634\n","Step: 97753, loss: 0.17764076590538025\n","Step: 97754, loss: 0.023709440603852272\n","Step: 97755, loss: 0.15150099992752075\n","Step: 97756, loss: 0.12800760567188263\n","Step: 97757, loss: 0.09747541695833206\n","Step: 97758, loss: 0.0\n","Step: 97759, loss: 0.10549123585224152\n","Step: 97760, loss: 0.05727940425276756\n","Step: 97761, loss: 0.024589452892541885\n","Step: 97762, loss: 0.11336484551429749\n","Step: 97763, loss: 0.06653895229101181\n","Step: 97764, loss: 0.0973193570971489\n","Step: 97765, loss: 0.2054564654827118\n","Step: 97766, loss: 0.23923355340957642\n","Step: 97767, loss: 0.0\n","Step: 97768, loss: 0.05060996860265732\n","Step: 97769, loss: 0.08180886507034302\n","Step: 97770, loss: 0.02908783219754696\n","Step: 97771, loss: 0.2046251744031906\n","Step: 97772, loss: 0.0\n","Step: 97773, loss: 0.04999086260795593\n","Step: 97774, loss: 0.0\n","Step: 97775, loss: 0.052758388221263885\n","Step: 97776, loss: 0.17445816099643707\n","Step: 97777, loss: 0.283198744058609\n","Step: 97778, loss: 0.19266155362129211\n","Step: 97779, loss: 0.13216149806976318\n","Step: 97780, loss: 0.11569598317146301\n","Step: 97781, loss: 0.0802014172077179\n","Step: 97782, loss: 0.17944921553134918\n","Step: 97783, loss: 0.16038192808628082\n","Step: 97784, loss: 0.048399247229099274\n","Step: 97785, loss: 0.11955146491527557\n","Step: 97786, loss: 0.06753316521644592\n","Step: 97787, loss: 0.16403540968894958\n","Step: 97788, loss: 0.0\n","Step: 97789, loss: 0.07821417599916458\n","Step: 97790, loss: 0.09775025397539139\n","Step: 97791, loss: 0.0829734206199646\n","Step: 97792, loss: 0.08401880413293839\n","Step: 97793, loss: 0.044964563101530075\n","Step: 97794, loss: 0.035196539014577866\n","Step: 97795, loss: 0.0351065918803215\n","Step: 97796, loss: 0.2956247925758362\n","Step: 97797, loss: 0.042862482368946075\n","Step: 97798, loss: 0.36354735493659973\n","Step: 97799, loss: 0.08901824802160263\n","Step: 97800, loss: 0.12253181636333466\n","Step: 97801, loss: 0.07513166964054108\n","Step: 97802, loss: 0.11112894117832184\n","Step: 97803, loss: 0.11457286030054092\n","Step: 97804, loss: 0.05030779913067818\n","Step: 97805, loss: 0.038730438798666\n","Step: 97806, loss: 0.07686169445514679\n","Step: 97807, loss: 0.08760352432727814\n","Step: 97808, loss: 0.10796243697404861\n","Step: 97809, loss: 0.029899321496486664\n","Step: 97810, loss: 0.349176824092865\n","Step: 97811, loss: 0.16507072746753693\n","Step: 97812, loss: 0.2022692710161209\n","Step: 97813, loss: 0.14716044068336487\n","Step: 97814, loss: 0.0808073878288269\n","Step: 97815, loss: 0.0718560591340065\n","Step: 97816, loss: 0.0\n","Step: 97817, loss: 0.053187187761068344\n","Step: 97818, loss: 0.18809278309345245\n","Step: 97819, loss: 0.1284826397895813\n","Step: 97820, loss: 0.27244770526885986\n","Step: 97821, loss: 0.2636660933494568\n","Step: 97822, loss: 0.13541436195373535\n","Step: 97823, loss: 0.3036302328109741\n","Step: 97824, loss: 0.07099220901727676\n","Step: 97825, loss: 0.12169677019119263\n","Step: 97826, loss: 0.13120776414871216\n","Step: 97827, loss: 0.16434426605701447\n","Step: 97828, loss: 0.04218186438083649\n","Step: 97829, loss: 0.253113716840744\n","Step: 97830, loss: 0.043790627270936966\n","Step: 97831, loss: 0.23959335684776306\n","Step: 97832, loss: 0.2031354159116745\n","Step: 97833, loss: 0.2489398568868637\n","Step: 97834, loss: 0.1470063179731369\n","Step: 97835, loss: 0.10148736089468002\n","Step: 97836, loss: 0.0\n","Step: 97837, loss: 0.0\n","Step: 97838, loss: 0.08169981837272644\n","Step: 97839, loss: 0.0\n","Step: 97840, loss: 0.06871723383665085\n","Step: 97841, loss: 0.0\n","Step: 97842, loss: 0.14556801319122314\n","Step: 97843, loss: 0.08958932757377625\n","Step: 97844, loss: 0.04230918735265732\n","Step: 97845, loss: 0.1642426997423172\n","Step: 97846, loss: 0.3001323938369751\n","Step: 97847, loss: 0.3440816104412079\n","Step: 97848, loss: 0.22855477035045624\n","Step: 97849, loss: 0.2152782380580902\n","Step: 97850, loss: 0.10212495177984238\n","Step: 97851, loss: 0.13598141074180603\n","Step: 97852, loss: 0.08657756447792053\n","Step: 97853, loss: 0.1625022441148758\n","Step: 97854, loss: 0.045717753469944\n","Step: 97855, loss: 0.042317166924476624\n","Step: 97856, loss: 0.2662445306777954\n","Step: 97857, loss: 0.07819340378046036\n","Step: 97858, loss: 0.1323322057723999\n","Step: 97859, loss: 0.07697237282991409\n","Step: 97860, loss: 0.039528604596853256\n","Step: 97861, loss: 0.11007767915725708\n","Step: 97862, loss: 0.3701492249965668\n","Step: 97863, loss: 0.271521657705307\n","Step: 97864, loss: 0.04218236356973648\n","Step: 97865, loss: 0.12300273030996323\n","Step: 97866, loss: 0.14978425204753876\n","Step: 97867, loss: 0.1717703640460968\n","Step: 97868, loss: 0.2092645764350891\n","Step: 97869, loss: 0.17997848987579346\n","Step: 97870, loss: 0.12944239377975464\n","Step: 97871, loss: 0.13180845975875854\n","Step: 97872, loss: 0.16927333176136017\n","Step: 97873, loss: 0.20222140848636627\n","Step: 97874, loss: 0.1936461627483368\n","Step: 97875, loss: 0.2134373039007187\n","Step: 97876, loss: 0.08082814514636993\n","Step: 97877, loss: 0.1258201003074646\n","Step: 97878, loss: 0.11992550641298294\n","Step: 97879, loss: 0.08547785133123398\n","Step: 97880, loss: 0.11063139885663986\n","Step: 97881, loss: 0.07187230885028839\n","Step: 97882, loss: 0.17350450158119202\n","Step: 97883, loss: 0.1264188140630722\n","Step: 97884, loss: 0.14492195844650269\n","Step: 97885, loss: 0.13596343994140625\n","Step: 97886, loss: 0.17285197973251343\n","Step: 97887, loss: 0.21800804138183594\n","Step: 97888, loss: 0.22793994843959808\n","Step: 97889, loss: 0.2006743997335434\n","Step: 97890, loss: 0.05022149160504341\n","Step: 97891, loss: 0.0\n","Step: 97892, loss: 0.08784075081348419\n","Step: 97893, loss: 0.14796949923038483\n","Step: 97894, loss: 0.08664695918560028\n","Step: 97895, loss: 0.2651420831680298\n","Step: 97896, loss: 0.23090320825576782\n","Step: 97897, loss: 0.13386587798595428\n","Step: 97898, loss: 0.0\n","Step: 97899, loss: 0.16484516859054565\n","Step: 97900, loss: 0.17807236313819885\n","Step: 97901, loss: 0.0\n","Step: 97902, loss: 0.2799089848995209\n","Step: 97903, loss: 0.17386798560619354\n","Step: 97904, loss: 0.0\n","Step: 97905, loss: 0.14618392288684845\n","Step: 97906, loss: 0.15507447719573975\n","Step: 97907, loss: 0.16852107644081116\n","Step: 97908, loss: 0.08766776323318481\n","Step: 97909, loss: 0.17425262928009033\n","Step: 97910, loss: 0.12827111780643463\n","Step: 97911, loss: 0.042304232716560364\n","Step: 97912, loss: 0.04192208871245384\n","Step: 97913, loss: 0.15245795249938965\n","Step: 97914, loss: 0.0797186940908432\n","Step: 97915, loss: 0.1620672047138214\n","Step: 97916, loss: 0.21691402792930603\n","Step: 97917, loss: 0.11498533934354782\n","Step: 97918, loss: 0.04303467646241188\n","Step: 97919, loss: 0.14104054868221283\n","Step: 97920, loss: 0.060715917497873306\n","Step: 97921, loss: 0.12537631392478943\n","Step: 97922, loss: 0.2150314748287201\n","Step: 97923, loss: 0.13154256343841553\n","Step: 97924, loss: 0.08280430734157562\n","Step: 97925, loss: 0.1697503626346588\n","Step: 97926, loss: 0.1040109172463417\n","Step: 97927, loss: 0.07977859675884247\n","Step: 97928, loss: 0.10528084635734558\n","Step: 97929, loss: 0.31027477979660034\n","Step: 97930, loss: 0.0\n","Step: 97931, loss: 0.05870421230792999\n","Step: 97932, loss: 0.11967891454696655\n","Step: 97933, loss: 0.05652279779314995\n","Step: 97934, loss: 0.09304115921258926\n","Step: 97935, loss: 0.09714273363351822\n","Step: 97936, loss: 0.25674399733543396\n","Step: 97937, loss: 0.0423307903110981\n","Step: 97938, loss: 0.07444444298744202\n","Step: 97939, loss: 0.1726384311914444\n","Step: 97940, loss: 0.06501586735248566\n","Step: 97941, loss: 0.17235244810581207\n","Step: 97942, loss: 0.12223785370588303\n","Step: 97943, loss: 0.388478547334671\n","Step: 97944, loss: 0.0\n","Step: 97945, loss: 0.18917347490787506\n","Step: 97946, loss: 0.03774736076593399\n","Step: 97947, loss: 0.13923898339271545\n","Step: 97948, loss: 0.03693777695298195\n","Step: 97949, loss: 0.06848759949207306\n","Step: 97950, loss: 0.1482718288898468\n","Step: 97951, loss: 0.26246848702430725\n","Step: 97952, loss: 0.21383517980575562\n","Step: 97953, loss: 0.14265917241573334\n","Step: 97954, loss: 0.2752404510974884\n","Step: 97955, loss: 0.10134055465459824\n","Step: 97956, loss: 0.2740628123283386\n","Step: 97957, loss: 0.034439414739608765\n","Step: 97958, loss: 0.1759503185749054\n","Step: 97959, loss: 0.0\n","Step: 97960, loss: 0.14555346965789795\n","Step: 97961, loss: 0.2370561957359314\n","Step: 97962, loss: 0.10644111037254333\n","Step: 97963, loss: 0.0\n","Step: 97964, loss: 0.03346416354179382\n","Step: 97965, loss: 0.10364827513694763\n","Step: 97966, loss: 0.15105637907981873\n","Step: 97967, loss: 0.08152195066213608\n","Step: 97968, loss: 0.1395077258348465\n","Step: 97969, loss: 0.08376716822385788\n","Step: 97970, loss: 0.0\n","Step: 97971, loss: 0.3615875244140625\n","Step: 97972, loss: 0.19395610690116882\n","Step: 97973, loss: 0.1652897298336029\n","Step: 97974, loss: 0.03201465681195259\n","Step: 97975, loss: 0.1361767053604126\n","Step: 97976, loss: 0.0\n","Step: 97977, loss: 0.12558043003082275\n","Step: 97978, loss: 0.2616696059703827\n","Step: 97979, loss: 0.16584213078022003\n","Step: 97980, loss: 0.10430631041526794\n","Step: 97981, loss: 0.12292207032442093\n","Step: 97982, loss: 0.24948135018348694\n","Step: 97983, loss: 0.05129380151629448\n","Step: 97984, loss: 0.13371582329273224\n","Step: 97985, loss: 0.24331817030906677\n","Step: 97986, loss: 0.12940387427806854\n","Step: 97987, loss: 0.1677337884902954\n","Step: 97988, loss: 0.1289154291152954\n","Step: 97989, loss: 0.035565704107284546\n","Step: 97990, loss: 0.07317706942558289\n","Step: 97991, loss: 0.07887046039104462\n","Step: 97992, loss: 0.2298901379108429\n","Step: 97993, loss: 0.05287102609872818\n","Step: 97994, loss: 0.0\n","Step: 97995, loss: 0.12163592129945755\n","Step: 97996, loss: 0.1590452343225479\n","Step: 97997, loss: 0.25597938895225525\n","Step: 97998, loss: 0.10651809722185135\n","Step: 97999, loss: 0.07437218725681305\n","Step: 98000, loss: 0.06288979947566986\n","Step: 98001, loss: 0.07168249040842056\n","Step: 98002, loss: 0.21732670068740845\n","Step: 98003, loss: 0.0\n","Step: 98004, loss: 0.03953864052891731\n","Step: 98005, loss: 0.11250566691160202\n","Step: 98006, loss: 0.0\n","Step: 98007, loss: 0.1586708426475525\n","Step: 98008, loss: 0.19082507491111755\n","Step: 98009, loss: 0.0\n","Step: 98010, loss: 0.07313837856054306\n","Step: 98011, loss: 0.16192463040351868\n","Step: 98012, loss: 0.11633116006851196\n","Step: 98013, loss: 0.05031626671552658\n","Step: 98014, loss: 0.0\n","Step: 98015, loss: 0.15403011441230774\n","Step: 98016, loss: 0.150922492146492\n","Step: 98017, loss: 0.07206553965806961\n","Step: 98018, loss: 0.1368630975484848\n","Step: 98019, loss: 0.04564305394887924\n","Step: 98020, loss: 0.09306683391332626\n","Step: 98021, loss: 0.10895756632089615\n","Step: 98022, loss: 0.10453836619853973\n","Step: 98023, loss: 0.09202419221401215\n","Step: 98024, loss: 0.17539669573307037\n","Step: 98025, loss: 0.40444886684417725\n","Step: 98026, loss: 0.0877315029501915\n","Step: 98027, loss: 0.0\n","Step: 98028, loss: 0.24793092906475067\n","Step: 98029, loss: 0.04295067489147186\n","Step: 98030, loss: 0.21825729310512543\n","Step: 98031, loss: 0.0\n","Step: 98032, loss: 0.09558320045471191\n","Step: 98033, loss: 0.16832222044467926\n","Step: 98034, loss: 0.06705239415168762\n","Step: 98035, loss: 0.1522921770811081\n","Step: 98036, loss: 0.0\n","Step: 98037, loss: 0.22857168316841125\n","Step: 98038, loss: 0.3135891258716583\n","Step: 98039, loss: 0.140027716755867\n","Step: 98040, loss: 0.17003844678401947\n","Step: 98041, loss: 0.0\n","Step: 98042, loss: 0.2658303380012512\n","Step: 98043, loss: 0.10157795250415802\n","Step: 98044, loss: 0.06543884426355362\n","Step: 98045, loss: 0.1684054434299469\n","Step: 98046, loss: 0.03104037046432495\n","Step: 98047, loss: 0.06027068942785263\n","Step: 98048, loss: 0.12785351276397705\n","Step: 98049, loss: 0.0\n","Step: 98050, loss: 0.21767911314964294\n","Step: 98051, loss: 0.15330499410629272\n","Step: 98052, loss: 0.0\n","Step: 98053, loss: 0.20185397565364838\n","Step: 98054, loss: 0.12456948310136795\n","Step: 98055, loss: 0.10123202949762344\n","Step: 98056, loss: 0.12693239748477936\n","Step: 98057, loss: 0.13920600712299347\n","Step: 98058, loss: 0.0\n","Step: 98059, loss: 0.06075797230005264\n","Step: 98060, loss: 0.10364396870136261\n","Step: 98061, loss: 0.14317084848880768\n","Step: 98062, loss: 0.07259631901979446\n","Step: 98063, loss: 0.231739804148674\n","Step: 98064, loss: 0.05246024206280708\n","Step: 98065, loss: 0.2417425960302353\n","Step: 98066, loss: 0.2435225248336792\n","Step: 98067, loss: 0.11233571171760559\n","Step: 98068, loss: 0.1525343954563141\n","Step: 98069, loss: 0.21380826830863953\n","Step: 98070, loss: 0.04083852469921112\n","Step: 98071, loss: 0.20297813415527344\n","Step: 98072, loss: 0.14981591701507568\n","Step: 98073, loss: 0.07949171960353851\n","Step: 98074, loss: 0.06431953608989716\n","Step: 98075, loss: 0.04961327463388443\n","Step: 98076, loss: 0.0\n","Step: 98077, loss: 0.15149758756160736\n","Step: 98078, loss: 0.0\n","Step: 98079, loss: 0.17752590775489807\n","Step: 98080, loss: 0.21297283470630646\n","Step: 98081, loss: 0.25565505027770996\n","Step: 98082, loss: 0.05403411015868187\n","Step: 98083, loss: 0.19224099814891815\n","Step: 98084, loss: 0.0\n","Step: 98085, loss: 0.14252091944217682\n","Step: 98086, loss: 0.046555787324905396\n","Step: 98087, loss: 0.04254758358001709\n","Step: 98088, loss: 0.13713616132736206\n","Step: 98089, loss: 0.11763367801904678\n","Step: 98090, loss: 0.04197775945067406\n","Step: 98091, loss: 0.12946276366710663\n","Step: 98092, loss: 0.24099962413311005\n","Step: 98093, loss: 0.07920970767736435\n","Step: 98094, loss: 0.1200912743806839\n","Step: 98095, loss: 0.11300864070653915\n","Step: 98096, loss: 0.08815813809633255\n","Step: 98097, loss: 0.0\n","Step: 98098, loss: 0.29895153641700745\n","Step: 98099, loss: 0.14831091463565826\n","Step: 98100, loss: 0.0\n","Step: 98101, loss: 0.20982849597930908\n","Step: 98102, loss: 0.22634051740169525\n","Step: 98103, loss: 0.03289751708507538\n","Step: 98104, loss: 0.046239785850048065\n","Step: 98105, loss: 0.11211786419153214\n","Step: 98106, loss: 0.05409378558397293\n","Step: 98107, loss: 0.08259588479995728\n","Step: 98108, loss: 0.12474312633275986\n","Step: 98109, loss: 0.10329832136631012\n","Step: 98110, loss: 0.2004777193069458\n","Step: 98111, loss: 0.14490582048892975\n","Step: 98112, loss: 0.2366974800825119\n","Step: 98113, loss: 0.0\n","Step: 98114, loss: 0.0\n","Step: 98115, loss: 0.04592857509851456\n","Step: 98116, loss: 0.12411413341760635\n","Step: 98117, loss: 0.04570428282022476\n","Step: 98118, loss: 0.4685874581336975\n","Step: 98119, loss: 0.1722734123468399\n","Step: 98120, loss: 0.14969491958618164\n","Step: 98121, loss: 0.08377187699079514\n","Step: 98122, loss: 0.21042875945568085\n","Step: 98123, loss: 0.13197261095046997\n","Step: 98124, loss: 0.23589958250522614\n","Step: 98125, loss: 0.23626920580863953\n","Step: 98126, loss: 0.15064826607704163\n","Step: 98127, loss: 0.2042284905910492\n","Step: 98128, loss: 0.1310325562953949\n","Step: 98129, loss: 0.1986677199602127\n","Step: 98130, loss: 0.11409778892993927\n","Step: 98131, loss: 0.10126474499702454\n","Step: 98132, loss: 0.20968224108219147\n","Step: 98133, loss: 0.24716147780418396\n","Step: 98134, loss: 0.2526242733001709\n","Step: 98135, loss: 0.1644597053527832\n","Step: 98136, loss: 0.10878606885671616\n","Step: 98137, loss: 0.1419583261013031\n","Step: 98138, loss: 0.11409050226211548\n","Step: 98139, loss: 0.22903938591480255\n","Step: 98140, loss: 0.0\n","Step: 98141, loss: 0.11937372386455536\n","Step: 98142, loss: 0.05380633473396301\n","Step: 98143, loss: 0.2606554925441742\n","Step: 98144, loss: 0.05652669444680214\n","Step: 98145, loss: 0.087384894490242\n","Step: 98146, loss: 0.24107001721858978\n","Step: 98147, loss: 0.15965668857097626\n","Step: 98148, loss: 0.20051909983158112\n","Step: 98149, loss: 0.1482028067111969\n","Step: 98150, loss: 0.04526394233107567\n","Step: 98151, loss: 0.14830690622329712\n","Step: 98152, loss: 0.08818835020065308\n","Step: 98153, loss: 0.1344710737466812\n","Step: 98154, loss: 0.14683657884597778\n","Step: 98155, loss: 0.13640807569026947\n","Step: 98156, loss: 0.052169494330883026\n","Step: 98157, loss: 0.0451764315366745\n","Step: 98158, loss: 0.11928687244653702\n","Step: 98159, loss: 0.09551364183425903\n","Step: 98160, loss: 0.19655227661132812\n","Step: 98161, loss: 0.1183164119720459\n","Step: 98162, loss: 0.0\n","Step: 98163, loss: 0.19518513977527618\n","Step: 98164, loss: 0.13841408491134644\n","Step: 98165, loss: 0.17658933997154236\n","Step: 98166, loss: 0.1176236942410469\n","Step: 98167, loss: 0.15417973697185516\n","Step: 98168, loss: 0.16887114942073822\n","Step: 98169, loss: 0.05056711286306381\n","Step: 98170, loss: 0.09752221405506134\n","Step: 98171, loss: 0.036186013370752335\n","Step: 98172, loss: 0.06001046672463417\n","Step: 98173, loss: 0.0\n","Step: 98174, loss: 0.30284184217453003\n","Step: 98175, loss: 0.15817198157310486\n","Step: 98176, loss: 0.04876241087913513\n","Step: 98177, loss: 0.0\n","Step: 98178, loss: 0.24483740329742432\n","Step: 98179, loss: 0.18287064135074615\n","Step: 98180, loss: 0.21472501754760742\n","Step: 98181, loss: 0.363348126411438\n","Step: 98182, loss: 0.04559190571308136\n","Step: 98183, loss: 0.24520771205425262\n","Step: 98184, loss: 0.13919153809547424\n","Step: 98185, loss: 0.2460537552833557\n","Step: 98186, loss: 0.033102478832006454\n","Step: 98187, loss: 0.18242190778255463\n","Step: 98188, loss: 0.07693827897310257\n","Step: 98189, loss: 0.14716142416000366\n","Step: 98190, loss: 0.2594355046749115\n","Step: 98191, loss: 0.12175899744033813\n","Step: 98192, loss: 0.06469681859016418\n","Step: 98193, loss: 0.06363257020711899\n","Step: 98194, loss: 0.14048166573047638\n","Step: 98195, loss: 0.08197478204965591\n","Step: 98196, loss: 0.08212682604789734\n","Step: 98197, loss: 0.0844515860080719\n","Step: 98198, loss: 0.09504613280296326\n","Step: 98199, loss: 0.13200223445892334\n","Step: 98200, loss: 0.26365235447883606\n","Step: 98201, loss: 0.2668020725250244\n","Step: 98202, loss: 0.12361927330493927\n","Step: 98203, loss: 0.14768747985363007\n","Step: 98204, loss: 0.027899660170078278\n","Step: 98205, loss: 0.2205335646867752\n","Step: 98206, loss: 0.3231502175331116\n","Step: 98207, loss: 0.12798546254634857\n","Step: 98208, loss: 0.08130074292421341\n","Step: 98209, loss: 0.1505611538887024\n","Step: 98210, loss: 0.10349730402231216\n","Step: 98211, loss: 0.07829931378364563\n","Step: 98212, loss: 0.10521264374256134\n","Step: 98213, loss: 0.1273307353258133\n","Step: 98214, loss: 0.0\n","Step: 98215, loss: 0.07745519280433655\n","Step: 98216, loss: 0.1833411157131195\n","Step: 98217, loss: 0.16193082928657532\n","Step: 98218, loss: 0.0\n","Step: 98219, loss: 0.18326443433761597\n","Step: 98220, loss: 0.09580180794000626\n","Step: 98221, loss: 0.027839768677949905\n","Step: 98222, loss: 0.14172521233558655\n","Step: 98223, loss: 0.13054773211479187\n","Step: 98224, loss: 0.027387944981455803\n","Step: 98225, loss: 0.19844090938568115\n","Step: 98226, loss: 0.033282700926065445\n","Step: 98227, loss: 0.10628969222307205\n","Step: 98228, loss: 0.26197242736816406\n","Step: 98229, loss: 0.027479160577058792\n","Step: 98230, loss: 0.10723410546779633\n","Step: 98231, loss: 0.2318286895751953\n","Step: 98232, loss: 0.18827208876609802\n","Step: 98233, loss: 0.05990687012672424\n","Step: 98234, loss: 0.128303661942482\n","Step: 98235, loss: 0.03289389982819557\n","Step: 98236, loss: 0.06409423053264618\n","Step: 98237, loss: 0.11983978003263474\n","Step: 98238, loss: 0.03231215104460716\n","Step: 98239, loss: 0.14143681526184082\n","Step: 98240, loss: 0.3087957203388214\n","Step: 98241, loss: 0.09896660596132278\n","Step: 98242, loss: 0.11816065013408661\n","Step: 98243, loss: 0.07832013815641403\n","Step: 98244, loss: 0.2033604085445404\n","Step: 98245, loss: 0.17131851613521576\n","Step: 98246, loss: 0.0464920848608017\n","Step: 98247, loss: 0.09723982214927673\n","Step: 98248, loss: 0.1321742683649063\n","Step: 98249, loss: 0.1828031986951828\n","Step: 98250, loss: 0.1358938068151474\n","Step: 98251, loss: 0.18927013874053955\n","Step: 98252, loss: 0.11906116455793381\n","Step: 98253, loss: 0.24697932600975037\n","Step: 98254, loss: 0.0\n","Step: 98255, loss: 0.11914363503456116\n","Step: 98256, loss: 0.12687236070632935\n","Step: 98257, loss: 0.04494161531329155\n","Step: 98258, loss: 0.14723128080368042\n","Step: 98259, loss: 0.0\n","Step: 98260, loss: 0.1721321940422058\n","Step: 98261, loss: 0.24850735068321228\n","Step: 98262, loss: 0.14230814576148987\n","Step: 98263, loss: 0.06812217086553574\n","Step: 98264, loss: 0.0\n","Step: 98265, loss: 0.17498046159744263\n","Step: 98266, loss: 0.0\n","Step: 98267, loss: 0.030985569581389427\n","Step: 98268, loss: 0.08499744534492493\n","Step: 98269, loss: 0.0\n","Step: 98270, loss: 0.1338568925857544\n","Step: 98271, loss: 0.042044077068567276\n","Step: 98272, loss: 0.0\n","Step: 98273, loss: 0.23637117445468903\n","Step: 98274, loss: 0.244944766163826\n","Step: 98275, loss: 0.08813697844743729\n","Step: 98276, loss: 0.2896311581134796\n","Step: 98277, loss: 0.0\n","Step: 98278, loss: 0.041584957391023636\n","Step: 98279, loss: 0.1311066448688507\n","Step: 98280, loss: 0.13003912568092346\n","Step: 98281, loss: 0.1900961995124817\n","Step: 98282, loss: 0.11873488128185272\n","Step: 98283, loss: 0.03938821703195572\n","Step: 98284, loss: 0.17642952501773834\n","Step: 98285, loss: 0.15786536037921906\n","Step: 98286, loss: 0.1332208216190338\n","Step: 98287, loss: 0.05447376146912575\n","Step: 98288, loss: 0.2464715987443924\n","Step: 98289, loss: 0.12515033781528473\n","Step: 98290, loss: 0.21369411051273346\n","Step: 98291, loss: 0.07549864053726196\n","Step: 98292, loss: 0.26182621717453003\n","Step: 98293, loss: 0.07940272986888885\n","Step: 98294, loss: 0.1720811426639557\n","Step: 98295, loss: 0.04995523393154144\n","Step: 98296, loss: 0.07722553610801697\n","Step: 98297, loss: 0.038253191858530045\n","Step: 98298, loss: 0.11803381145000458\n","Step: 98299, loss: 0.037445344030857086\n","Step: 98300, loss: 0.0\n","Step: 98301, loss: 0.39236167073249817\n","Step: 98302, loss: 0.11159919947385788\n","Step: 98303, loss: 0.10658513009548187\n","Step: 98304, loss: 0.13355450332164764\n","Step: 98305, loss: 0.21821819245815277\n","Step: 98306, loss: 0.04998653754591942\n","Step: 98307, loss: 0.09464658796787262\n","Step: 98308, loss: 0.0843711867928505\n","Step: 98309, loss: 0.16064713895320892\n","Step: 98310, loss: 0.12800416350364685\n","Step: 98311, loss: 0.3227868378162384\n","Step: 98312, loss: 0.037420496344566345\n","Step: 98313, loss: 0.27456748485565186\n","Step: 98314, loss: 0.2101554125547409\n","Step: 98315, loss: 0.09447550028562546\n","Step: 98316, loss: 0.11081056296825409\n","Step: 98317, loss: 0.10561870783567429\n","Step: 98318, loss: 0.06626466661691666\n","Step: 98319, loss: 0.2361377328634262\n","Step: 98320, loss: 0.04068859666585922\n","Step: 98321, loss: 0.0\n","Step: 98322, loss: 0.0\n","Step: 98323, loss: 0.10103441029787064\n","Step: 98324, loss: 0.11798204481601715\n","Step: 98325, loss: 0.11825220286846161\n","Step: 98326, loss: 0.12825755774974823\n","Step: 98327, loss: 0.07830050587654114\n","Step: 98328, loss: 0.1098623052239418\n","Step: 98329, loss: 0.07425232231616974\n","Step: 98330, loss: 0.12423478066921234\n","Step: 98331, loss: 0.09257803857326508\n","Step: 98332, loss: 0.11207297444343567\n","Step: 98333, loss: 0.08714798092842102\n","Step: 98334, loss: 0.04979562386870384\n","Step: 98335, loss: 0.38090381026268005\n","Step: 98336, loss: 0.3459383249282837\n","Step: 98337, loss: 0.0\n","Step: 98338, loss: 0.1332397162914276\n","Step: 98339, loss: 0.09281466156244278\n","Step: 98340, loss: 0.03592078760266304\n","Step: 98341, loss: 0.0\n","Step: 98342, loss: 0.04989716410636902\n","Step: 98343, loss: 0.20757435262203217\n","Step: 98344, loss: 0.09256540983915329\n","Step: 98345, loss: 0.11438767611980438\n","Step: 98346, loss: 0.04763604328036308\n","Step: 98347, loss: 0.10595746338367462\n","Step: 98348, loss: 0.1622185856103897\n","Step: 98349, loss: 0.08374150097370148\n","Step: 98350, loss: 0.0\n","Step: 98351, loss: 0.11965243518352509\n","Step: 98352, loss: 0.04845426604151726\n","Step: 98353, loss: 0.10353601723909378\n","Step: 98354, loss: 0.23819683492183685\n","Step: 98355, loss: 0.0\n","Step: 98356, loss: 0.09475751966238022\n","Step: 98357, loss: 0.047013234347105026\n","Step: 98358, loss: 0.0\n","Step: 98359, loss: 0.14771722257137299\n","Step: 98360, loss: 0.17529800534248352\n","Step: 98361, loss: 0.3142097592353821\n","Step: 98362, loss: 0.07478873431682587\n","Step: 98363, loss: 0.04936752840876579\n","Step: 98364, loss: 0.12335879355669022\n","Step: 98365, loss: 0.21693997085094452\n","Step: 98366, loss: 0.0\n","Step: 98367, loss: 0.033373963087797165\n","Step: 98368, loss: 0.0931098461151123\n","Step: 98369, loss: 0.09312960505485535\n","Step: 98370, loss: 0.12153033167123795\n","Step: 98371, loss: 0.0\n","Step: 98372, loss: 0.09215480089187622\n","Step: 98373, loss: 0.16317737102508545\n","Step: 98374, loss: 0.22622795403003693\n","Step: 98375, loss: 0.0\n","Step: 98376, loss: 0.12504883110523224\n","Step: 98377, loss: 0.15153592824935913\n","Step: 98378, loss: 0.09841220825910568\n","Step: 98379, loss: 0.1334996223449707\n","Step: 98380, loss: 0.18482044339179993\n","Step: 98381, loss: 0.27853870391845703\n","Step: 98382, loss: 0.07895645499229431\n","Step: 98383, loss: 0.08765450119972229\n","Step: 98384, loss: 0.23972484469413757\n","Step: 98385, loss: 0.1498039960861206\n","Step: 98386, loss: 0.06972777098417282\n","Step: 98387, loss: 0.16723093390464783\n","Step: 98388, loss: 0.2512377202510834\n","Step: 98389, loss: 0.09973978996276855\n","Step: 98390, loss: 0.15531091392040253\n","Step: 98391, loss: 0.033669453114271164\n","Step: 98392, loss: 0.16382652521133423\n","Step: 98393, loss: 0.24036087095737457\n","Step: 98394, loss: 0.10421139001846313\n","Step: 98395, loss: 0.05804019793868065\n","Step: 98396, loss: 0.0\n","Step: 98397, loss: 0.04481075331568718\n","Step: 98398, loss: 0.15982668101787567\n","Step: 98399, loss: 0.19449123740196228\n","Step: 98400, loss: 0.18681268393993378\n","Step: 98401, loss: 0.17287272214889526\n","Step: 98402, loss: 0.046021949499845505\n","Step: 98403, loss: 0.07314490526914597\n","Step: 98404, loss: 0.21488244831562042\n","Step: 98405, loss: 0.22603219747543335\n","Step: 98406, loss: 0.19652967154979706\n","Step: 98407, loss: 0.14354875683784485\n","Step: 98408, loss: 0.2935525178909302\n","Step: 98409, loss: 0.069727823138237\n","Step: 98410, loss: 0.1398594081401825\n","Step: 98411, loss: 0.0\n","Step: 98412, loss: 0.23602651059627533\n","Step: 98413, loss: 0.18133068084716797\n","Step: 98414, loss: 0.08237538486719131\n","Step: 98415, loss: 0.21252766251564026\n","Step: 98416, loss: 0.2213861495256424\n","Step: 98417, loss: 0.1239902675151825\n","Step: 98418, loss: 0.11933320015668869\n","Step: 98419, loss: 0.09820947051048279\n","Step: 98420, loss: 0.1952982097864151\n","Step: 98421, loss: 0.13077695667743683\n","Step: 98422, loss: 0.27943533658981323\n","Step: 98423, loss: 0.05523640662431717\n","Step: 98424, loss: 0.1550920158624649\n","Step: 98425, loss: 0.300120085477829\n","Step: 98426, loss: 0.06542940437793732\n","Step: 98427, loss: 0.04352523013949394\n","Step: 98428, loss: 0.04729969799518585\n","Step: 98429, loss: 0.05250142514705658\n","Step: 98430, loss: 0.050892237573862076\n","Step: 98431, loss: 0.3266623318195343\n","Step: 98432, loss: 0.04347594082355499\n","Step: 98433, loss: 0.051860686391592026\n","Step: 98434, loss: 0.13365453481674194\n","Step: 98435, loss: 0.07919596880674362\n","Step: 98436, loss: 0.22163677215576172\n","Step: 98437, loss: 0.08073221147060394\n","Step: 98438, loss: 0.09708868712186813\n","Step: 98439, loss: 0.09688502550125122\n","Step: 98440, loss: 0.12916529178619385\n","Step: 98441, loss: 0.08579951524734497\n","Step: 98442, loss: 0.2448289841413498\n","Step: 98443, loss: 0.04644898325204849\n","Step: 98444, loss: 0.13700507581233978\n","Step: 98445, loss: 0.06700744479894638\n","Step: 98446, loss: 0.08192858099937439\n","Step: 98447, loss: 0.13499592244625092\n","Step: 98448, loss: 0.2159554809331894\n","Step: 98449, loss: 0.032532498240470886\n","Step: 98450, loss: 0.24609199166297913\n","Step: 98451, loss: 0.0\n","Step: 98452, loss: 0.0809568390250206\n","Step: 98453, loss: 0.13796575367450714\n","Step: 98454, loss: 0.08504265546798706\n","Step: 98455, loss: 0.3640071749687195\n","Step: 98456, loss: 0.1391659677028656\n","Step: 98457, loss: 0.17966052889823914\n","Step: 98458, loss: 0.14321424067020416\n","Step: 98459, loss: 0.1736394762992859\n","Step: 98460, loss: 0.0\n","Step: 98461, loss: 0.2109414041042328\n","Step: 98462, loss: 0.22280463576316833\n","Step: 98463, loss: 0.1663329154253006\n","Step: 98464, loss: 0.15011374652385712\n","Step: 98465, loss: 0.0\n","Step: 98466, loss: 0.2577455937862396\n","Step: 98467, loss: 0.18582923710346222\n","Step: 98468, loss: 0.17965151369571686\n","Step: 98469, loss: 0.08773452043533325\n","Step: 98470, loss: 0.07650869339704514\n","Step: 98471, loss: 0.0\n","Step: 98472, loss: 0.16271625459194183\n","Step: 98473, loss: 0.0\n","Step: 98474, loss: 0.29502561688423157\n","Step: 98475, loss: 0.13227728009223938\n","Step: 98476, loss: 0.050452861934900284\n","Step: 98477, loss: 0.20920628309249878\n","Step: 98478, loss: 0.15336337685585022\n","Step: 98479, loss: 0.039772018790245056\n","Step: 98480, loss: 0.04895106330513954\n","Step: 98481, loss: 0.20639067888259888\n","Step: 98482, loss: 0.13133974373340607\n","Step: 98483, loss: 0.043777480721473694\n","Step: 98484, loss: 0.0\n","Step: 98485, loss: 0.03819911181926727\n","Step: 98486, loss: 0.03965200483798981\n","Step: 98487, loss: 0.12659679353237152\n","Step: 98488, loss: 0.0974452868103981\n","Step: 98489, loss: 0.29063963890075684\n","Step: 98490, loss: 0.08356454223394394\n","Step: 98491, loss: 0.07448284327983856\n","Step: 98492, loss: 0.20846286416053772\n","Step: 98493, loss: 0.17646364867687225\n","Step: 98494, loss: 0.14339768886566162\n","Step: 98495, loss: 0.11305427551269531\n","Step: 98496, loss: 0.22511468827724457\n","Step: 98497, loss: 0.0\n","Step: 98498, loss: 0.0\n","Step: 98499, loss: 0.03399345278739929\n","Step: 98500, loss: 0.2602282464504242\n","Step: 98501, loss: 0.04838648810982704\n","Step: 98502, loss: 0.04757288843393326\n","Step: 98503, loss: 0.25779446959495544\n","Step: 98504, loss: 0.2664133310317993\n","Step: 98505, loss: 0.1830175518989563\n","Step: 98506, loss: 0.1885572224855423\n","Step: 98507, loss: 0.029646029695868492\n","Step: 98508, loss: 0.046986985951662064\n","Step: 98509, loss: 0.08228360861539841\n","Step: 98510, loss: 0.17883171141147614\n","Step: 98511, loss: 0.07636722922325134\n","Step: 98512, loss: 0.14892274141311646\n","Step: 98513, loss: 0.19639773666858673\n","Step: 98514, loss: 0.10823256522417068\n","Step: 98515, loss: 0.24173597991466522\n","Step: 98516, loss: 0.315499484539032\n","Step: 98517, loss: 0.17854928970336914\n","Step: 98518, loss: 0.11439470201730728\n","Step: 98519, loss: 0.10353244841098785\n","Step: 98520, loss: 0.0\n","Step: 98521, loss: 0.028194010257720947\n","Step: 98522, loss: 0.16982896625995636\n","Step: 98523, loss: 0.11863061785697937\n","Step: 98524, loss: 0.11160688102245331\n","Step: 98525, loss: 0.1226516142487526\n","Step: 98526, loss: 0.07521730661392212\n","Step: 98527, loss: 0.028100397437810898\n","Step: 98528, loss: 0.4456974267959595\n","Step: 98529, loss: 0.19386032223701477\n","Step: 98530, loss: 0.2767498791217804\n","Step: 98531, loss: 0.07609526813030243\n","Step: 98532, loss: 0.10665549337863922\n","Step: 98533, loss: 0.20609790086746216\n","Step: 98534, loss: 0.11592063307762146\n","Step: 98535, loss: 0.16321641206741333\n","Step: 98536, loss: 0.16821123659610748\n","Step: 98537, loss: 0.1559649407863617\n","Step: 98538, loss: 0.0\n","Step: 98539, loss: 0.2976551651954651\n","Step: 98540, loss: 0.22268159687519073\n","Step: 98541, loss: 0.04215247929096222\n","Step: 98542, loss: 0.1400560438632965\n","Step: 98543, loss: 0.04166535288095474\n","Step: 98544, loss: 0.0\n","Step: 98545, loss: 0.22854281961917877\n","Step: 98546, loss: 0.08711444586515427\n","Step: 98547, loss: 0.06405279040336609\n","Step: 98548, loss: 0.0\n","Step: 98549, loss: 0.09694171696901321\n","Step: 98550, loss: 0.2847241461277008\n","Step: 98551, loss: 0.23910994827747345\n","Step: 98552, loss: 0.19675171375274658\n","Step: 98553, loss: 0.17531409859657288\n","Step: 98554, loss: 0.0313306599855423\n","Step: 98555, loss: 0.24120822548866272\n","Step: 98556, loss: 0.05737180635333061\n","Step: 98557, loss: 0.25833457708358765\n","Step: 98558, loss: 0.04364033043384552\n","Step: 98559, loss: 0.0\n","Step: 98560, loss: 0.10875626653432846\n","Step: 98561, loss: 0.19896741211414337\n","Step: 98562, loss: 0.2289448231458664\n","Step: 98563, loss: 0.08243663609027863\n","Step: 98564, loss: 0.0437169224023819\n","Step: 98565, loss: 0.04049965739250183\n","Step: 98566, loss: 0.07362737506628036\n","Step: 98567, loss: 0.2664510905742645\n","Step: 98568, loss: 0.0\n","Step: 98569, loss: 0.2550166845321655\n","Step: 98570, loss: 0.0\n","Step: 98571, loss: 0.0\n","Step: 98572, loss: 0.15039224922657013\n","Step: 98573, loss: 0.17128676176071167\n","Step: 98574, loss: 0.08805269002914429\n","Step: 98575, loss: 0.11523975431919098\n","Step: 98576, loss: 0.26852938532829285\n","Step: 98577, loss: 0.0\n","Step: 98578, loss: 0.1888381987810135\n","Step: 98579, loss: 0.20429447293281555\n","Step: 98580, loss: 0.1018095314502716\n","Step: 98581, loss: 0.18496032059192657\n","Step: 98582, loss: 0.15279628336429596\n","Step: 98583, loss: 0.19806848466396332\n","Step: 98584, loss: 0.0\n","Step: 98585, loss: 0.10047426074743271\n","Step: 98586, loss: 0.1372239589691162\n","Step: 98587, loss: 0.09506740421056747\n","Step: 98588, loss: 0.16845948994159698\n","Step: 98589, loss: 0.11886483430862427\n","Step: 98590, loss: 0.1269225925207138\n","Step: 98591, loss: 0.11681751161813736\n","Step: 98592, loss: 0.17231152951717377\n","Step: 98593, loss: 0.07673916965723038\n","Step: 98594, loss: 0.2082623988389969\n","Step: 98595, loss: 0.0\n","Step: 98596, loss: 0.2602247893810272\n","Step: 98597, loss: 0.08225762099027634\n","Step: 98598, loss: 0.22282817959785461\n","Step: 98599, loss: 0.1411176323890686\n","Step: 98600, loss: 0.02934255450963974\n","Step: 98601, loss: 0.05678670108318329\n","Step: 98602, loss: 0.07706543803215027\n","Step: 98603, loss: 0.0581919401884079\n","Step: 98604, loss: 0.09626531600952148\n","Step: 98605, loss: 0.10053098946809769\n","Step: 98606, loss: 0.16479037702083588\n","Step: 98607, loss: 0.07151800394058228\n","Step: 98608, loss: 0.07406295835971832\n","Step: 98609, loss: 0.21976040303707123\n","Step: 98610, loss: 0.09166744351387024\n","Step: 98611, loss: 0.1417812556028366\n","Step: 98612, loss: 0.0438038595020771\n","Step: 98613, loss: 0.14980579912662506\n","Step: 98614, loss: 0.12327802926301956\n","Step: 98615, loss: 0.048690203577280045\n","Step: 98616, loss: 0.04383442923426628\n","Step: 98617, loss: 0.22493068873882294\n","Step: 98618, loss: 0.26303765177726746\n","Step: 98619, loss: 0.1319720596075058\n","Step: 98620, loss: 0.10884500294923782\n","Step: 98621, loss: 0.03006804548203945\n","Step: 98622, loss: 0.1807062178850174\n","Step: 98623, loss: 0.1508508175611496\n","Step: 98624, loss: 0.3245522379875183\n","Step: 98625, loss: 0.13409708440303802\n","Step: 98626, loss: 0.051279593259096146\n","Step: 98627, loss: 0.16580556333065033\n","Step: 98628, loss: 0.2420315146446228\n","Step: 98629, loss: 0.1563921868801117\n","Step: 98630, loss: 0.2147938758134842\n","Step: 98631, loss: 0.13411104679107666\n","Step: 98632, loss: 0.24921266734600067\n","Step: 98633, loss: 0.2589989900588989\n","Step: 98634, loss: 0.23684875667095184\n","Step: 98635, loss: 0.12305841594934464\n","Step: 98636, loss: 0.0\n","Step: 98637, loss: 0.09631780534982681\n","Step: 98638, loss: 0.2182716727256775\n","Step: 98639, loss: 0.07456955313682556\n","Step: 98640, loss: 0.1479637175798416\n","Step: 98641, loss: 0.19338656961917877\n","Step: 98642, loss: 0.13686221837997437\n","Step: 98643, loss: 0.1690385490655899\n","Step: 98644, loss: 0.036910947412252426\n","Step: 98645, loss: 0.10313094407320023\n","Step: 98646, loss: 0.24354080855846405\n","Step: 98647, loss: 0.037728700786828995\n","Step: 98648, loss: 0.03845585510134697\n","Step: 98649, loss: 0.12088745087385178\n","Step: 98650, loss: 0.09566119313240051\n","Step: 98651, loss: 0.15784765779972076\n","Step: 98652, loss: 0.03894035518169403\n","Step: 98653, loss: 0.07370773702859879\n","Step: 98654, loss: 0.05162849277257919\n","Step: 98655, loss: 0.0631164163351059\n","Step: 98656, loss: 0.12111165374517441\n","Step: 98657, loss: 0.16551649570465088\n","Step: 98658, loss: 0.1370452344417572\n","Step: 98659, loss: 0.10157208144664764\n","Step: 98660, loss: 0.23852865397930145\n","Step: 98661, loss: 0.2052399218082428\n","Step: 98662, loss: 0.0\n","Step: 98663, loss: 0.0\n","Step: 98664, loss: 0.07161754369735718\n","Step: 98665, loss: 0.08337225019931793\n","Step: 98666, loss: 0.14450542628765106\n","Step: 98667, loss: 0.24065575003623962\n","Step: 98668, loss: 0.0\n","Step: 98669, loss: 0.04905024170875549\n","Step: 98670, loss: 0.1974961906671524\n","Step: 98671, loss: 0.07092040777206421\n","Step: 98672, loss: 0.08754346519708633\n","Step: 98673, loss: 0.03993075713515282\n","Step: 98674, loss: 0.04858841001987457\n","Step: 98675, loss: 0.20872633159160614\n","Step: 98676, loss: 0.0380648709833622\n","Step: 98677, loss: 0.08315400779247284\n","Step: 98678, loss: 0.23404459655284882\n","Step: 98679, loss: 0.29888126254081726\n","Step: 98680, loss: 0.1492891013622284\n","Step: 98681, loss: 0.18390770256519318\n","Step: 98682, loss: 0.0\n","Step: 98683, loss: 0.14202724397182465\n","Step: 98684, loss: 0.1577293425798416\n","Step: 98685, loss: 0.19531561434268951\n","Step: 98686, loss: 0.08786813914775848\n","Step: 98687, loss: 0.17290857434272766\n","Step: 98688, loss: 0.0460919626057148\n","Step: 98689, loss: 0.0964321419596672\n","Step: 98690, loss: 0.19047637283802032\n","Step: 98691, loss: 0.2105736881494522\n","Step: 98692, loss: 0.14944739639759064\n","Step: 98693, loss: 0.0\n","Step: 98694, loss: 0.09307540953159332\n","Step: 98695, loss: 0.32343021035194397\n","Step: 98696, loss: 0.22605128586292267\n","Step: 98697, loss: 0.06757216155529022\n","Step: 98698, loss: 0.09612850099802017\n","Step: 98699, loss: 0.2751266360282898\n","Step: 98700, loss: 0.10298527032136917\n","Step: 98701, loss: 0.0\n","Step: 98702, loss: 0.12999005615711212\n","Step: 98703, loss: 0.12571479380130768\n","Step: 98704, loss: 0.11998652666807175\n","Step: 98705, loss: 0.14010822772979736\n","Step: 98706, loss: 0.1785261482000351\n","Step: 98707, loss: 0.2167588770389557\n","Step: 98708, loss: 0.0\n","Step: 98709, loss: 0.04816412553191185\n","Step: 98710, loss: 0.0802329033613205\n","Step: 98711, loss: 0.0\n","Step: 98712, loss: 0.2209613174200058\n","Step: 98713, loss: 0.1255587935447693\n","Step: 98714, loss: 0.08935702592134476\n","Step: 98715, loss: 0.08597797155380249\n","Step: 98716, loss: 0.09458762407302856\n","Step: 98717, loss: 0.0\n","Step: 98718, loss: 0.08565524965524673\n","Step: 98719, loss: 0.1263926923274994\n","Step: 98720, loss: 0.2798077464103699\n","Step: 98721, loss: 0.23698051273822784\n","Step: 98722, loss: 0.11205488443374634\n","Step: 98723, loss: 0.4162285625934601\n","Step: 98724, loss: 0.08643245697021484\n","Step: 98725, loss: 0.16857928037643433\n","Step: 98726, loss: 0.2835811376571655\n","Step: 98727, loss: 0.18185892701148987\n","Step: 98728, loss: 0.08027426898479462\n","Step: 98729, loss: 0.0\n","Step: 98730, loss: 0.08489394932985306\n","Step: 98731, loss: 0.17035144567489624\n","Step: 98732, loss: 0.12528614699840546\n","Step: 98733, loss: 0.0\n","Step: 98734, loss: 0.15033869445323944\n","Step: 98735, loss: 0.12880143523216248\n","Step: 98736, loss: 0.23604129254817963\n","Step: 98737, loss: 0.2539077401161194\n","Step: 98738, loss: 0.07750877737998962\n","Step: 98739, loss: 0.07302426546812057\n","Step: 98740, loss: 0.18204401433467865\n","Step: 98741, loss: 0.21902631223201752\n","Step: 98742, loss: 0.3481568396091461\n","Step: 98743, loss: 0.05137346312403679\n","Step: 98744, loss: 0.09754464030265808\n","Step: 98745, loss: 0.0\n","Step: 98746, loss: 0.12387092411518097\n","Step: 98747, loss: 0.09091199189424515\n","Step: 98748, loss: 0.03153172880411148\n","Step: 98749, loss: 0.09928706288337708\n","Step: 98750, loss: 0.23320715129375458\n","Step: 98751, loss: 0.07518338412046432\n","Step: 98752, loss: 0.05428893864154816\n","Step: 98753, loss: 0.12225566059350967\n","Step: 98754, loss: 0.16253718733787537\n","Step: 98755, loss: 0.19769981503486633\n","Step: 98756, loss: 0.0\n","Step: 98757, loss: 0.23434054851531982\n","Step: 98758, loss: 0.04156814515590668\n","Step: 98759, loss: 0.07671765983104706\n","Step: 98760, loss: 0.030091125518083572\n","Step: 98761, loss: 0.0964055135846138\n","Step: 98762, loss: 0.15987353026866913\n","Step: 98763, loss: 0.050665177404880524\n","Step: 98764, loss: 0.33489540219306946\n","Step: 98765, loss: 0.14925234019756317\n","Step: 98766, loss: 0.21451616287231445\n","Step: 98767, loss: 0.1413922756910324\n","Step: 98768, loss: 0.2796027660369873\n","Step: 98769, loss: 0.07886022329330444\n","Step: 98770, loss: 0.032096900045871735\n","Step: 98771, loss: 0.16867122054100037\n","Step: 98772, loss: 0.09356308728456497\n","Step: 98773, loss: 0.11900296807289124\n","Step: 98774, loss: 0.03272887319326401\n","Step: 98775, loss: 0.15069638192653656\n","Step: 98776, loss: 0.07381343096494675\n","Step: 98777, loss: 0.14711375534534454\n","Step: 98778, loss: 0.19948863983154297\n","Step: 98779, loss: 0.1422613114118576\n","Step: 98780, loss: 0.25396186113357544\n","Step: 98781, loss: 0.03195253387093544\n","Step: 98782, loss: 0.2113652378320694\n","Step: 98783, loss: 0.13925430178642273\n","Step: 98784, loss: 0.25207459926605225\n","Step: 98785, loss: 0.09499066323041916\n","Step: 98786, loss: 0.10153251886367798\n","Step: 98787, loss: 0.15009695291519165\n","Step: 98788, loss: 0.0\n","Step: 98789, loss: 0.15243946015834808\n","Step: 98790, loss: 0.21351438760757446\n","Step: 98791, loss: 0.029091767966747284\n","Step: 98792, loss: 0.1392621099948883\n","Step: 98793, loss: 0.12368085980415344\n","Step: 98794, loss: 0.12172332406044006\n","Step: 98795, loss: 0.12871211767196655\n","Step: 98796, loss: 0.2102379947900772\n","Step: 98797, loss: 0.04650704562664032\n","Step: 98798, loss: 0.05948305130004883\n","Step: 98799, loss: 0.06326862424612045\n","Step: 98800, loss: 0.1890527307987213\n","Step: 98801, loss: 0.0\n","Step: 98802, loss: 0.056262291967868805\n","Step: 98803, loss: 0.04733655974268913\n","Step: 98804, loss: 0.16830916702747345\n","Step: 98805, loss: 0.13838480412960052\n","Step: 98806, loss: 0.05192701518535614\n","Step: 98807, loss: 0.10105951130390167\n","Step: 98808, loss: 0.07902393490076065\n","Step: 98809, loss: 0.057682447135448456\n","Step: 98810, loss: 0.18954412639141083\n","Step: 98811, loss: 0.15435275435447693\n","Step: 98812, loss: 0.26576003432273865\n","Step: 98813, loss: 0.139744371175766\n","Step: 98814, loss: 0.20278732478618622\n","Step: 98815, loss: 0.14679645001888275\n","Step: 98816, loss: 0.10786127299070358\n","Step: 98817, loss: 0.10610951483249664\n","Step: 98818, loss: 0.045567501336336136\n","Step: 98819, loss: 0.08982593566179276\n","Step: 98820, loss: 0.1937391608953476\n","Step: 98821, loss: 0.253182977437973\n","Step: 98822, loss: 0.19388730823993683\n","Step: 98823, loss: 0.041870880872011185\n","Step: 98824, loss: 0.32593151926994324\n","Step: 98825, loss: 0.18834906816482544\n","Step: 98826, loss: 0.15970346331596375\n","Step: 98827, loss: 0.1493973582983017\n","Step: 98828, loss: 0.0\n","Step: 98829, loss: 0.0\n","Step: 98830, loss: 0.3310982882976532\n","Step: 98831, loss: 0.08183063566684723\n","Step: 98832, loss: 0.12001386284828186\n","Step: 98833, loss: 0.04535331204533577\n","Step: 98834, loss: 0.09888233989477158\n","Step: 98835, loss: 0.17639896273612976\n","Step: 98836, loss: 0.2337159663438797\n","Step: 98837, loss: 0.05251700058579445\n","Step: 98838, loss: 0.2252964824438095\n","Step: 98839, loss: 0.04017399623990059\n","Step: 98840, loss: 0.22649534046649933\n","Step: 98841, loss: 0.0\n","Step: 98842, loss: 0.12843847274780273\n","Step: 98843, loss: 0.22847209870815277\n","Step: 98844, loss: 0.040497809648513794\n","Step: 98845, loss: 0.12738393247127533\n","Step: 98846, loss: 0.05753033235669136\n","Step: 98847, loss: 0.15126724541187286\n","Step: 98848, loss: 0.16730996966362\n","Step: 98849, loss: 0.32295843958854675\n","Step: 98850, loss: 0.20549611747264862\n","Step: 98851, loss: 0.12853650748729706\n","Step: 98852, loss: 0.2288869023323059\n","Step: 98853, loss: 0.22334741055965424\n","Step: 98854, loss: 0.07640089839696884\n","Step: 98855, loss: 0.23830562829971313\n","Step: 98856, loss: 0.04266351833939552\n","Step: 98857, loss: 0.03329586610198021\n","Step: 98858, loss: 0.12163808196783066\n","Step: 98859, loss: 0.0\n","Step: 98860, loss: 0.11318754404783249\n","Step: 98861, loss: 0.1023818701505661\n","Step: 98862, loss: 0.12061604857444763\n","Step: 98863, loss: 0.06386727839708328\n","Step: 98864, loss: 0.13812848925590515\n","Step: 98865, loss: 0.162742018699646\n","Step: 98866, loss: 0.1303883045911789\n","Step: 98867, loss: 0.21638000011444092\n","Step: 98868, loss: 0.13465037941932678\n","Step: 98869, loss: 0.26543158292770386\n","Step: 98870, loss: 0.07537511736154556\n","Step: 98871, loss: 0.3450687527656555\n","Step: 98872, loss: 0.05043358728289604\n","Step: 98873, loss: 0.2798297107219696\n","Step: 98874, loss: 0.0\n","Step: 98875, loss: 0.17657224833965302\n","Step: 98876, loss: 0.087997205555439\n","Step: 98877, loss: 0.08770952373743057\n","Step: 98878, loss: 0.2162223756313324\n","Step: 98879, loss: 0.18478024005889893\n","Step: 98880, loss: 0.08845547586679459\n","Step: 98881, loss: 0.18907102942466736\n","Step: 98882, loss: 0.06642036139965057\n","Step: 98883, loss: 0.0\n","Step: 98884, loss: 0.0\n","Step: 98885, loss: 0.1582331657409668\n","Step: 98886, loss: 0.04526853561401367\n","Step: 98887, loss: 0.04487185552716255\n","Step: 98888, loss: 0.07372596859931946\n","Step: 98889, loss: 0.04356178641319275\n","Step: 98890, loss: 0.16470356285572052\n","Step: 98891, loss: 0.051022883504629135\n","Step: 98892, loss: 0.09532202035188675\n","Step: 98893, loss: 0.18049806356430054\n","Step: 98894, loss: 0.2454594075679779\n","Step: 98895, loss: 0.12961500883102417\n","Step: 98896, loss: 0.16302351653575897\n","Step: 98897, loss: 0.2926681935787201\n","Step: 98898, loss: 0.1493968367576599\n","Step: 98899, loss: 0.035812702029943466\n","Step: 98900, loss: 0.10877075046300888\n","Step: 98901, loss: 0.053746528923511505\n","Step: 98902, loss: 0.03560200333595276\n","Step: 98903, loss: 0.1222408264875412\n","Step: 98904, loss: 0.15333817899227142\n","Step: 98905, loss: 0.0\n","Step: 98906, loss: 0.19815070927143097\n","Step: 98907, loss: 0.12532281875610352\n","Step: 98908, loss: 0.2351686805486679\n","Step: 98909, loss: 0.11310301721096039\n","Step: 98910, loss: 0.0\n","Step: 98911, loss: 0.18874290585517883\n","Step: 98912, loss: 0.07052988559007645\n","Step: 98913, loss: 0.13455310463905334\n","Step: 98914, loss: 0.056076232343912125\n","Step: 98915, loss: 0.06807222217321396\n","Step: 98916, loss: 0.044529471546411514\n","Step: 98917, loss: 0.1356663703918457\n","Step: 98918, loss: 0.1813010424375534\n","Step: 98919, loss: 0.1431836038827896\n","Step: 98920, loss: 0.1533673256635666\n","Step: 98921, loss: 0.08509579300880432\n","Step: 98922, loss: 0.07085473835468292\n","Step: 98923, loss: 0.0\n","Step: 98924, loss: 0.03071868233382702\n","Step: 98925, loss: 0.17743267118930817\n","Step: 98926, loss: 0.10160423070192337\n","Step: 98927, loss: 0.04156158119440079\n","Step: 98928, loss: 0.10739289969205856\n","Step: 98929, loss: 0.13108080625534058\n","Step: 98930, loss: 0.0\n","Step: 98931, loss: 0.2288810908794403\n","Step: 98932, loss: 0.33488166332244873\n","Step: 98933, loss: 0.21233567595481873\n","Step: 98934, loss: 0.06610367447137833\n","Step: 98935, loss: 0.0\n","Step: 98936, loss: 0.11400003731250763\n","Step: 98937, loss: 0.0\n","Step: 98938, loss: 0.05649813637137413\n","Step: 98939, loss: 0.12786270678043365\n","Step: 98940, loss: 0.10461772233247757\n","Step: 98941, loss: 0.10853312164545059\n","Step: 98942, loss: 0.09883733838796616\n","Step: 98943, loss: 0.0\n","Step: 98944, loss: 0.03504766523838043\n","Step: 98945, loss: 0.12224334478378296\n","Step: 98946, loss: 0.05832593888044357\n","Step: 98947, loss: 0.21965819597244263\n","Step: 98948, loss: 0.16821657121181488\n","Step: 98949, loss: 0.2971997559070587\n","Step: 98950, loss: 0.33567628264427185\n","Step: 98951, loss: 0.07166393101215363\n","Step: 98952, loss: 0.19425460696220398\n","Step: 98953, loss: 0.09308972209692001\n","Step: 98954, loss: 0.0\n","Step: 98955, loss: 0.05298694968223572\n","Step: 98956, loss: 0.0374533087015152\n","Step: 98957, loss: 0.17754800617694855\n","Step: 98958, loss: 0.06154138594865799\n","Step: 98959, loss: 0.049880482256412506\n","Step: 98960, loss: 0.1964718997478485\n","Step: 98961, loss: 0.19540542364120483\n","Step: 98962, loss: 0.14424701035022736\n","Step: 98963, loss: 0.1660417914390564\n","Step: 98964, loss: 0.1099005863070488\n","Step: 98965, loss: 0.09372418373823166\n","Step: 98966, loss: 0.088300921022892\n","Step: 98967, loss: 0.0\n","Step: 98968, loss: 0.18590949475765228\n","Step: 98969, loss: 0.25282013416290283\n","Step: 98970, loss: 0.03822149336338043\n","Step: 98971, loss: 0.21932780742645264\n","Step: 98972, loss: 0.02704259566962719\n","Step: 98973, loss: 0.17154745757579803\n","Step: 98974, loss: 0.39694541692733765\n","Step: 98975, loss: 0.026924559846520424\n","Step: 98976, loss: 0.11759431660175323\n","Step: 98977, loss: 0.29323941469192505\n","Step: 98978, loss: 0.2972780168056488\n","Step: 98979, loss: 0.0\n","Step: 98980, loss: 0.08139412850141525\n","Step: 98981, loss: 0.15902315080165863\n","Step: 98982, loss: 0.1128687635064125\n","Step: 98983, loss: 0.0\n","Step: 98984, loss: 0.25605708360671997\n","Step: 98985, loss: 0.1925206035375595\n","Step: 98986, loss: 0.15144333243370056\n","Step: 98987, loss: 0.0547783188521862\n","Step: 98988, loss: 0.2039223462343216\n","Step: 98989, loss: 0.2653103172779083\n","Step: 98990, loss: 0.1177087053656578\n","Step: 98991, loss: 0.0\n","Step: 98992, loss: 0.08521068841218948\n","Step: 98993, loss: 0.0\n","Step: 98994, loss: 0.1325421780347824\n","Step: 98995, loss: 0.1607915163040161\n","Step: 98996, loss: 0.15668246150016785\n","Step: 98997, loss: 0.0870351791381836\n","Step: 98998, loss: 0.29584014415740967\n","Step: 98999, loss: 0.1590421050786972\n","Step: 99000, loss: 0.06017804145812988\n","Step: 99001, loss: 0.11285669356584549\n","Step: 99002, loss: 0.1359347403049469\n","Step: 99003, loss: 0.07236560434103012\n","Step: 99004, loss: 0.15265145897865295\n","Step: 99005, loss: 0.2787805199623108\n","Step: 99006, loss: 0.06968609243631363\n","Step: 99007, loss: 0.048389796167612076\n","Step: 99008, loss: 0.0\n","Step: 99009, loss: 0.3232990503311157\n","Step: 99010, loss: 0.0\n","Step: 99011, loss: 0.09964577853679657\n","Step: 99012, loss: 0.08793802559375763\n","Step: 99013, loss: 0.23120932281017303\n","Step: 99014, loss: 0.1588371843099594\n","Step: 99015, loss: 0.03271611034870148\n","Step: 99016, loss: 0.047853268682956696\n","Step: 99017, loss: 0.05939369276165962\n","Step: 99018, loss: 0.09818725287914276\n","Step: 99019, loss: 0.0\n","Step: 99020, loss: 0.16336841881275177\n","Step: 99021, loss: 0.0792236402630806\n","Step: 99022, loss: 0.06757228076457977\n","Step: 99023, loss: 0.06678745150566101\n","Step: 99024, loss: 0.17281731963157654\n","Step: 99025, loss: 0.1367727667093277\n","Step: 99026, loss: 0.12711578607559204\n","Step: 99027, loss: 0.23120039701461792\n","Step: 99028, loss: 0.24588824808597565\n","Step: 99029, loss: 0.030259592458605766\n","Step: 99030, loss: 0.1855279952287674\n","Step: 99031, loss: 0.2800918221473694\n","Step: 99032, loss: 0.07851599901914597\n","Step: 99033, loss: 0.05152784287929535\n","Step: 99034, loss: 0.21962596476078033\n","Step: 99035, loss: 0.21539992094039917\n","Step: 99036, loss: 0.10548656433820724\n","Step: 99037, loss: 0.13408271968364716\n","Step: 99038, loss: 0.03102591447532177\n","Step: 99039, loss: 0.2578555643558502\n","Step: 99040, loss: 0.1353142410516739\n","Step: 99041, loss: 0.0386158786714077\n","Step: 99042, loss: 0.08006072789430618\n","Step: 99043, loss: 0.0\n","Step: 99044, loss: 0.12105290591716766\n","Step: 99045, loss: 0.07529551535844803\n","Step: 99046, loss: 0.09803895652294159\n","Step: 99047, loss: 0.13303284347057343\n","Step: 99048, loss: 0.408644437789917\n","Step: 99049, loss: 0.04392428323626518\n","Step: 99050, loss: 0.21918189525604248\n","Step: 99051, loss: 0.08123254776000977\n","Step: 99052, loss: 0.08402418345212936\n","Step: 99053, loss: 0.0\n","Step: 99054, loss: 0.279674768447876\n","Step: 99055, loss: 0.09711549431085587\n","Step: 99056, loss: 0.07503450661897659\n","Step: 99057, loss: 0.1373785436153412\n","Step: 99058, loss: 0.09778953343629837\n","Step: 99059, loss: 0.06959874927997589\n","Step: 99060, loss: 0.1705981194972992\n","Step: 99061, loss: 0.0\n","Step: 99062, loss: 0.15917128324508667\n","Step: 99063, loss: 0.21070635318756104\n","Step: 99064, loss: 0.08436854183673859\n","Step: 99065, loss: 0.33069369196891785\n","Step: 99066, loss: 0.12221243232488632\n","Step: 99067, loss: 0.24471990764141083\n","Step: 99068, loss: 0.2825320363044739\n","Step: 99069, loss: 0.09172330051660538\n","Step: 99070, loss: 0.17528976500034332\n","Step: 99071, loss: 0.1158866137266159\n","Step: 99072, loss: 0.17296858131885529\n","Step: 99073, loss: 0.13663023710250854\n","Step: 99074, loss: 0.07518196105957031\n","Step: 99075, loss: 0.07108528912067413\n","Step: 99076, loss: 0.22681352496147156\n","Step: 99077, loss: 0.17883367836475372\n","Step: 99078, loss: 0.031268369406461716\n","Step: 99079, loss: 0.11230193823575974\n","Step: 99080, loss: 0.14045444130897522\n","Step: 99081, loss: 0.29729190468788147\n","Step: 99082, loss: 0.12850122153759003\n","Step: 99083, loss: 0.15973779559135437\n","Step: 99084, loss: 0.09895852953195572\n","Step: 99085, loss: 0.19431671500205994\n","Step: 99086, loss: 0.16353648900985718\n","Step: 99087, loss: 0.05340007692575455\n","Step: 99088, loss: 0.0677761361002922\n","Step: 99089, loss: 0.1303480863571167\n","Step: 99090, loss: 0.1699230968952179\n","Step: 99091, loss: 0.04798997938632965\n","Step: 99092, loss: 0.2200850248336792\n","Step: 99093, loss: 0.2503458261489868\n","Step: 99094, loss: 0.0\n","Step: 99095, loss: 0.17783382534980774\n","Step: 99096, loss: 0.1398770660161972\n","Step: 99097, loss: 0.14126691222190857\n","Step: 99098, loss: 0.09072326868772507\n","Step: 99099, loss: 0.1637587696313858\n","Step: 99100, loss: 0.04153883084654808\n","Step: 99101, loss: 0.212234765291214\n","Step: 99102, loss: 0.1258389800786972\n","Step: 99103, loss: 0.12558287382125854\n","Step: 99104, loss: 0.11656728386878967\n","Step: 99105, loss: 0.11262069642543793\n","Step: 99106, loss: 0.1191348135471344\n","Step: 99107, loss: 0.09206147491931915\n","Step: 99108, loss: 0.11446759849786758\n","Step: 99109, loss: 0.1737835556268692\n","Step: 99110, loss: 0.07579350471496582\n","Step: 99111, loss: 0.08035419136285782\n","Step: 99112, loss: 0.036335136741399765\n","Step: 99113, loss: 0.04303564876317978\n","Step: 99114, loss: 0.12581320106983185\n","Step: 99115, loss: 0.1358451545238495\n","Step: 99116, loss: 0.09784659743309021\n","Step: 99117, loss: 0.19917243719100952\n","Step: 99118, loss: 0.11186090111732483\n","Step: 99119, loss: 0.05417364090681076\n","Step: 99120, loss: 0.07389789819717407\n","Step: 99121, loss: 0.07720790058374405\n","Step: 99122, loss: 0.12473750859498978\n","Step: 99123, loss: 0.03325951471924782\n","Step: 99124, loss: 0.13800190389156342\n","Step: 99125, loss: 0.14720723032951355\n","Step: 99126, loss: 0.2508959174156189\n","Step: 99127, loss: 0.15511487424373627\n","Step: 99128, loss: 0.0\n","Step: 99129, loss: 0.18056243658065796\n","Step: 99130, loss: 0.1791772097349167\n","Step: 99131, loss: 0.15383197367191315\n","Step: 99132, loss: 0.0\n","Step: 99133, loss: 0.1213972344994545\n","Step: 99134, loss: 0.16439132392406464\n","Step: 99135, loss: 0.0\n","Step: 99136, loss: 0.03253738954663277\n","Step: 99137, loss: 0.05249536409974098\n","Step: 99138, loss: 0.18430766463279724\n","Step: 99139, loss: 0.08781927824020386\n","Step: 99140, loss: 0.2692852318286896\n","Step: 99141, loss: 0.1537361741065979\n","Step: 99142, loss: 0.08132032305002213\n","Step: 99143, loss: 0.09439717233181\n","Step: 99144, loss: 0.0778827965259552\n","Step: 99145, loss: 0.1978486180305481\n","Step: 99146, loss: 0.0\n","Step: 99147, loss: 0.08282371610403061\n","Step: 99148, loss: 0.11573862284421921\n","Step: 99149, loss: 0.0\n","Step: 99150, loss: 0.07219824194908142\n","Step: 99151, loss: 0.163868710398674\n","Step: 99152, loss: 0.21790176630020142\n","Step: 99153, loss: 0.1279268115758896\n","Step: 99154, loss: 0.04171168431639671\n","Step: 99155, loss: 0.20431652665138245\n","Step: 99156, loss: 0.1136179193854332\n","Step: 99157, loss: 0.14796705543994904\n","Step: 99158, loss: 0.07679884135723114\n","Step: 99159, loss: 0.053793009370565414\n","Step: 99160, loss: 0.12433044612407684\n","Step: 99161, loss: 0.13749587535858154\n","Step: 99162, loss: 0.04695296287536621\n","Step: 99163, loss: 0.2450091540813446\n","Step: 99164, loss: 0.14793188869953156\n","Step: 99165, loss: 0.17744866013526917\n","Step: 99166, loss: 0.0\n","Step: 99167, loss: 0.10132739692926407\n","Step: 99168, loss: 0.2631572484970093\n","Step: 99169, loss: 0.09356945008039474\n","Step: 99170, loss: 0.16082444787025452\n","Step: 99171, loss: 0.11111436784267426\n","Step: 99172, loss: 0.12593553960323334\n","Step: 99173, loss: 0.1811772584915161\n","Step: 99174, loss: 0.2667733430862427\n","Step: 99175, loss: 0.02925010398030281\n","Step: 99176, loss: 0.15785987675189972\n","Step: 99177, loss: 0.048790160566568375\n","Step: 99178, loss: 0.09519398957490921\n","Step: 99179, loss: 0.030236706137657166\n","Step: 99180, loss: 0.12941990792751312\n","Step: 99181, loss: 0.15133994817733765\n","Step: 99182, loss: 0.2835179567337036\n","Step: 99183, loss: 0.2669842541217804\n","Step: 99184, loss: 0.07173855602741241\n","Step: 99185, loss: 0.0\n","Step: 99186, loss: 0.17585082352161407\n","Step: 99187, loss: 0.17472173273563385\n","Step: 99188, loss: 0.0\n","Step: 99189, loss: 0.17564822733402252\n","Step: 99190, loss: 0.2876181900501251\n","Step: 99191, loss: 0.15427815914154053\n","Step: 99192, loss: 0.16230030357837677\n","Step: 99193, loss: 0.08428327739238739\n","Step: 99194, loss: 0.1301000714302063\n","Step: 99195, loss: 0.07802005857229233\n","Step: 99196, loss: 0.12934647500514984\n","Step: 99197, loss: 0.20750635862350464\n","Step: 99198, loss: 0.1199556365609169\n","Step: 99199, loss: 0.07933253049850464\n","Step: 99200, loss: 0.14688514173030853\n","Step: 99201, loss: 0.21275852620601654\n","Step: 99202, loss: 0.0781485065817833\n","Step: 99203, loss: 0.03760930150747299\n","Step: 99204, loss: 0.06372120976448059\n","Step: 99205, loss: 0.1668473482131958\n","Step: 99206, loss: 0.16237527132034302\n","Step: 99207, loss: 0.17960262298583984\n","Step: 99208, loss: 0.0858440175652504\n","Step: 99209, loss: 0.23703579604625702\n","Step: 99210, loss: 0.04067356139421463\n","Step: 99211, loss: 0.21079491078853607\n","Step: 99212, loss: 0.16268564760684967\n","Step: 99213, loss: 0.1961836963891983\n","Step: 99214, loss: 0.189017191529274\n","Step: 99215, loss: 0.03244160860776901\n","Step: 99216, loss: 0.050340842455625534\n","Step: 99217, loss: 0.26482081413269043\n","Step: 99218, loss: 0.1347220540046692\n","Step: 99219, loss: 0.04313277080655098\n","Step: 99220, loss: 0.04163086786866188\n","Step: 99221, loss: 0.21527710556983948\n","Step: 99222, loss: 0.5020897388458252\n","Step: 99223, loss: 0.0\n","Step: 99224, loss: 0.35133087635040283\n","Step: 99225, loss: 0.12459518760442734\n","Step: 99226, loss: 0.2032942920923233\n","Step: 99227, loss: 0.21852819621562958\n","Step: 99228, loss: 0.09095501899719238\n","Step: 99229, loss: 0.19619013369083405\n","Step: 99230, loss: 0.07093096524477005\n","Step: 99231, loss: 0.09977994114160538\n","Step: 99232, loss: 0.12316778302192688\n","Step: 99233, loss: 0.053337763994932175\n","Step: 99234, loss: 0.08551307022571564\n","Step: 99235, loss: 0.1581708937883377\n","Step: 99236, loss: 0.28310132026672363\n","Step: 99237, loss: 0.21888870000839233\n","Step: 99238, loss: 0.14063255488872528\n","Step: 99239, loss: 0.03502865508198738\n","Step: 99240, loss: 0.07454951107501984\n","Step: 99241, loss: 0.1738172322511673\n","Step: 99242, loss: 0.12349487096071243\n","Step: 99243, loss: 0.1670927107334137\n","Step: 99244, loss: 0.03276682272553444\n","Step: 99245, loss: 0.0\n","Step: 99246, loss: 0.22183384001255035\n","Step: 99247, loss: 0.1025383248925209\n","Step: 99248, loss: 0.15802159905433655\n","Step: 99249, loss: 0.0\n","Step: 99250, loss: 0.0\n","Step: 99251, loss: 0.08762706816196442\n","Step: 99252, loss: 0.07557734847068787\n","Step: 99253, loss: 0.10400097817182541\n","Step: 99254, loss: 0.14392274618148804\n","Step: 99255, loss: 0.0696784108877182\n","Step: 99256, loss: 0.0991332083940506\n","Step: 99257, loss: 0.2196984440088272\n","Step: 99258, loss: 0.25027093291282654\n","Step: 99259, loss: 0.0686059519648552\n","Step: 99260, loss: 0.04422230273485184\n","Step: 99261, loss: 0.14554941654205322\n","Step: 99262, loss: 0.13360685110092163\n","Step: 99263, loss: 0.11731573939323425\n","Step: 99264, loss: 0.0\n","Step: 99265, loss: 0.07977188378572464\n","Step: 99266, loss: 0.18148653209209442\n","Step: 99267, loss: 0.03889203071594238\n","Step: 99268, loss: 0.07080978900194168\n","Step: 99269, loss: 0.17021606862545013\n","Step: 99270, loss: 0.312835156917572\n","Step: 99271, loss: 0.027666307985782623\n","Step: 99272, loss: 0.12482969462871552\n","Step: 99273, loss: 0.08636587113142014\n","Step: 99274, loss: 0.09635380655527115\n","Step: 99275, loss: 0.043377045542001724\n","Step: 99276, loss: 0.16329596936702728\n","Step: 99277, loss: 0.1293479949235916\n","Step: 99278, loss: 0.09466557204723358\n","Step: 99279, loss: 0.10856229066848755\n","Step: 99280, loss: 0.04327859729528427\n","Step: 99281, loss: 0.26751092076301575\n","Step: 99282, loss: 0.11219169944524765\n","Step: 99283, loss: 0.10549180954694748\n","Step: 99284, loss: 0.0\n","Step: 99285, loss: 0.15894542634487152\n","Step: 99286, loss: 0.0\n","Step: 99287, loss: 0.1732761263847351\n","Step: 99288, loss: 0.21134036779403687\n","Step: 99289, loss: 0.0397147461771965\n","Step: 99290, loss: 0.16251140832901\n","Step: 99291, loss: 0.08493378013372421\n","Step: 99292, loss: 0.23041847348213196\n","Step: 99293, loss: 0.20505668222904205\n","Step: 99294, loss: 0.09609463810920715\n","Step: 99295, loss: 0.0844896137714386\n","Step: 99296, loss: 0.25436362624168396\n","Step: 99297, loss: 0.2848445773124695\n","Step: 99298, loss: 0.0\n","Step: 99299, loss: 0.04955872520804405\n","Step: 99300, loss: 0.06589207053184509\n","Step: 99301, loss: 0.12396823614835739\n","Step: 99302, loss: 0.17852699756622314\n","Step: 99303, loss: 0.03994065150618553\n","Step: 99304, loss: 0.08770601451396942\n","Step: 99305, loss: 0.030951013788580894\n","Step: 99306, loss: 0.0\n","Step: 99307, loss: 0.03167901933193207\n","Step: 99308, loss: 0.14165546000003815\n","Step: 99309, loss: 0.17854920029640198\n","Step: 99310, loss: 0.10172063112258911\n","Step: 99311, loss: 0.032467760145664215\n","Step: 99312, loss: 0.19225461781024933\n","Step: 99313, loss: 0.2587434649467468\n","Step: 99314, loss: 0.12411539256572723\n","Step: 99315, loss: 0.3675174415111542\n","Step: 99316, loss: 0.1216583177447319\n","Step: 99317, loss: 0.03913860023021698\n","Step: 99318, loss: 0.14039194583892822\n","Step: 99319, loss: 0.0\n","Step: 99320, loss: 0.15711313486099243\n","Step: 99321, loss: 0.07448873668909073\n","Step: 99322, loss: 0.12470567971467972\n","Step: 99323, loss: 0.09038145840167999\n","Step: 99324, loss: 0.1811520904302597\n","Step: 99325, loss: 0.0\n","Step: 99326, loss: 0.26203426718711853\n","Step: 99327, loss: 0.2736169397830963\n","Step: 99328, loss: 0.0\n","Step: 99329, loss: 0.2751775085926056\n","Step: 99330, loss: 0.22454003989696503\n","Step: 99331, loss: 0.18025930225849152\n","Step: 99332, loss: 0.10646841675043106\n","Step: 99333, loss: 0.0\n","Step: 99334, loss: 0.12453967332839966\n","Step: 99335, loss: 0.10219931602478027\n","Step: 99336, loss: 0.08644577860832214\n","Step: 99337, loss: 0.06969818472862244\n","Step: 99338, loss: 0.20229113101959229\n","Step: 99339, loss: 0.15560628473758698\n","Step: 99340, loss: 0.09001557528972626\n","Step: 99341, loss: 0.04140500724315643\n","Step: 99342, loss: 0.11177138239145279\n","Step: 99343, loss: 0.07909218966960907\n","Step: 99344, loss: 0.30343344807624817\n","Step: 99345, loss: 0.04836674779653549\n","Step: 99346, loss: 0.13842777907848358\n","Step: 99347, loss: 0.2191367745399475\n","Step: 99348, loss: 0.09415344148874283\n","Step: 99349, loss: 0.21808376908302307\n","Step: 99350, loss: 0.0\n","Step: 99351, loss: 0.26998400688171387\n","Step: 99352, loss: 0.18712560832500458\n","Step: 99353, loss: 0.06384361535310745\n","Step: 99354, loss: 0.03385486826300621\n","Step: 99355, loss: 0.3653309941291809\n","Step: 99356, loss: 0.07824669033288956\n","Step: 99357, loss: 0.12066889554262161\n","Step: 99358, loss: 0.07101437449455261\n","Step: 99359, loss: 0.18574222922325134\n","Step: 99360, loss: 0.2187987118959427\n","Step: 99361, loss: 0.08658628165721893\n","Step: 99362, loss: 0.23072800040245056\n","Step: 99363, loss: 0.2046249806880951\n","Step: 99364, loss: 0.06943709403276443\n","Step: 99365, loss: 0.0671040266752243\n","Step: 99366, loss: 0.1886265128850937\n","Step: 99367, loss: 0.15080668032169342\n","Step: 99368, loss: 0.1963127851486206\n","Step: 99369, loss: 0.428314208984375\n","Step: 99370, loss: 0.11141803860664368\n","Step: 99371, loss: 0.08845312893390656\n","Step: 99372, loss: 0.09031453728675842\n","Step: 99373, loss: 0.03175843879580498\n","Step: 99374, loss: 0.056433189660310745\n","Step: 99375, loss: 0.17243815958499908\n","Step: 99376, loss: 0.20463229715824127\n","Step: 99377, loss: 0.0\n","Step: 99378, loss: 0.10753999650478363\n","Step: 99379, loss: 0.02942594140768051\n","Step: 99380, loss: 0.058035288006067276\n","Step: 99381, loss: 0.14588452875614166\n","Step: 99382, loss: 0.22347649931907654\n","Step: 99383, loss: 0.15239036083221436\n","Step: 99384, loss: 0.2648773491382599\n","Step: 99385, loss: 0.0\n","Step: 99386, loss: 0.15236862003803253\n","Step: 99387, loss: 0.11641748249530792\n","Step: 99388, loss: 0.025710182264447212\n","Step: 99389, loss: 0.23480932414531708\n","Step: 99390, loss: 0.27669888734817505\n","Step: 99391, loss: 0.0\n","Step: 99392, loss: 0.07724803686141968\n","Step: 99393, loss: 0.12601380050182343\n","Step: 99394, loss: 0.05442627891898155\n","Step: 99395, loss: 0.09907195717096329\n","Step: 99396, loss: 0.15276870131492615\n","Step: 99397, loss: 0.27434656023979187\n","Step: 99398, loss: 0.10172861814498901\n","Step: 99399, loss: 0.11654810607433319\n","Step: 99400, loss: 0.11935114115476608\n","Step: 99401, loss: 0.04368782415986061\n","Step: 99402, loss: 0.20132556557655334\n","Step: 99403, loss: 0.4861012399196625\n","Step: 99404, loss: 0.14274387061595917\n","Step: 99405, loss: 0.09644870460033417\n","Step: 99406, loss: 0.11762437969446182\n","Step: 99407, loss: 0.15866488218307495\n","Step: 99408, loss: 0.2136802077293396\n","Step: 99409, loss: 0.10614575445652008\n","Step: 99410, loss: 0.15299762785434723\n","Step: 99411, loss: 0.06746430695056915\n","Step: 99412, loss: 0.14685267210006714\n","Step: 99413, loss: 0.057769786566495895\n","Step: 99414, loss: 0.08113241195678711\n","Step: 99415, loss: 0.07849618047475815\n","Step: 99416, loss: 0.050312433391809464\n","Step: 99417, loss: 0.06221778690814972\n","Step: 99418, loss: 0.05907590687274933\n","Step: 99419, loss: 0.04964453727006912\n","Step: 99420, loss: 0.2181916981935501\n","Step: 99421, loss: 0.058652788400650024\n","Step: 99422, loss: 0.13231173157691956\n","Step: 99423, loss: 0.28887704014778137\n","Step: 99424, loss: 0.0\n","Step: 99425, loss: 0.096824511885643\n","Step: 99426, loss: 0.036854613572359085\n","Step: 99427, loss: 0.07240570336580276\n","Step: 99428, loss: 0.09325481206178665\n","Step: 99429, loss: 0.06105581298470497\n","Step: 99430, loss: 0.09218794852495193\n","Step: 99431, loss: 0.24736084043979645\n","Step: 99432, loss: 0.0\n","Step: 99433, loss: 0.13854481279850006\n","Step: 99434, loss: 0.20130763947963715\n","Step: 99435, loss: 0.25169476866722107\n","Step: 99436, loss: 0.0377965122461319\n","Step: 99437, loss: 0.3024919033050537\n","Step: 99438, loss: 0.08542592078447342\n","Step: 99439, loss: 0.0\n","Step: 99440, loss: 0.09083467721939087\n","Step: 99441, loss: 0.3051691949367523\n","Step: 99442, loss: 0.21827539801597595\n","Step: 99443, loss: 0.11823607236146927\n","Step: 99444, loss: 0.1748712956905365\n","Step: 99445, loss: 0.12181847542524338\n","Step: 99446, loss: 0.29138562083244324\n","Step: 99447, loss: 0.16582927107810974\n","Step: 99448, loss: 0.17712639272212982\n","Step: 99449, loss: 0.07617434859275818\n","Step: 99450, loss: 0.04887032136321068\n","Step: 99451, loss: 0.1274261325597763\n","Step: 99452, loss: 0.09624328464269638\n","Step: 99453, loss: 0.3227463960647583\n","Step: 99454, loss: 0.09356094151735306\n","Step: 99455, loss: 0.08898533880710602\n","Step: 99456, loss: 0.31258824467658997\n","Step: 99457, loss: 0.26037558913230896\n","Step: 99458, loss: 0.19065098464488983\n","Step: 99459, loss: 0.03433172404766083\n","Step: 99460, loss: 0.13412755727767944\n","Step: 99461, loss: 0.07392460852861404\n","Step: 99462, loss: 0.1445675641298294\n","Step: 99463, loss: 0.0\n","Step: 99464, loss: 0.1231468990445137\n","Step: 99465, loss: 0.08266592770814896\n","Step: 99466, loss: 0.045450955629348755\n","Step: 99467, loss: 0.20563921332359314\n","Step: 99468, loss: 0.16023452579975128\n","Step: 99469, loss: 0.04936865344643593\n","Step: 99470, loss: 0.050544966012239456\n","Step: 99471, loss: 0.11179079115390778\n","Step: 99472, loss: 0.14392651617527008\n","Step: 99473, loss: 0.16364112496376038\n","Step: 99474, loss: 0.1528758555650711\n","Step: 99475, loss: 0.11301368474960327\n","Step: 99476, loss: 0.2026422917842865\n","Step: 99477, loss: 0.0\n","Step: 99478, loss: 0.10202596336603165\n","Step: 99479, loss: 0.2008161097764969\n","Step: 99480, loss: 0.1660880148410797\n","Step: 99481, loss: 0.10297265648841858\n","Step: 99482, loss: 0.026825405657291412\n","Step: 99483, loss: 0.17912250757217407\n","Step: 99484, loss: 0.04478592798113823\n","Step: 99485, loss: 0.2069096565246582\n","Step: 99486, loss: 0.07600197941064835\n","Step: 99487, loss: 0.06564819067716599\n","Step: 99488, loss: 0.1430477350950241\n","Step: 99489, loss: 0.06871923804283142\n","Step: 99490, loss: 0.09322413802146912\n","Step: 99491, loss: 0.07640931010246277\n","Step: 99492, loss: 0.18070770800113678\n","Step: 99493, loss: 0.16221535205841064\n","Step: 99494, loss: 0.18922653794288635\n","Step: 99495, loss: 0.14236918091773987\n","Step: 99496, loss: 0.0\n","Step: 99497, loss: 0.05613335967063904\n","Step: 99498, loss: 0.18874597549438477\n","Step: 99499, loss: 0.0916450023651123\n","Step: 99500, loss: 0.09178642928600311\n","Step: 99501, loss: 0.09092918038368225\n","Step: 99502, loss: 0.12772637605667114\n","Step: 99503, loss: 0.08553431183099747\n","Step: 99504, loss: 0.08493637293577194\n","Step: 99505, loss: 0.2538592219352722\n","Step: 99506, loss: 0.18911632895469666\n","Step: 99507, loss: 0.4023767113685608\n","Step: 99508, loss: 0.20435699820518494\n","Step: 99509, loss: 0.1226746141910553\n","Step: 99510, loss: 0.16002830862998962\n","Step: 99511, loss: 0.09650793671607971\n","Step: 99512, loss: 0.2861804962158203\n","Step: 99513, loss: 0.08224775642156601\n","Step: 99514, loss: 0.2093590945005417\n","Step: 99515, loss: 0.0912749320268631\n","Step: 99516, loss: 0.15637420117855072\n","Step: 99517, loss: 0.19510991871356964\n","Step: 99518, loss: 0.2096165418624878\n","Step: 99519, loss: 0.12039490044116974\n","Step: 99520, loss: 0.25418099761009216\n","Step: 99521, loss: 0.11390456557273865\n","Step: 99522, loss: 0.12202616780996323\n","Step: 99523, loss: 0.049108050763607025\n","Step: 99524, loss: 0.07120095938444138\n","Step: 99525, loss: 0.0\n","Step: 99526, loss: 0.14027011394500732\n","Step: 99527, loss: 0.07250094413757324\n","Step: 99528, loss: 0.032173726707696915\n","Step: 99529, loss: 0.2331220656633377\n","Step: 99530, loss: 0.08202420175075531\n","Step: 99531, loss: 0.0\n","Step: 99532, loss: 0.3029675781726837\n","Step: 99533, loss: 0.06578262895345688\n","Step: 99534, loss: 0.282306969165802\n","Step: 99535, loss: 0.16457678377628326\n","Step: 99536, loss: 0.16640758514404297\n","Step: 99537, loss: 0.11678113043308258\n","Step: 99538, loss: 0.11916093528270721\n","Step: 99539, loss: 0.05851238593459129\n","Step: 99540, loss: 0.0\n","Step: 99541, loss: 0.04874393343925476\n","Step: 99542, loss: 0.04942791163921356\n","Step: 99543, loss: 0.2208205759525299\n","Step: 99544, loss: 0.1069980338215828\n","Step: 99545, loss: 0.035175248980522156\n","Step: 99546, loss: 0.17466098070144653\n","Step: 99547, loss: 0.0\n","Step: 99548, loss: 0.04687805473804474\n","Step: 99549, loss: 0.0\n","Step: 99550, loss: 0.08568432927131653\n","Step: 99551, loss: 0.15828756988048553\n","Step: 99552, loss: 0.16362342238426208\n","Step: 99553, loss: 0.0\n","Step: 99554, loss: 0.12238560616970062\n","Step: 99555, loss: 0.25879040360450745\n","Step: 99556, loss: 0.1579737514257431\n","Step: 99557, loss: 0.21393512189388275\n","Step: 99558, loss: 0.17462432384490967\n","Step: 99559, loss: 0.15567828714847565\n","Step: 99560, loss: 0.16569384932518005\n","Step: 99561, loss: 0.07941257208585739\n","Step: 99562, loss: 0.08951406180858612\n","Step: 99563, loss: 0.048342179507017136\n","Step: 99564, loss: 0.2096978724002838\n","Step: 99565, loss: 0.2360408753156662\n","Step: 99566, loss: 0.08974714577198029\n","Step: 99567, loss: 0.0\n","Step: 99568, loss: 0.033143602311611176\n","Step: 99569, loss: 0.13562172651290894\n","Step: 99570, loss: 0.11859021335840225\n","Step: 99571, loss: 0.06228645145893097\n","Step: 99572, loss: 0.15998156368732452\n","Step: 99573, loss: 0.053131092339754105\n","Step: 99574, loss: 0.07838476449251175\n","Step: 99575, loss: 0.14317092299461365\n","Step: 99576, loss: 0.22011123597621918\n","Step: 99577, loss: 0.09051761031150818\n","Step: 99578, loss: 0.19500797986984253\n","Step: 99579, loss: 0.07126536220312119\n","Step: 99580, loss: 0.12093609571456909\n","Step: 99581, loss: 0.08229085803031921\n","Step: 99582, loss: 0.20567013323307037\n","Step: 99583, loss: 0.0677015483379364\n","Step: 99584, loss: 0.04644482210278511\n","Step: 99585, loss: 0.15004181861877441\n","Step: 99586, loss: 0.1824057400226593\n","Step: 99587, loss: 0.13239416480064392\n","Step: 99588, loss: 0.1112615168094635\n","Step: 99589, loss: 0.03251691907644272\n","Step: 99590, loss: 0.2667842209339142\n","Step: 99591, loss: 0.16070908308029175\n","Step: 99592, loss: 0.19432958960533142\n","Step: 99593, loss: 0.0\n","Step: 99594, loss: 0.048439737409353256\n","Step: 99595, loss: 0.24720364809036255\n","Step: 99596, loss: 0.0\n","Step: 99597, loss: 0.08991067856550217\n","Step: 99598, loss: 0.06957339495420456\n","Step: 99599, loss: 0.14166343212127686\n","Step: 99600, loss: 0.08468586206436157\n","Step: 99601, loss: 0.104204922914505\n","Step: 99602, loss: 0.040957145392894745\n","Step: 99603, loss: 0.07616768777370453\n","Step: 99604, loss: 0.07615157961845398\n","Step: 99605, loss: 0.07538363337516785\n","Step: 99606, loss: 0.19860927760601044\n","Step: 99607, loss: 0.09774139523506165\n","Step: 99608, loss: 0.06607862561941147\n","Step: 99609, loss: 0.07888367027044296\n","Step: 99610, loss: 0.2620210349559784\n","Step: 99611, loss: 0.15741434693336487\n","Step: 99612, loss: 0.19003798067569733\n","Step: 99613, loss: 0.07534918189048767\n","Step: 99614, loss: 0.27800533175468445\n","Step: 99615, loss: 0.2198493927717209\n","Step: 99616, loss: 0.0\n","Step: 99617, loss: 0.1693771332502365\n","Step: 99618, loss: 0.2135477066040039\n","Step: 99619, loss: 0.1385970115661621\n","Step: 99620, loss: 0.21219374239444733\n","Step: 99621, loss: 0.02590256929397583\n","Step: 99622, loss: 0.2650686204433441\n","Step: 99623, loss: 0.04792141914367676\n","Step: 99624, loss: 0.09442980587482452\n","Step: 99625, loss: 0.11983384937047958\n","Step: 99626, loss: 0.20288227498531342\n","Step: 99627, loss: 0.18550075590610504\n","Step: 99628, loss: 0.12999801337718964\n","Step: 99629, loss: 0.0726303830742836\n","Step: 99630, loss: 0.08921797573566437\n","Step: 99631, loss: 0.07382702082395554\n","Step: 99632, loss: 0.07718151807785034\n","Step: 99633, loss: 0.048389434814453125\n","Step: 99634, loss: 0.21675457060337067\n","Step: 99635, loss: 0.1628805547952652\n","Step: 99636, loss: 0.11894901096820831\n","Step: 99637, loss: 0.31658419966697693\n","Step: 99638, loss: 0.04815298691391945\n","Step: 99639, loss: 0.05202801525592804\n","Step: 99640, loss: 0.05173758417367935\n","Step: 99641, loss: 0.10331445932388306\n","Step: 99642, loss: 0.04641224816441536\n","Step: 99643, loss: 0.08752134442329407\n","Step: 99644, loss: 0.0\n","Step: 99645, loss: 0.0\n","Step: 99646, loss: 0.1715887039899826\n","Step: 99647, loss: 0.05833878368139267\n","Step: 99648, loss: 0.18376876413822174\n","Step: 99649, loss: 0.14142842590808868\n","Step: 99650, loss: 0.17303432524204254\n","Step: 99651, loss: 0.13121263682842255\n","Step: 99652, loss: 0.15265798568725586\n","Step: 99653, loss: 0.17987579107284546\n","Step: 99654, loss: 0.13646912574768066\n","Step: 99655, loss: 0.1835489273071289\n","Step: 99656, loss: 0.35935384035110474\n","Step: 99657, loss: 0.11153016984462738\n","Step: 99658, loss: 0.24793528020381927\n","Step: 99659, loss: 0.20481720566749573\n","Step: 99660, loss: 0.0483003705739975\n","Step: 99661, loss: 0.048104315996170044\n","Step: 99662, loss: 0.18290817737579346\n","Step: 99663, loss: 0.17337314784526825\n","Step: 99664, loss: 0.08186811208724976\n","Step: 99665, loss: 0.059312764555215836\n","Step: 99666, loss: 0.06586595624685287\n","Step: 99667, loss: 0.0\n","Step: 99668, loss: 0.20690658688545227\n","Step: 99669, loss: 0.0\n","Step: 99670, loss: 0.03995651751756668\n","Step: 99671, loss: 0.08710748702287674\n","Step: 99672, loss: 0.15313957631587982\n","Step: 99673, loss: 0.0\n","Step: 99674, loss: 0.2652939558029175\n","Step: 99675, loss: 0.12606537342071533\n","Step: 99676, loss: 0.1715451180934906\n","Step: 99677, loss: 0.0\n","Step: 99678, loss: 0.0\n","Step: 99679, loss: 0.19517360627651215\n","Step: 99680, loss: 0.0\n","Step: 99681, loss: 0.10192447900772095\n","Step: 99682, loss: 0.16033746302127838\n","Step: 99683, loss: 0.11231055110692978\n","Step: 99684, loss: 0.08472736179828644\n","Step: 99685, loss: 0.1323578953742981\n","Step: 99686, loss: 0.10385316610336304\n","Step: 99687, loss: 0.22347038984298706\n","Step: 99688, loss: 0.19799980521202087\n","Step: 99689, loss: 0.22538655996322632\n","Step: 99690, loss: 0.1312502920627594\n","Step: 99691, loss: 0.2179241180419922\n","Step: 99692, loss: 0.14989078044891357\n","Step: 99693, loss: 0.04420967027544975\n","Step: 99694, loss: 0.17657111585140228\n","Step: 99695, loss: 0.0\n","Step: 99696, loss: 0.11229372769594193\n","Step: 99697, loss: 0.0\n","Step: 99698, loss: 0.12378987669944763\n","Step: 99699, loss: 0.0\n","Step: 99700, loss: 0.2757580280303955\n","Step: 99701, loss: 0.15075327455997467\n","Step: 99702, loss: 0.06607572734355927\n","Step: 99703, loss: 0.032634079456329346\n","Step: 99704, loss: 0.07503286004066467\n","Step: 99705, loss: 0.08051606267690659\n","Step: 99706, loss: 0.2847570478916168\n","Step: 99707, loss: 0.21930579841136932\n","Step: 99708, loss: 0.29478082060813904\n","Step: 99709, loss: 0.1439836621284485\n","Step: 99710, loss: 0.030764011666178703\n","Step: 99711, loss: 0.042955685406923294\n","Step: 99712, loss: 0.07335513830184937\n","Step: 99713, loss: 0.10155453532934189\n","Step: 99714, loss: 0.10059931874275208\n","Step: 99715, loss: 0.22575023770332336\n","Step: 99716, loss: 0.12573827803134918\n","Step: 99717, loss: 0.048647962510585785\n","Step: 99718, loss: 0.029997581616044044\n","Step: 99719, loss: 0.1737530529499054\n","Step: 99720, loss: 0.1436077207326889\n","Step: 99721, loss: 0.200597882270813\n","Step: 99722, loss: 0.1575838029384613\n","Step: 99723, loss: 0.0\n","Step: 99724, loss: 0.16891446709632874\n","Step: 99725, loss: 0.2917341887950897\n","Step: 99726, loss: 0.1534765362739563\n","Step: 99727, loss: 0.18976695835590363\n","Step: 99728, loss: 0.03218697011470795\n","Step: 99729, loss: 0.22733527421951294\n","Step: 99730, loss: 0.08316101133823395\n","Step: 99731, loss: 0.18061405420303345\n","Step: 99732, loss: 0.13759703934192657\n","Step: 99733, loss: 0.03472736477851868\n","Step: 99734, loss: 0.1038980707526207\n","Step: 99735, loss: 0.08804433792829514\n","Step: 99736, loss: 0.11737281084060669\n","Step: 99737, loss: 0.16605699062347412\n","Step: 99738, loss: 0.04031338542699814\n","Step: 99739, loss: 0.16094781458377838\n","Step: 99740, loss: 0.22955456376075745\n","Step: 99741, loss: 0.12515035271644592\n","Step: 99742, loss: 0.28764787316322327\n","Step: 99743, loss: 0.08643102645874023\n","Step: 99744, loss: 0.13390737771987915\n","Step: 99745, loss: 0.03488133102655411\n","Step: 99746, loss: 0.15951164066791534\n","Step: 99747, loss: 0.15759101510047913\n","Step: 99748, loss: 0.15292960405349731\n","Step: 99749, loss: 0.106879822909832\n","Step: 99750, loss: 0.058157339692115784\n","Step: 99751, loss: 0.2623990774154663\n","Step: 99752, loss: 0.18081003427505493\n","Step: 99753, loss: 0.11520815640687943\n","Step: 99754, loss: 0.22928649187088013\n","Step: 99755, loss: 0.19329361617565155\n","Step: 99756, loss: 0.1895873248577118\n","Step: 99757, loss: 0.28724440932273865\n","Step: 99758, loss: 0.0\n","Step: 99759, loss: 0.1598256677389145\n","Step: 99760, loss: 0.1318809539079666\n","Step: 99761, loss: 0.0887458324432373\n","Step: 99762, loss: 0.12374739348888397\n","Step: 99763, loss: 0.04719391465187073\n","Step: 99764, loss: 0.0\n","Step: 99765, loss: 0.18515253067016602\n","Step: 99766, loss: 0.36195728182792664\n","Step: 99767, loss: 0.24381835758686066\n","Step: 99768, loss: 0.039381954818964005\n","Step: 99769, loss: 0.07872966676950455\n","Step: 99770, loss: 0.0\n","Step: 99771, loss: 0.1411609649658203\n","Step: 99772, loss: 0.18127571046352386\n","Step: 99773, loss: 0.11418823152780533\n","Step: 99774, loss: 0.30096328258514404\n","Step: 99775, loss: 0.23939825594425201\n","Step: 99776, loss: 0.2541583776473999\n","Step: 99777, loss: 0.119917131960392\n","Step: 99778, loss: 0.3732087016105652\n","Step: 99779, loss: 0.11440681666135788\n","Step: 99780, loss: 0.3379000723361969\n","Step: 99781, loss: 0.14702965319156647\n","Step: 99782, loss: 0.03706887364387512\n","Step: 99783, loss: 0.20134609937667847\n","Step: 99784, loss: 0.0722695142030716\n","Step: 99785, loss: 0.254332035779953\n","Step: 99786, loss: 0.036724332720041275\n","Step: 99787, loss: 0.09860257059335709\n","Step: 99788, loss: 0.03656219318509102\n","Step: 99789, loss: 0.038327306509017944\n","Step: 99790, loss: 0.4401830732822418\n","Step: 99791, loss: 0.07722138613462448\n","Step: 99792, loss: 0.0\n","Step: 99793, loss: 0.13194540143013\n","Step: 99794, loss: 0.14306706190109253\n","Step: 99795, loss: 0.0\n","Step: 99796, loss: 0.13067254424095154\n","Step: 99797, loss: 0.03900200501084328\n","Step: 99798, loss: 0.05135970190167427\n","Step: 99799, loss: 0.30717089772224426\n","Step: 99800, loss: 0.10475094616413116\n","Step: 99801, loss: 0.14165958762168884\n","Step: 99802, loss: 0.14076033234596252\n","Step: 99803, loss: 0.06952231377363205\n","Step: 99804, loss: 0.10387947410345078\n","Step: 99805, loss: 0.0\n","Step: 99806, loss: 0.13533993065357208\n","Step: 99807, loss: 0.15627840161323547\n","Step: 99808, loss: 0.17500285804271698\n","Step: 99809, loss: 0.035787858068943024\n","Step: 99810, loss: 0.17530837655067444\n","Step: 99811, loss: 0.18118205666542053\n","Step: 99812, loss: 0.24469435214996338\n","Step: 99813, loss: 0.04561775177717209\n","Step: 99814, loss: 0.17711453139781952\n","Step: 99815, loss: 0.163243368268013\n","Step: 99816, loss: 0.0951588824391365\n","Step: 99817, loss: 0.12931817770004272\n","Step: 99818, loss: 0.08044952154159546\n","Step: 99819, loss: 0.1179318055510521\n","Step: 99820, loss: 0.03040217235684395\n","Step: 99821, loss: 0.030009258538484573\n","Step: 99822, loss: 0.0\n","Step: 99823, loss: 0.05079595744609833\n","Step: 99824, loss: 0.028569156304001808\n","Step: 99825, loss: 0.26379191875457764\n","Step: 99826, loss: 0.19013649225234985\n","Step: 99827, loss: 0.14953437447547913\n","Step: 99828, loss: 0.0\n","Step: 99829, loss: 0.049820948392152786\n","Step: 99830, loss: 0.04589051380753517\n","Step: 99831, loss: 0.02787184901535511\n","Step: 99832, loss: 0.12382258474826813\n","Step: 99833, loss: 0.09849420189857483\n","Step: 99834, loss: 0.045710813254117966\n","Step: 99835, loss: 0.09750786423683167\n","Step: 99836, loss: 0.13820162415504456\n","Step: 99837, loss: 0.25086697936058044\n","Step: 99838, loss: 0.0281910989433527\n","Step: 99839, loss: 0.044765084981918335\n","Step: 99840, loss: 0.1603730320930481\n","Step: 99841, loss: 0.07040466368198395\n","Step: 99842, loss: 0.11879066377878189\n","Step: 99843, loss: 0.14473271369934082\n","Step: 99844, loss: 0.093227319419384\n","Step: 99845, loss: 0.09825500845909119\n","Step: 99846, loss: 0.05326443165540695\n","Step: 99847, loss: 0.11466377228498459\n","Step: 99848, loss: 0.14852242171764374\n","Step: 99849, loss: 0.2163081169128418\n","Step: 99850, loss: 0.1442740559577942\n","Step: 99851, loss: 0.09884759783744812\n","Step: 99852, loss: 0.16166552901268005\n","Step: 99853, loss: 0.155572772026062\n","Step: 99854, loss: 0.15078935027122498\n","Step: 99855, loss: 0.0\n","Step: 99856, loss: 0.08423656225204468\n","Step: 99857, loss: 0.15514764189720154\n","Step: 99858, loss: 0.0458674393594265\n","Step: 99859, loss: 0.0\n","Step: 99860, loss: 0.0668133944272995\n","Step: 99861, loss: 0.07386651635169983\n","Step: 99862, loss: 0.07221940904855728\n","Step: 99863, loss: 0.18776041269302368\n","Step: 99864, loss: 0.1769137978553772\n","Step: 99865, loss: 0.0\n","Step: 99866, loss: 0.15484578907489777\n","Step: 99867, loss: 0.03655416518449783\n","Step: 99868, loss: 0.11345762014389038\n","Step: 99869, loss: 0.17321251332759857\n","Step: 99870, loss: 0.12093956023454666\n","Step: 99871, loss: 0.278927206993103\n","Step: 99872, loss: 0.0738910362124443\n","Step: 99873, loss: 0.16475501656532288\n","Step: 99874, loss: 0.0\n","Step: 99875, loss: 0.18074539303779602\n","Step: 99876, loss: 0.44795963168144226\n","Step: 99877, loss: 0.13606451451778412\n","Step: 99878, loss: 0.14149978756904602\n","Step: 99879, loss: 0.1435023546218872\n","Step: 99880, loss: 0.12505443394184113\n","Step: 99881, loss: 0.05518811568617821\n","Step: 99882, loss: 0.10705618560314178\n","Step: 99883, loss: 0.20352348685264587\n","Step: 99884, loss: 0.11832378059625626\n","Step: 99885, loss: 0.2744475305080414\n","Step: 99886, loss: 0.17872567474842072\n","Step: 99887, loss: 0.0\n","Step: 99888, loss: 0.32207003235816956\n","Step: 99889, loss: 0.185010626912117\n","Step: 99890, loss: 0.12126405537128448\n","Step: 99891, loss: 0.22231703996658325\n","Step: 99892, loss: 0.21537667512893677\n","Step: 99893, loss: 0.09955649077892303\n","Step: 99894, loss: 0.036225851625204086\n","Step: 99895, loss: 0.1884014904499054\n","Step: 99896, loss: 0.03894435241818428\n","Step: 99897, loss: 0.17377041280269623\n","Step: 99898, loss: 0.24732589721679688\n","Step: 99899, loss: 0.22790716588497162\n","Step: 99900, loss: 0.048348721116781235\n","Step: 99901, loss: 0.1324877291917801\n","Step: 99902, loss: 0.0346018522977829\n","Step: 99903, loss: 0.0\n","Step: 99904, loss: 0.033112600445747375\n","Step: 99905, loss: 0.12702424824237823\n","Step: 99906, loss: 0.07565464079380035\n","Step: 99907, loss: 0.22537018358707428\n","Step: 99908, loss: 0.0\n","Step: 99909, loss: 0.12962782382965088\n","Step: 99910, loss: 0.057614754885435104\n","Step: 99911, loss: 0.13523606956005096\n","Step: 99912, loss: 0.25718092918395996\n","Step: 99913, loss: 0.21130403876304626\n","Step: 99914, loss: 0.19872769713401794\n","Step: 99915, loss: 0.17883437871932983\n","Step: 99916, loss: 0.17905351519584656\n","Step: 99917, loss: 0.09174853563308716\n","Step: 99918, loss: 0.2512477934360504\n","Step: 99919, loss: 0.04573802649974823\n","Step: 99920, loss: 0.13350051641464233\n","Step: 99921, loss: 0.3102557063102722\n","Step: 99922, loss: 0.13224953413009644\n","Step: 99923, loss: 0.08227160573005676\n","Step: 99924, loss: 0.0961015447974205\n","Step: 99925, loss: 0.2704612612724304\n","Step: 99926, loss: 0.12786617875099182\n","Step: 99927, loss: 0.1345633864402771\n","Step: 99928, loss: 0.10090411454439163\n","Step: 99929, loss: 0.0918133482336998\n","Step: 99930, loss: 0.08211909979581833\n","Step: 99931, loss: 0.0\n","Step: 99932, loss: 0.1714988350868225\n","Step: 99933, loss: 0.035981062799692154\n","Step: 99934, loss: 0.2075791209936142\n","Step: 99935, loss: 0.19033566117286682\n","Step: 99936, loss: 0.17026937007904053\n","Step: 99937, loss: 0.14988067746162415\n","Step: 99938, loss: 0.11537805199623108\n","Step: 99939, loss: 0.19452674686908722\n","Step: 99940, loss: 0.046226177364587784\n","Step: 99941, loss: 0.08406808972358704\n","Step: 99942, loss: 0.11334463208913803\n","Step: 99943, loss: 0.2923629879951477\n","Step: 99944, loss: 0.0\n","Step: 99945, loss: 0.08359353989362717\n","Step: 99946, loss: 0.19285953044891357\n","Step: 99947, loss: 0.048115480691194534\n","Step: 99948, loss: 0.11895302683115005\n","Step: 99949, loss: 0.0\n","Step: 99950, loss: 0.0\n","Step: 99951, loss: 0.17064471542835236\n","Step: 99952, loss: 0.15462608635425568\n","Step: 99953, loss: 0.0\n","Step: 99954, loss: 0.10879417508840561\n","Step: 99955, loss: 0.08533576875925064\n","Step: 99956, loss: 0.07489435374736786\n","Step: 99957, loss: 0.09907425194978714\n","Step: 99958, loss: 0.0\n","Step: 99959, loss: 0.30678460001945496\n","Step: 99960, loss: 0.2872973084449768\n","Step: 99961, loss: 0.030891766771674156\n","Step: 99962, loss: 0.351835161447525\n","Step: 99963, loss: 0.1327161341905594\n","Step: 99964, loss: 0.10722861438989639\n","Step: 99965, loss: 0.14779433608055115\n","Step: 99966, loss: 0.09598340839147568\n","Step: 99967, loss: 0.20804695785045624\n","Step: 99968, loss: 0.16311891376972198\n","Step: 99969, loss: 0.17219890654087067\n","Step: 99970, loss: 0.12879464030265808\n","Step: 99971, loss: 0.09784149378538132\n","Step: 99972, loss: 0.031305354088544846\n","Step: 99973, loss: 0.0800490751862526\n","Step: 99974, loss: 0.07534612715244293\n","Step: 99975, loss: 0.21514782309532166\n","Step: 99976, loss: 0.10756868124008179\n","Step: 99977, loss: 0.0\n","Step: 99978, loss: 0.10420456528663635\n","Step: 99979, loss: 0.19624219834804535\n","Step: 99980, loss: 0.17284747958183289\n","Step: 99981, loss: 0.23383431136608124\n","Step: 99982, loss: 0.04337023198604584\n","Step: 99983, loss: 0.1806342750787735\n","Step: 99984, loss: 0.11705433577299118\n","Step: 99985, loss: 0.18145674467086792\n","Step: 99986, loss: 0.04673392325639725\n","Step: 99987, loss: 0.25190213322639465\n","Step: 99988, loss: 0.031232459470629692\n","Step: 99989, loss: 0.0\n","Step: 99990, loss: 0.1310163140296936\n","Step: 99991, loss: 0.09245175123214722\n","Step: 99992, loss: 0.1649664342403412\n","Step: 99993, loss: 0.0\n","Step: 99994, loss: 0.032181065529584885\n","Step: 99995, loss: 0.0\n","Step: 99996, loss: 0.11397777497768402\n","Step: 99997, loss: 0.19622617959976196\n","Step: 99998, loss: 0.12231816351413727\n","Step: 99999, loss: 0.16323356330394745\n","Step: 100000, loss: 0.07793046534061432\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lFAfseXHUyuo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621050711302,"user_tz":-420,"elapsed":3743,"user":{"displayName":"Hoang Phi Nguyen","photoUrl":"","userId":"08938458352174786823"}},"outputId":"e497d617-622d-4623-b1f2-405d8bd506ab"},"source":["# test_data_reader = DataReader(\n","#     data_path= '/content/drive/MyDrive/DSLab/Session 3/data/test_tf_idf.txt',\n","#     batch_size= 50,\n","#     vocab_size= vocab_size\n","# )\n","with tf.Session() as sess:\n","  epoch = 442\n","  trainable_varibles = tf.trainable_variables()\n","  for variable in trainable_varibles:\n","    saved_value = restore_parameter(variable.name, epoch)\n","    assign_op = variable.assign(saved_value)\n","    sess.run(assign_op)\n","  \n","  num_true_preds = 0\n","  while True:\n","    test_data, test_labels = test_data_reader.next_batch()\n","    test_plables_eval = sess.run(\n","        predicted_labels,\n","        feed_dict= {\n","            mlp._X: test_data,\n","            mlp._real_Y: test_labels\n","        }\n","    )\n","    matches = np.equal(test_plables_eval, test_labels)\n","    num_true_preds += np.sum(matches.astype(float))\n","\n","    if test_data_reader._batch_id == 0:\n","      break\n","  \n","  print('Epoch: ', epoch)\n","  print('Accuracy on test data: ', num_true_preds / len(test_data_reader._data))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch:  442\n","Accuracy on test data:  0.7193308550185874\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O2hP7FW4ZF71"},"source":["Epoch:  442\n","Accuracy on test data:  0.7193308550185874"]}]}